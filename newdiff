diff --git a/_config.yml b/_config.yml
deleted file mode 100644
index 9da9a0291..000000000
--- a/_config.yml
+++ /dev/null
@@ -1 +0,0 @@
-theme: jekyll-theme-dinky
\ No newline at end of file
diff --git a/bootstrap-core/src/main/java/com/navercorp/pinpoint/bootstrap/config/DefaultProfilerConfig.java b/bootstrap-core/src/main/java/com/navercorp/pinpoint/bootstrap/config/DefaultProfilerConfig.java
index 4df07e5f2..efb4ee642 100644
--- a/bootstrap-core/src/main/java/com/navercorp/pinpoint/bootstrap/config/DefaultProfilerConfig.java
+++ b/bootstrap-core/src/main/java/com/navercorp/pinpoint/bootstrap/config/DefaultProfilerConfig.java
@@ -170,6 +170,8 @@ public class DefaultProfilerConfig implements ProfilerConfig {
 
     private List<String> httpStatusCodeErrors = Collections.emptyList();
 
+    private String injectionModuleFactoryClazzName = null;
+
     public DefaultProfilerConfig() {
         this.properties = new Properties();
     }
@@ -462,6 +464,11 @@ public class DefaultProfilerConfig implements ProfilerConfig {
         return httpStatusCodeErrors;
     }
 
+    @Override
+    public String getInjectionModuleFactoryClazzName() {
+        return injectionModuleFactoryClazzName;
+    }
+
     // for test
     void readPropertyValues() {
         // TODO : use Properties' default value instead of using a temp variable.
@@ -570,6 +577,8 @@ public class DefaultProfilerConfig implements ProfilerConfig {
 
         this.httpStatusCodeErrors = readList("profiler.http.status.code.errors");
 
+        this.injectionModuleFactoryClazzName = readString("profiler.guice.module.factory", null);
+
         logger.info("configuration loaded successfully.");
     }
 
@@ -730,6 +739,7 @@ public class DefaultProfilerConfig implements ProfilerConfig {
         sb.append(", supportLambdaExpressions=").append(supportLambdaExpressions);
         sb.append(", proxyHttpHeaderEnable=").append(proxyHttpHeaderEnable);
         sb.append(", httpStatusCodeErrors=").append(httpStatusCodeErrors);
+        sb.append(", injectionModuleFactoryClazzName=").append(injectionModuleFactoryClazzName);
         sb.append('}');
         return sb.toString();
     }
diff --git a/bootstrap-core/src/main/java/com/navercorp/pinpoint/bootstrap/config/ProfilerConfig.java b/bootstrap-core/src/main/java/com/navercorp/pinpoint/bootstrap/config/ProfilerConfig.java
index 4a5c27283..d3a084c67 100644
--- a/bootstrap-core/src/main/java/com/navercorp/pinpoint/bootstrap/config/ProfilerConfig.java
+++ b/bootstrap-core/src/main/java/com/navercorp/pinpoint/bootstrap/config/ProfilerConfig.java
@@ -133,6 +133,8 @@ public interface ProfilerConfig {
 
     List<String> getHttpStatusCodeErrors();
 
+    String getInjectionModuleFactoryClazzName();
+
     String readString(String propertyName, String defaultValue);
 
     int readInt(String propertyName, int defaultValue);
diff --git a/collector/src/main/java/com/navercorp/pinpoint/collector/cluster/flink/FlinkClusterConnectionManager.java b/collector/src/main/java/com/navercorp/pinpoint/collector/cluster/flink/FlinkClusterConnectionManager.java
index a40430f19..f2a939347 100644
--- a/collector/src/main/java/com/navercorp/pinpoint/collector/cluster/flink/FlinkClusterConnectionManager.java
+++ b/collector/src/main/java/com/navercorp/pinpoint/collector/cluster/flink/FlinkClusterConnectionManager.java
@@ -20,6 +20,8 @@ import com.navercorp.pinpoint.profiler.sender.TcpDataSender;
 import com.navercorp.pinpoint.rpc.client.DefaultPinpointClientFactory;
 import com.navercorp.pinpoint.rpc.client.PinpointClientFactory;
 import com.navercorp.pinpoint.thrift.io.FlinkHeaderTBaseSerializerFactory;
+import com.navercorp.pinpoint.thrift.io.HeaderTBaseSerializer;
+import com.navercorp.pinpoint.thrift.io.SerializerFactory;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -34,7 +36,7 @@ public class FlinkClusterConnectionManager implements ClusterConnectionManager {
     private final Logger logger = LoggerFactory.getLogger(this.getClass());
     private final PinpointClientFactory pinpointClientFactory;
     private final TcpDataSenderRepository tcpDataSenderRepository;
-    private final FlinkHeaderTBaseSerializerFactory flinkHeaderTBaseSerializerFactory;
+    private final SerializerFactory<HeaderTBaseSerializer> flinkHeaderTBaseSerializerFactory;
 
     public FlinkClusterConnectionManager(TcpDataSenderRepository tcpDataSenderRepository) {
         this.tcpDataSenderRepository = tcpDataSenderRepository;
diff --git a/collector/src/main/java/com/navercorp/pinpoint/collector/controller/ServerTimeController.java b/collector/src/main/java/com/navercorp/pinpoint/collector/controller/ServerTimeController.java
new file mode 100644
index 000000000..459c01b16
--- /dev/null
+++ b/collector/src/main/java/com/navercorp/pinpoint/collector/controller/ServerTimeController.java
@@ -0,0 +1,52 @@
+/*
+ * Copyright 2018 NAVER Corp.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package com.navercorp.pinpoint.collector.controller;
+
+import com.fasterxml.jackson.annotation.JsonProperty;
+import org.springframework.stereotype.Controller;
+import org.springframework.web.bind.annotation.RequestMapping;
+import org.springframework.web.bind.annotation.RequestMethod;
+import org.springframework.web.bind.annotation.ResponseBody;
+
+/**
+ * @author Taejin Koo
+ */
+@Controller
+public class ServerTimeController {
+
+    @RequestMapping(value = "/serverTime", method = RequestMethod.GET)
+    @ResponseBody
+    public ServerTime getServerTime() {
+        return new ServerTime();
+    }
+
+
+    private static class ServerTime {
+
+        private final long currentServerTime;
+
+        public ServerTime() {
+            this.currentServerTime = System.currentTimeMillis();
+        }
+
+        @JsonProperty("currentServerTime")
+        public long getCurrentServerTime() {
+            return currentServerTime;
+        }
+    }
+
+}
diff --git a/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/HbaseAgentEventDao.java b/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/HbaseAgentEventDao.java
index 584c4c302..84707cabd 100644
--- a/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/HbaseAgentEventDao.java
+++ b/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/HbaseAgentEventDao.java
@@ -16,6 +16,8 @@
 
 package com.navercorp.pinpoint.collector.dao.hbase;
 
+import com.navercorp.pinpoint.common.hbase.TableNameProvider;
+import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -44,6 +46,9 @@ public class HbaseAgentEventDao implements AgentEventDao {
     private HbaseOperations2 hbaseTemplate;
 
     @Autowired
+    private TableNameProvider tableNameProvider;
+
+    @Autowired
     private AgentEventValueMapper valueMapper;
 
     @Override
@@ -64,7 +69,8 @@ public class HbaseAgentEventDao implements AgentEventDao {
         final AgentEventType eventType = agentEventBo.getEventType();
         byte[] qualifier = Bytes.toBytes(eventType.getCode());
 
-        this.hbaseTemplate.put(HBaseTables.AGENT_EVENT, rowKey, HBaseTables.AGENT_EVENT_CF_EVENTS, qualifier, agentEventBo, this.valueMapper);
+        TableName agentEventTableName = tableNameProvider.getTableName(HBaseTables.AGENT_EVENT_STR);
+        this.hbaseTemplate.put(agentEventTableName, rowKey, HBaseTables.AGENT_EVENT_CF_EVENTS, qualifier, agentEventBo, this.valueMapper);
     }
 
     byte[] createRowKey(String agentId, long eventTimestamp) {
diff --git a/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/HbaseAgentInfoDao.java b/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/HbaseAgentInfoDao.java
index d166a4970..f589a4643 100644
--- a/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/HbaseAgentInfoDao.java
+++ b/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/HbaseAgentInfoDao.java
@@ -18,6 +18,7 @@ package com.navercorp.pinpoint.collector.dao.hbase;
 
 import com.navercorp.pinpoint.collector.dao.AgentInfoDao;
 import com.navercorp.pinpoint.collector.mapper.thrift.ThriftBoMapper;
+import com.navercorp.pinpoint.common.hbase.TableNameProvider;
 import com.navercorp.pinpoint.common.server.bo.AgentInfoBo;
 import com.navercorp.pinpoint.common.server.bo.JvmInfoBo;
 import com.navercorp.pinpoint.common.server.bo.ServerMetaDataBo;
@@ -29,6 +30,7 @@ import com.navercorp.pinpoint.thrift.dto.TAgentInfo;
 import com.navercorp.pinpoint.thrift.dto.TJvmInfo;
 import com.navercorp.pinpoint.thrift.dto.TServerMetaData;
 
+import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.client.Put;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.slf4j.Logger;
@@ -49,6 +51,9 @@ public class HbaseAgentInfoDao implements AgentInfoDao {
     private HbaseOperations2 hbaseTemplate;
 
     @Autowired
+    private TableNameProvider tableNameProvider;
+
+    @Autowired
     @Qualifier("agentInfoBoMapper")
     private ThriftBoMapper<AgentInfoBo, TAgentInfo> agentInfoBoMapper;
 
@@ -93,6 +98,7 @@ public class HbaseAgentInfoDao implements AgentInfoDao {
             put.addColumn(HBaseTables.AGENTINFO_CF_INFO, HBaseTables.AGENTINFO_CF_INFO_JVM, jvmInfoBoValue);
         }
 
-        hbaseTemplate.put(HBaseTables.AGENTINFO, put);
+        TableName agentInfoTableName = tableNameProvider.getTableName(HBaseTables.AGENTINFO_STR);
+        hbaseTemplate.put(agentInfoTableName, put);
     }
 }
diff --git a/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/HbaseAgentLifeCycleDao.java b/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/HbaseAgentLifeCycleDao.java
index 6890013d7..0568381fe 100644
--- a/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/HbaseAgentLifeCycleDao.java
+++ b/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/HbaseAgentLifeCycleDao.java
@@ -16,6 +16,8 @@
 
 package com.navercorp.pinpoint.collector.dao.hbase;
 
+import com.navercorp.pinpoint.common.hbase.TableNameProvider;
+import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -42,6 +44,9 @@ public class HbaseAgentLifeCycleDao implements AgentLifeCycleDao {
     private HbaseOperations2 hbaseTemplate;
 
     @Autowired
+    private TableNameProvider tableNameProvider;
+
+    @Autowired
     private AgentLifeCycleValueMapper valueMapper;
 
     @Override
@@ -60,7 +65,8 @@ public class HbaseAgentLifeCycleDao implements AgentLifeCycleDao {
 
         byte[] rowKey = createRowKey(agentId, startTimestamp, eventIdentifier);
 
-        this.hbaseTemplate.put(HBaseTables.AGENT_LIFECYCLE, rowKey, HBaseTables.AGENT_LIFECYCLE_CF_STATUS, HBaseTables.AGENT_LIFECYCLE_CF_STATUS_QUALI_STATES,
+        TableName agentLifeCycleTableName = tableNameProvider.getTableName(HBaseTables.AGENT_LIFECYCLE_STR);
+        this.hbaseTemplate.put(agentLifeCycleTableName, rowKey, HBaseTables.AGENT_LIFECYCLE_CF_STATUS, HBaseTables.AGENT_LIFECYCLE_CF_STATUS_QUALI_STATES,
                 agentLifeCycleBo, this.valueMapper);
     }
 
diff --git a/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/HbaseApiMetaDataDao.java b/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/HbaseApiMetaDataDao.java
index 00e9a0d62..42920054b 100644
--- a/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/HbaseApiMetaDataDao.java
+++ b/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/HbaseApiMetaDataDao.java
@@ -17,6 +17,7 @@
 package com.navercorp.pinpoint.collector.dao.hbase;
 
 import com.navercorp.pinpoint.collector.dao.ApiMetaDataDao;
+import com.navercorp.pinpoint.common.hbase.TableNameProvider;
 import com.navercorp.pinpoint.common.server.bo.ApiMetaDataBo;
 import com.navercorp.pinpoint.common.buffer.AutomaticBuffer;
 import com.navercorp.pinpoint.common.buffer.Buffer;
@@ -25,6 +26,7 @@ import com.navercorp.pinpoint.common.hbase.HbaseOperations2;
 import com.navercorp.pinpoint.thrift.dto.TApiMetaData;
 import com.sematext.hbase.wd.RowKeyDistributorByHashPrefix;
 
+import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.client.Put;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -45,6 +47,9 @@ public class HbaseApiMetaDataDao implements ApiMetaDataDao {
     private HbaseOperations2 hbaseTemplate;
 
     @Autowired
+    private TableNameProvider tableNameProvider;
+
+    @Autowired
     @Qualifier("metadataRowKeyDistributor")
     private RowKeyDistributorByHashPrefix rowKeyDistributorByHashPrefix;
 
@@ -77,7 +82,8 @@ public class HbaseApiMetaDataDao implements ApiMetaDataDao {
         final byte[] apiMetaDataBytes = buffer.getBuffer();
         put.addColumn(HBaseTables.API_METADATA_CF_API, HBaseTables.API_METADATA_CF_API_QUALI_SIGNATURE, apiMetaDataBytes);
 
-        hbaseTemplate.put(HBaseTables.API_METADATA, put);
+        TableName apiMetaDataTableName = tableNameProvider.getTableName(HBaseTables.API_METADATA_STR);
+        hbaseTemplate.put(apiMetaDataTableName, put);
     }
 
     private byte[] getDistributedKey(byte[] rowKey) {
diff --git a/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/HbaseApplicationIndexDao.java b/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/HbaseApplicationIndexDao.java
index 247dc2f91..3da0ce13c 100644
--- a/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/HbaseApplicationIndexDao.java
+++ b/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/HbaseApplicationIndexDao.java
@@ -20,8 +20,10 @@ import static com.navercorp.pinpoint.common.hbase.HBaseTables.*;
 
 import com.navercorp.pinpoint.collector.dao.ApplicationIndexDao;
 import com.navercorp.pinpoint.common.hbase.HbaseOperations2;
+import com.navercorp.pinpoint.common.hbase.TableNameProvider;
 import com.navercorp.pinpoint.thrift.dto.TAgentInfo;
 
+import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.client.Put;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.slf4j.Logger;
@@ -43,6 +45,9 @@ public class HbaseApplicationIndexDao implements ApplicationIndexDao {
     @Autowired
     private HbaseOperations2 hbaseTemplate;
 
+    @Autowired
+    private TableNameProvider tableNameProvider;
+
     @Override
     public void insert(final TAgentInfo agentInfo) {
         if (agentInfo == null) {
@@ -54,8 +59,9 @@ public class HbaseApplicationIndexDao implements ApplicationIndexDao {
         byte[] value = Bytes.toBytes(agentInfo.getServiceType());
         
         put.addColumn(APPLICATION_INDEX_CF_AGENTS, qualifier, value);
-        
-        hbaseTemplate.put(APPLICATION_INDEX, put);
+
+        TableName applicationIndexTableName = tableNameProvider.getTableName(APPLICATION_INDEX_STR);
+        hbaseTemplate.put(applicationIndexTableName, put);
 
         logger.debug("Insert agentInfo. {}", agentInfo);
     }
diff --git a/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/HbaseApplicationTraceIndexDao.java b/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/HbaseApplicationTraceIndexDao.java
index d4acd7397..41d9aae62 100644
--- a/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/HbaseApplicationTraceIndexDao.java
+++ b/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/HbaseApplicationTraceIndexDao.java
@@ -19,6 +19,7 @@ package com.navercorp.pinpoint.collector.dao.hbase;
 import static com.navercorp.pinpoint.common.hbase.HBaseTables.*;
 
 import com.navercorp.pinpoint.collector.dao.ApplicationTraceIndexDao;
+import com.navercorp.pinpoint.common.hbase.TableNameProvider;
 import com.navercorp.pinpoint.common.server.util.AcceptedTimeService;
 import com.navercorp.pinpoint.common.buffer.AutomaticBuffer;
 import com.navercorp.pinpoint.common.buffer.Buffer;
@@ -27,6 +28,7 @@ import com.navercorp.pinpoint.common.server.util.SpanUtils;
 import com.navercorp.pinpoint.thrift.dto.TSpan;
 import com.sematext.hbase.wd.AbstractRowKeyDistributor;
 
+import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.client.Put;
 import org.springframework.beans.factory.annotation.Autowired;
 import org.springframework.beans.factory.annotation.Qualifier;
@@ -45,6 +47,9 @@ public class HbaseApplicationTraceIndexDao implements ApplicationTraceIndexDao {
     private HbaseOperations2 hbaseTemplate;
 
     @Autowired
+    private TableNameProvider tableNameProvider;
+
+    @Autowired
     private AcceptedTimeService acceptedTimeService;
 
     @Autowired
@@ -69,9 +74,10 @@ public class HbaseApplicationTraceIndexDao implements ApplicationTraceIndexDao {
 
         put.addColumn(APPLICATION_TRACE_INDEX_CF_TRACE, makeQualifier(span) , acceptedTime, value);
 
-        boolean success = hbaseTemplate.asyncPut(APPLICATION_TRACE_INDEX, put);
+        TableName applicationTraceIndexTableName = tableNameProvider.getTableName(APPLICATION_TRACE_INDEX_STR);
+        boolean success = hbaseTemplate.asyncPut(applicationTraceIndexTableName, put);
         if (!success) {
-            hbaseTemplate.put(APPLICATION_TRACE_INDEX, put);
+            hbaseTemplate.put(applicationTraceIndexTableName, put);
         }
     }
 
diff --git a/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/HbaseHostApplicationMapDao.java b/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/HbaseHostApplicationMapDao.java
index 28f04e102..cdb87dd32 100644
--- a/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/HbaseHostApplicationMapDao.java
+++ b/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/HbaseHostApplicationMapDao.java
@@ -17,6 +17,7 @@
 package com.navercorp.pinpoint.collector.dao.hbase;
 
 import com.navercorp.pinpoint.collector.dao.HostApplicationMapDao;
+import com.navercorp.pinpoint.common.hbase.TableNameProvider;
 import com.navercorp.pinpoint.common.server.util.AcceptedTimeService;
 import com.navercorp.pinpoint.collector.util.AtomicLongUpdateMap;
 import com.navercorp.pinpoint.common.buffer.AutomaticBuffer;
@@ -26,6 +27,7 @@ import com.navercorp.pinpoint.common.hbase.HbaseOperations2;
 import com.navercorp.pinpoint.common.util.TimeSlot;
 import com.navercorp.pinpoint.common.util.TimeUtils;
 import com.sematext.hbase.wd.AbstractRowKeyDistributor;
+import org.apache.hadoop.hbase.TableName;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 import org.springframework.beans.factory.annotation.Autowired;
@@ -46,6 +48,9 @@ public class HbaseHostApplicationMapDao implements HostApplicationMapDao {
     private HbaseOperations2 hbaseTemplate;
 
     @Autowired
+    private TableNameProvider tableNameProvider;
+
+    @Autowired
     private AcceptedTimeService acceptedTimeService;
 
     @Autowired
@@ -98,11 +103,12 @@ public class HbaseHostApplicationMapDao implements HostApplicationMapDao {
 
         byte[] columnName = createColumnName(host, bindApplicationName, bindServiceType);
 
+        TableName hostApplicationMapTableName = tableNameProvider.getTableName(HBaseTables.HOST_APPLICATION_MAP_VER2_STR);
         try {
-            hbaseTemplate.put(HBaseTables.HOST_APPLICATION_MAP_VER2, rowKey, HBaseTables.HOST_APPLICATION_MAP_VER2_CF_MAP, columnName, null);
+            hbaseTemplate.put(hostApplicationMapTableName, rowKey, HBaseTables.HOST_APPLICATION_MAP_VER2_CF_MAP, columnName, null);
         } catch (Exception ex) {
             logger.warn("retry one. Caused:{}", ex.getCause(), ex);
-            hbaseTemplate.put(HBaseTables.HOST_APPLICATION_MAP_VER2, rowKey, HBaseTables.HOST_APPLICATION_MAP_VER2_CF_MAP, columnName, null);
+            hbaseTemplate.put(hostApplicationMapTableName, rowKey, HBaseTables.HOST_APPLICATION_MAP_VER2_CF_MAP, columnName, null);
         }
     }
 
diff --git a/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/HbaseMapResponseTimeDao.java b/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/HbaseMapResponseTimeDao.java
index df4b01665..5e02f266a 100644
--- a/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/HbaseMapResponseTimeDao.java
+++ b/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/HbaseMapResponseTimeDao.java
@@ -16,10 +16,9 @@
 
 package com.navercorp.pinpoint.collector.dao.hbase;
 
-import com.google.common.util.concurrent.AtomicLongMap;
 import com.navercorp.pinpoint.collector.dao.MapResponseTimeDao;
 import com.navercorp.pinpoint.collector.dao.hbase.statistics.*;
-import com.navercorp.pinpoint.collector.util.AtomicLongMapUtils;
+import com.navercorp.pinpoint.common.hbase.TableNameProvider;
 import com.navercorp.pinpoint.common.server.util.AcceptedTimeService;
 import com.navercorp.pinpoint.common.hbase.HbaseOperations2;
 import com.navercorp.pinpoint.common.trace.ServiceType;
@@ -27,6 +26,7 @@ import com.navercorp.pinpoint.common.util.ApplicationMapStatisticsUtils;
 import com.navercorp.pinpoint.common.util.TimeSlot;
 
 import com.sematext.hbase.wd.RowKeyDistributorByHashPrefix;
+import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.client.Increment;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -45,6 +45,7 @@ import static com.navercorp.pinpoint.common.hbase.HBaseTables.*;
  * @author netspider
  * @author emeroad
  * @author jaehong.kim
+ * @author HyunGil Jeong
  */
 @Repository
 public class HbaseMapResponseTimeDao implements MapResponseTimeDao {
@@ -55,14 +56,17 @@ public class HbaseMapResponseTimeDao implements MapResponseTimeDao {
     private HbaseOperations2 hbaseTemplate;
 
     @Autowired
+    private TableNameProvider tableNameProvider;
+
+    @Autowired
     private AcceptedTimeService acceptedTimeService;
 
     @Autowired
     private TimeSlot timeSlot;
 
     @Autowired
-    @Qualifier("selfMerge")
-    private RowKeyMerge rowKeyMerge;
+    @Qualifier("selfBulkIncrementer")
+    private BulkIncrementer bulkIncrementer;
 
     @Autowired
     @Qualifier("statisticsSelfRowKeyDistributor")
@@ -70,8 +74,6 @@ public class HbaseMapResponseTimeDao implements MapResponseTimeDao {
 
     private final boolean useBulk;
 
-    private final AtomicLongMap<RowInfo> counter = AtomicLongMap.create();
-
     public HbaseMapResponseTimeDao() {
         this(true);
     }
@@ -101,8 +103,8 @@ public class HbaseMapResponseTimeDao implements MapResponseTimeDao {
         final short slotNumber = ApplicationMapStatisticsUtils.getSlotNumber(applicationServiceType, elapsed, isError);
         final ColumnName selfColumnName = new ResponseColumnName(agentId, slotNumber);
         if (useBulk) {
-            RowInfo rowInfo = new DefaultRowInfo(selfRowKey, selfColumnName);
-            this.counter.incrementAndGet(rowInfo);
+            TableName mapStatisticsSelfTableName = tableNameProvider.getTableName(MAP_STATISTICS_SELF_VER2_STR);
+            bulkIncrementer.increment(mapStatisticsSelfTableName, selfRowKey, selfColumnName);
         } else {
             final byte[] rowKey = getDistributedKey(selfRowKey.getRowKey());
             // column name is the name of caller app.
@@ -118,7 +120,8 @@ public class HbaseMapResponseTimeDao implements MapResponseTimeDao {
         if (columnName == null) {
             throw new NullPointerException("columnName must not be null");
         }
-        hbaseTemplate.incrementColumnValue(MAP_STATISTICS_SELF_VER2, rowKey, MAP_STATISTICS_SELF_VER2_CF_COUNTER, columnName, increment);
+        TableName mapStatisticsSelfTableName = tableNameProvider.getTableName(MAP_STATISTICS_SELF_VER2_STR);
+        hbaseTemplate.incrementColumnValue(mapStatisticsSelfTableName, rowKey, MAP_STATISTICS_SELF_VER2_CF_COUNTER, columnName, increment);
     }
 
 
@@ -128,18 +131,15 @@ public class HbaseMapResponseTimeDao implements MapResponseTimeDao {
             throw new IllegalStateException("useBulk is " + useBulk);
         }
 
-        // update statistics by rowkey and column for now. need to update it by rowkey later.
-        final Map<RowInfo, Long> remove = AtomicLongMapUtils.remove(this.counter);
-
-        final List<Increment> merge = rowKeyMerge.createBulkIncrement(remove, rowKeyDistributorByHashPrefix);
-        if (merge.isEmpty()) {
-            return;
-        }
-
-        if (logger.isDebugEnabled()) {
-            logger.debug("flush {} Increment:{}", this.getClass().getSimpleName(), merge.size());
+        Map<TableName, List<Increment>> incrementMap = bulkIncrementer.getIncrements(rowKeyDistributorByHashPrefix);
+        for (Map.Entry<TableName, List<Increment>> e : incrementMap.entrySet()) {
+            TableName tableName = e.getKey();
+            List<Increment> increments = e.getValue();
+            if (logger.isDebugEnabled()) {
+                logger.debug("flush {} to [{}] Increment:{}", this.getClass().getSimpleName(), tableName.getNameAsString(), increments.size());
+            }
+            hbaseTemplate.increment(tableName, increments);
         }
-        hbaseTemplate.increment(MAP_STATISTICS_SELF_VER2, merge);
     }
 
     private byte[] getDistributedKey(byte[] rowKey) {
diff --git a/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/HbaseMapStatisticsCalleeDao.java b/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/HbaseMapStatisticsCalleeDao.java
index ac9662ddf..e25fa2c17 100644
--- a/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/HbaseMapStatisticsCalleeDao.java
+++ b/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/HbaseMapStatisticsCalleeDao.java
@@ -18,10 +18,9 @@ package com.navercorp.pinpoint.collector.dao.hbase;
 
 import static com.navercorp.pinpoint.common.hbase.HBaseTables.*;
 
-import com.google.common.util.concurrent.AtomicLongMap;
 import com.navercorp.pinpoint.collector.dao.MapStatisticsCalleeDao;
 import com.navercorp.pinpoint.collector.dao.hbase.statistics.*;
-import com.navercorp.pinpoint.collector.util.AtomicLongMapUtils;
+import com.navercorp.pinpoint.common.hbase.TableNameProvider;
 import com.navercorp.pinpoint.common.server.util.AcceptedTimeService;
 import com.navercorp.pinpoint.common.hbase.HbaseOperations2;
 import com.navercorp.pinpoint.common.trace.ServiceType;
@@ -30,6 +29,7 @@ import com.navercorp.pinpoint.common.util.TimeSlot;
 
 import com.sematext.hbase.wd.RowKeyDistributorByHashPrefix;
 import org.apache.commons.lang3.StringUtils;
+import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.client.Increment;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -45,6 +45,7 @@ import java.util.Map;
  * 
  * @author netspider
  * @author emeroad
+ * @author HyunGil Jeong
  */
 @Repository
 public class HbaseMapStatisticsCalleeDao implements MapStatisticsCalleeDao {
@@ -55,14 +56,17 @@ public class HbaseMapStatisticsCalleeDao implements MapStatisticsCalleeDao {
     private HbaseOperations2 hbaseTemplate;
 
     @Autowired
+    private TableNameProvider tableNameProvider;
+
+    @Autowired
     private AcceptedTimeService acceptedTimeService;
 
     @Autowired
     private TimeSlot timeSlot;
 
     @Autowired
-    @Qualifier("calleeMerge")
-    private RowKeyMerge rowKeyMerge;
+    @Qualifier("calleeBulkIncrementer")
+    private BulkIncrementer bulkIncrementer;
 
     @Autowired
     @Qualifier("statisticsCalleeRowKeyDistributor")
@@ -70,8 +74,6 @@ public class HbaseMapStatisticsCalleeDao implements MapStatisticsCalleeDao {
 
     private final boolean useBulk;
 
-    private final AtomicLongMap<RowInfo> counter = AtomicLongMap.create();
-
     public HbaseMapStatisticsCalleeDao() {
         this(true);
     }
@@ -80,7 +82,6 @@ public class HbaseMapStatisticsCalleeDao implements MapStatisticsCalleeDao {
         this.useBulk = useBulk;
     }
 
-
     @Override
     public void update(String calleeApplicationName, ServiceType calleeServiceType, String callerApplicationName, ServiceType callerServiceType, String callerHost, int elapsed, boolean isError) {
         if (callerApplicationName == null) {
@@ -107,8 +108,8 @@ public class HbaseMapStatisticsCalleeDao implements MapStatisticsCalleeDao {
         final ColumnName callerColumnName = new CallerColumnName(callerServiceType.getCode(), callerApplicationName, callerHost, callerSlotNumber);
 
         if (useBulk) {
-            RowInfo rowInfo = new DefaultRowInfo(calleeRowKey, callerColumnName);
-            counter.incrementAndGet(rowInfo);
+            TableName mapStatisticsCallerTableName = tableNameProvider.getTableName(MAP_STATISTICS_CALLER_VER2_STR);
+            bulkIncrementer.increment(mapStatisticsCallerTableName, calleeRowKey, callerColumnName);
         } else {
             final byte[] rowKey = getDistributedKey(calleeRowKey.getRowKey());
 
@@ -125,7 +126,8 @@ public class HbaseMapStatisticsCalleeDao implements MapStatisticsCalleeDao {
         if (columnName == null) {
             throw new NullPointerException("columnName must not be null");
         }
-        hbaseTemplate.incrementColumnValue(MAP_STATISTICS_CALLER_VER2, rowKey, MAP_STATISTICS_CALLER_VER2_CF_COUNTER, columnName, increment);
+        TableName mapStatisticsCallerTableName = tableNameProvider.getTableName(MAP_STATISTICS_CALLER_VER2_STR);
+        hbaseTemplate.incrementColumnValue(mapStatisticsCallerTableName, rowKey, MAP_STATISTICS_CALLER_VER2_CF_COUNTER, columnName, increment);
     }
 
     @Override
@@ -134,17 +136,16 @@ public class HbaseMapStatisticsCalleeDao implements MapStatisticsCalleeDao {
             throw new IllegalStateException();
         }
 
-        final Map<RowInfo, Long> remove = AtomicLongMapUtils.remove(this.counter);
+        Map<TableName, List<Increment>> incrementMap = bulkIncrementer.getIncrements(rowKeyDistributorByHashPrefix);
 
-        final List<Increment> merge = rowKeyMerge.createBulkIncrement(remove, rowKeyDistributorByHashPrefix);
-        if (merge.isEmpty()) {
-            return;
-        }
-
-        if (logger.isDebugEnabled()) {
-            logger.debug("flush {} Increment:{}", this.getClass().getSimpleName(), merge.size());
+        for (Map.Entry<TableName, List<Increment>> e : incrementMap.entrySet()) {
+            TableName tableName = e.getKey();
+            List<Increment> increments = e.getValue();
+            if (logger.isDebugEnabled()) {
+                logger.debug("flush {} to [{}] Increment:{}", this.getClass().getSimpleName(), tableName.getNameAsString(), increments.size());
+            }
+            hbaseTemplate.increment(tableName, increments);
         }
-        hbaseTemplate.increment(MAP_STATISTICS_CALLER_VER2, merge);
 
     }
 
diff --git a/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/HbaseMapStatisticsCallerDao.java b/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/HbaseMapStatisticsCallerDao.java
index 033beab9b..e798c2746 100644
--- a/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/HbaseMapStatisticsCallerDao.java
+++ b/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/HbaseMapStatisticsCallerDao.java
@@ -18,10 +18,9 @@ package com.navercorp.pinpoint.collector.dao.hbase;
 
 import static com.navercorp.pinpoint.common.hbase.HBaseTables.*;
 
-import com.google.common.util.concurrent.AtomicLongMap;
 import com.navercorp.pinpoint.collector.dao.MapStatisticsCallerDao;
 import com.navercorp.pinpoint.collector.dao.hbase.statistics.*;
-import com.navercorp.pinpoint.collector.util.AtomicLongMapUtils;
+import com.navercorp.pinpoint.common.hbase.TableNameProvider;
 import com.navercorp.pinpoint.common.server.util.AcceptedTimeService;
 import com.navercorp.pinpoint.common.hbase.HbaseOperations2;
 import com.navercorp.pinpoint.common.trace.ServiceType;
@@ -30,6 +29,7 @@ import com.navercorp.pinpoint.common.util.TimeSlot;
 
 import com.sematext.hbase.wd.RowKeyDistributorByHashPrefix;
 import org.apache.commons.lang3.StringUtils;
+import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.client.Increment;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -45,6 +45,7 @@ import java.util.Map;
  * 
  * @author netspider
  * @author emeroad
+ * @author HyunGil Jeong
  */
 @Repository
 public class HbaseMapStatisticsCallerDao implements MapStatisticsCallerDao {
@@ -55,23 +56,24 @@ public class HbaseMapStatisticsCallerDao implements MapStatisticsCallerDao {
     private HbaseOperations2 hbaseTemplate;
 
     @Autowired
+    private TableNameProvider tableNameProvider;
+
+    @Autowired
     private AcceptedTimeService acceptedTimeService;
 
     @Autowired
-    @Qualifier("callerMerge")
-    private RowKeyMerge rowKeyMerge;
+    private TimeSlot timeSlot;
 
     @Autowired
-    @Qualifier("statisticsCallerRowKeyDistributor")
-    private RowKeyDistributorByHashPrefix rowKeyDistributorByHashPrefix;
+    @Qualifier("callerBulkIncrementer")
+    private BulkIncrementer bulkIncrementer;
 
     @Autowired
-    private TimeSlot timeSlot;
+    @Qualifier("statisticsCallerRowKeyDistributor")
+    private RowKeyDistributorByHashPrefix rowKeyDistributorByHashPrefix;
 
     private final boolean useBulk;
 
-    private final AtomicLongMap<RowInfo> counter = AtomicLongMap.create();
-
     public HbaseMapStatisticsCallerDao() {
         this(true);
     }
@@ -105,8 +107,8 @@ public class HbaseMapStatisticsCallerDao implements MapStatisticsCallerDao {
         final short calleeSlotNumber = ApplicationMapStatisticsUtils.getSlotNumber(calleeServiceType, elapsed, isError);
         final ColumnName calleeColumnName = new CalleeColumnName(callerAgentid, calleeServiceType.getCode(), calleeApplicationName, calleeHost, calleeSlotNumber);
         if (useBulk) {
-            RowInfo rowInfo = new DefaultRowInfo(callerRowKey, calleeColumnName);
-            this.counter.incrementAndGet(rowInfo);
+            TableName mapStatisticsCalleeTableName = tableNameProvider.getTableName(MAP_STATISTICS_CALLEE_VER2_STR);
+            bulkIncrementer.increment(mapStatisticsCalleeTableName, callerRowKey, calleeColumnName);
         } else {
             final byte[] rowKey = getDistributedKey(callerRowKey.getRowKey());
             // column name is the name of caller app.
@@ -122,7 +124,8 @@ public class HbaseMapStatisticsCallerDao implements MapStatisticsCallerDao {
         if (columnName == null) {
             throw new NullPointerException("columnName must not be null");
         }
-        hbaseTemplate.incrementColumnValue(MAP_STATISTICS_CALLEE_VER2, rowKey, MAP_STATISTICS_CALLEE_VER2_CF_COUNTER, columnName, increment);
+        TableName mapStatisticsCalleeTableName = tableNameProvider.getTableName(MAP_STATISTICS_CALLEE_VER2_STR);
+        hbaseTemplate.incrementColumnValue(mapStatisticsCalleeTableName, rowKey, MAP_STATISTICS_CALLEE_VER2_CF_COUNTER, columnName, increment);
     }
 
     @Override
@@ -131,17 +134,16 @@ public class HbaseMapStatisticsCallerDao implements MapStatisticsCallerDao {
             throw new IllegalStateException();
         }
         // update statistics by rowkey and column for now. need to update it by rowkey later.
-        final Map<RowInfo, Long> remove = AtomicLongMapUtils.remove(this.counter);
-
-        final List<Increment> merge = rowKeyMerge.createBulkIncrement(remove, rowKeyDistributorByHashPrefix);
-        if (merge.isEmpty()) {
-            return;
-        }
-
-        if (logger.isDebugEnabled()) {
-            logger.debug("flush {} Increment:{}", this.getClass().getSimpleName(), merge.size());
+        Map<TableName, List<Increment>> incrementMap = bulkIncrementer.getIncrements(rowKeyDistributorByHashPrefix);
+
+        for (Map.Entry<TableName, List<Increment>> e : incrementMap.entrySet()) {
+            TableName tableName = e.getKey();
+            List<Increment> increments = e.getValue();
+            if (logger.isDebugEnabled()) {
+                logger.debug("flush {} to [{}] Increment:{}", this.getClass().getSimpleName(), tableName.getNameAsString(), increments.size());
+            }
+            hbaseTemplate.increment(tableName, increments);
         }
-        hbaseTemplate.increment(MAP_STATISTICS_CALLEE_VER2, merge);
     }
 
     private byte[] getDistributedKey(byte[] rowKey) {
diff --git a/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/HbaseSqlMetaDataDao.java b/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/HbaseSqlMetaDataDao.java
index 1ff7c1a91..dc6305b94 100644
--- a/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/HbaseSqlMetaDataDao.java
+++ b/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/HbaseSqlMetaDataDao.java
@@ -17,12 +17,14 @@
 package com.navercorp.pinpoint.collector.dao.hbase;
 
 import com.navercorp.pinpoint.collector.dao.SqlMetaDataDao;
+import com.navercorp.pinpoint.common.hbase.TableNameProvider;
 import com.navercorp.pinpoint.common.server.bo.SqlMetaDataBo;
 import com.navercorp.pinpoint.common.hbase.HBaseTables;
 import com.navercorp.pinpoint.common.hbase.HbaseOperations2;
 import com.navercorp.pinpoint.thrift.dto.TSqlMetaData;
 import com.sematext.hbase.wd.RowKeyDistributorByHashPrefix;
 
+import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.client.Put;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.slf4j.Logger;
@@ -42,6 +44,9 @@ public class HbaseSqlMetaDataDao implements SqlMetaDataDao {
     private HbaseOperations2 hbaseTemplate;
 
     @Autowired
+    private TableNameProvider tableNameProvider;
+
+    @Autowired
     @Qualifier("metadataRowKeyDistributor2")
     private RowKeyDistributorByHashPrefix rowKeyDistributorByHashPrefix;
 
@@ -64,7 +69,8 @@ public class HbaseSqlMetaDataDao implements SqlMetaDataDao {
 
         put.addColumn(HBaseTables.SQL_METADATA_VER2_CF_SQL, HBaseTables.SQL_METADATA_VER2_CF_SQL_QUALI_SQLSTATEMENT, sqlBytes);
 
-        hbaseTemplate.put(HBaseTables.SQL_METADATA_VER2, put);
+        TableName sqlMetaDataTableName = tableNameProvider.getTableName(HBaseTables.SQL_METADATA_VER2_STR);
+        hbaseTemplate.put(sqlMetaDataTableName, put);
     }
 
     private byte[] getDistributedKey(byte[] rowKey) {
diff --git a/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/HbaseStringMetaDataDao.java b/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/HbaseStringMetaDataDao.java
index ce9dd0987..11f26a9ef 100644
--- a/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/HbaseStringMetaDataDao.java
+++ b/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/HbaseStringMetaDataDao.java
@@ -17,12 +17,14 @@
 package com.navercorp.pinpoint.collector.dao.hbase;
 
 import com.navercorp.pinpoint.collector.dao.StringMetaDataDao;
+import com.navercorp.pinpoint.common.hbase.TableNameProvider;
 import com.navercorp.pinpoint.common.server.bo.StringMetaDataBo;
 import com.navercorp.pinpoint.common.hbase.HBaseTables;
 import com.navercorp.pinpoint.common.hbase.HbaseOperations2;
 import com.navercorp.pinpoint.thrift.dto.TStringMetaData;
 import com.sematext.hbase.wd.RowKeyDistributorByHashPrefix;
 
+import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.client.Put;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.slf4j.Logger;
@@ -44,6 +46,9 @@ public class HbaseStringMetaDataDao implements StringMetaDataDao {
     private HbaseOperations2 hbaseTemplate;
 
     @Autowired
+    private TableNameProvider tableNameProvider;
+
+    @Autowired
     @Qualifier("metadataRowKeyDistributor")
     private RowKeyDistributorByHashPrefix rowKeyDistributorByHashPrefix;
 
@@ -65,7 +70,8 @@ public class HbaseStringMetaDataDao implements StringMetaDataDao {
         byte[] sqlBytes = Bytes.toBytes(stringValue);
         put.addColumn(HBaseTables.STRING_METADATA_CF_STR, HBaseTables.STRING_METADATA_CF_STR_QUALI_STRING, sqlBytes);
 
-        hbaseTemplate.put(HBaseTables.STRING_METADATA, put);
+        TableName stringMetaDataTableName = tableNameProvider.getTableName(HBaseTables.STRING_METADATA_STR);
+        hbaseTemplate.put(stringMetaDataTableName, put);
     }
 
     private byte[] getDistributedKey(byte[] rowKey) {
diff --git a/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/HbaseTraceDaoV2.java b/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/HbaseTraceDaoV2.java
index bdb85d80b..bc2ed504f 100644
--- a/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/HbaseTraceDaoV2.java
+++ b/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/HbaseTraceDaoV2.java
@@ -2,6 +2,7 @@ package com.navercorp.pinpoint.collector.dao.hbase;
 
 import com.navercorp.pinpoint.collector.dao.TraceDao;
 import com.navercorp.pinpoint.common.hbase.HbaseOperations2;
+import com.navercorp.pinpoint.common.hbase.TableNameProvider;
 import com.navercorp.pinpoint.common.server.bo.SpanBo;
 import com.navercorp.pinpoint.common.server.bo.SpanChunkBo;
 import com.navercorp.pinpoint.common.server.bo.SpanEventBo;
@@ -10,6 +11,7 @@ import com.navercorp.pinpoint.common.server.bo.serializer.trace.v2.SpanChunkSeri
 import com.navercorp.pinpoint.common.server.bo.serializer.trace.v2.SpanSerializerV2;
 import com.navercorp.pinpoint.common.util.TransactionId;
 import org.apache.commons.collections.CollectionUtils;
+import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.client.Put;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -19,7 +21,7 @@ import org.springframework.stereotype.Repository;
 
 import java.util.List;
 
-import static com.navercorp.pinpoint.common.hbase.HBaseTables.TRACE_V2;
+import static com.navercorp.pinpoint.common.hbase.HBaseTables.TRACE_V2_STR;
 
 /**
  * @author Woonduk Kang(emeroad)
@@ -32,6 +34,8 @@ public class HbaseTraceDaoV2 implements TraceDao {
     @Autowired
     private HbaseOperations2 hbaseTemplate;
 
+    @Autowired
+    private TableNameProvider tableNameProvider;
 
     @Autowired
     private SpanSerializerV2 spanSerializer;
@@ -59,10 +63,10 @@ public class HbaseTraceDaoV2 implements TraceDao {
 
         this.spanSerializer.serialize(spanBo, put, null);
 
-
-        boolean success = hbaseTemplate.asyncPut(TRACE_V2, put);
+        TableName traceTableName = tableNameProvider.getTableName(TRACE_V2_STR);
+        boolean success = hbaseTemplate.asyncPut(traceTableName, put);
         if (!success) {
-            hbaseTemplate.put(TRACE_V2, put);
+            hbaseTemplate.put(traceTableName, put);
         }
 
     }
@@ -86,9 +90,10 @@ public class HbaseTraceDaoV2 implements TraceDao {
         this.spanChunkSerializer.serialize(spanChunkBo, put, null);
 
         if (!put.isEmpty()) {
-            boolean success = hbaseTemplate.asyncPut(TRACE_V2, put);
+            TableName traceTableName = tableNameProvider.getTableName(TRACE_V2_STR);
+            boolean success = hbaseTemplate.asyncPut(traceTableName, put);
             if (!success) {
-                hbaseTemplate.put(TRACE_V2, put);
+                hbaseTemplate.put(traceTableName, put);
             }
         }
     }
diff --git a/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/stat/HbaseActiveTraceDao.java b/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/stat/HbaseActiveTraceDao.java
index f5d23da8c..c541cf324 100644
--- a/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/stat/HbaseActiveTraceDao.java
+++ b/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/stat/HbaseActiveTraceDao.java
@@ -19,11 +19,13 @@ package com.navercorp.pinpoint.collector.dao.hbase.stat;
 import com.navercorp.pinpoint.collector.dao.AgentStatDaoV2;
 import com.navercorp.pinpoint.common.hbase.HBaseTables;
 import com.navercorp.pinpoint.common.hbase.HbaseOperations2;
+import com.navercorp.pinpoint.common.hbase.TableNameProvider;
 import com.navercorp.pinpoint.common.server.bo.serializer.stat.ActiveTraceSerializer;
 import com.navercorp.pinpoint.common.server.bo.serializer.stat.AgentStatHbaseOperationFactory;
 import com.navercorp.pinpoint.common.server.bo.stat.ActiveTraceBo;
 import com.navercorp.pinpoint.common.server.bo.stat.AgentStatType;
 import org.apache.commons.collections.CollectionUtils;
+import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.client.Put;
 import org.springframework.beans.factory.annotation.Autowired;
 import org.springframework.stereotype.Repository;
@@ -40,6 +42,9 @@ public class HbaseActiveTraceDao implements AgentStatDaoV2<ActiveTraceBo> {
     private HbaseOperations2 hbaseTemplate;
 
     @Autowired
+    private TableNameProvider tableNameProvider;
+
+    @Autowired
     private AgentStatHbaseOperationFactory agentStatHbaseOperationFactory;
 
     @Autowired
@@ -55,9 +60,10 @@ public class HbaseActiveTraceDao implements AgentStatDaoV2<ActiveTraceBo> {
         }
         List<Put> activeTracePuts = this.agentStatHbaseOperationFactory.createPuts(agentId, AgentStatType.ACTIVE_TRACE, agentStatDataPoints, this.activeTraceSerializer);
         if (!activeTracePuts.isEmpty()) {
-            List<Put> rejectedPuts = this.hbaseTemplate.asyncPut(HBaseTables.AGENT_STAT_VER2, activeTracePuts);
+            TableName agentStatTableName = tableNameProvider.getTableName(HBaseTables.AGENT_STAT_VER2_STR);
+            List<Put> rejectedPuts = this.hbaseTemplate.asyncPut(agentStatTableName, activeTracePuts);
             if (CollectionUtils.isNotEmpty(rejectedPuts)) {
-                this.hbaseTemplate.put(HBaseTables.AGENT_STAT_VER2, rejectedPuts);
+                this.hbaseTemplate.put(agentStatTableName, rejectedPuts);
             }
         }
     }
diff --git a/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/stat/HbaseCpuLoadDao.java b/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/stat/HbaseCpuLoadDao.java
index 3a8e15c29..849e08aa5 100644
--- a/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/stat/HbaseCpuLoadDao.java
+++ b/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/stat/HbaseCpuLoadDao.java
@@ -19,11 +19,13 @@ package com.navercorp.pinpoint.collector.dao.hbase.stat;
 import com.navercorp.pinpoint.collector.dao.AgentStatDaoV2;
 import com.navercorp.pinpoint.common.hbase.HBaseTables;
 import com.navercorp.pinpoint.common.hbase.HbaseOperations2;
+import com.navercorp.pinpoint.common.hbase.TableNameProvider;
 import com.navercorp.pinpoint.common.server.bo.serializer.stat.AgentStatHbaseOperationFactory;
 import com.navercorp.pinpoint.common.server.bo.serializer.stat.CpuLoadSerializer;
 import com.navercorp.pinpoint.common.server.bo.stat.AgentStatType;
 import com.navercorp.pinpoint.common.server.bo.stat.CpuLoadBo;
 import org.apache.commons.collections.CollectionUtils;
+import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.client.Put;
 import org.springframework.beans.factory.annotation.Autowired;
 import org.springframework.stereotype.Repository;
@@ -40,6 +42,9 @@ public class HbaseCpuLoadDao implements AgentStatDaoV2<CpuLoadBo> {
     private HbaseOperations2 hbaseTemplate;
 
     @Autowired
+    private TableNameProvider tableNameProvider;
+
+    @Autowired
     private AgentStatHbaseOperationFactory agentStatHbaseOperationFactory;
 
     @Autowired
@@ -55,9 +60,10 @@ public class HbaseCpuLoadDao implements AgentStatDaoV2<CpuLoadBo> {
         }
         List<Put> cpuLoadPuts = this.agentStatHbaseOperationFactory.createPuts(agentId, AgentStatType.CPU_LOAD, cpuLoadBos, this.cpuLoadSerializer);
         if (!cpuLoadPuts.isEmpty()) {
-            List<Put> rejectedPuts = this.hbaseTemplate.asyncPut(HBaseTables.AGENT_STAT_VER2, cpuLoadPuts);
+            TableName agentStatTableName = tableNameProvider.getTableName(HBaseTables.AGENT_STAT_VER2_STR);
+            List<Put> rejectedPuts = this.hbaseTemplate.asyncPut(agentStatTableName, cpuLoadPuts);
             if (CollectionUtils.isNotEmpty(rejectedPuts)) {
-                this.hbaseTemplate.put(HBaseTables.AGENT_STAT_VER2, rejectedPuts);
+                this.hbaseTemplate.put(agentStatTableName, rejectedPuts);
             }
         }
     }
diff --git a/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/stat/HbaseDataSourceListDao.java b/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/stat/HbaseDataSourceListDao.java
index bce4b631d..ed6d6fd01 100644
--- a/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/stat/HbaseDataSourceListDao.java
+++ b/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/stat/HbaseDataSourceListDao.java
@@ -19,6 +19,7 @@ package com.navercorp.pinpoint.collector.dao.hbase.stat;
 import com.navercorp.pinpoint.collector.dao.AgentStatDaoV2;
 import com.navercorp.pinpoint.common.hbase.HBaseTables;
 import com.navercorp.pinpoint.common.hbase.HbaseOperations2;
+import com.navercorp.pinpoint.common.hbase.TableNameProvider;
 import com.navercorp.pinpoint.common.server.bo.serializer.stat.AgentStatHbaseOperationFactory;
 import com.navercorp.pinpoint.common.server.bo.serializer.stat.AgentStatUtils;
 import com.navercorp.pinpoint.common.server.bo.serializer.stat.DataSourceSerializer;
@@ -27,6 +28,7 @@ import com.navercorp.pinpoint.common.server.bo.stat.DataSourceBo;
 import com.navercorp.pinpoint.common.server.bo.stat.DataSourceListBo;
 import com.navercorp.pinpoint.common.util.CollectionUtils;
 import org.apache.commons.collections.map.MultiKeyMap;
+import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.client.Put;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -49,6 +51,9 @@ public class HbaseDataSourceListDao implements AgentStatDaoV2<DataSourceListBo>
     private HbaseOperations2 hbaseTemplate;
 
     @Autowired
+    private TableNameProvider tableNameProvider;
+
+    @Autowired
     private AgentStatHbaseOperationFactory agentStatHbaseOperationFactory;
 
     @Autowired
@@ -66,9 +71,10 @@ public class HbaseDataSourceListDao implements AgentStatDaoV2<DataSourceListBo>
         List<DataSourceListBo> reorderedDataSourceListBos = reorderDataSourceListBos(dataSourceListBos);
         List<Put> activeTracePuts = this.agentStatHbaseOperationFactory.createPuts(agentId, AgentStatType.DATASOURCE, reorderedDataSourceListBos, dataSourceSerializer);
         if (!activeTracePuts.isEmpty()) {
-            List<Put> rejectedPuts = this.hbaseTemplate.asyncPut(HBaseTables.AGENT_STAT_VER2, activeTracePuts);
+            TableName agentStatTableName = tableNameProvider.getTableName(HBaseTables.AGENT_STAT_VER2_STR);
+            List<Put> rejectedPuts = this.hbaseTemplate.asyncPut(agentStatTableName, activeTracePuts);
             if (CollectionUtils.hasLength(rejectedPuts)) {
-                this.hbaseTemplate.put(HBaseTables.AGENT_STAT_VER2, rejectedPuts);
+                this.hbaseTemplate.put(agentStatTableName, rejectedPuts);
             }
         }
     }
diff --git a/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/stat/HbaseDeadlockDao.java b/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/stat/HbaseDeadlockDao.java
index e4a34e827..7171acba4 100644
--- a/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/stat/HbaseDeadlockDao.java
+++ b/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/stat/HbaseDeadlockDao.java
@@ -19,11 +19,13 @@ package com.navercorp.pinpoint.collector.dao.hbase.stat;
 import com.navercorp.pinpoint.collector.dao.AgentStatDaoV2;
 import com.navercorp.pinpoint.common.hbase.HBaseTables;
 import com.navercorp.pinpoint.common.hbase.HbaseOperations2;
+import com.navercorp.pinpoint.common.hbase.TableNameProvider;
 import com.navercorp.pinpoint.common.server.bo.serializer.stat.AgentStatHbaseOperationFactory;
 import com.navercorp.pinpoint.common.server.bo.serializer.stat.DeadlockSerializer;
 import com.navercorp.pinpoint.common.server.bo.stat.AgentStatType;
 import com.navercorp.pinpoint.common.server.bo.stat.DeadlockBo;
 import org.apache.commons.collections.CollectionUtils;
+import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.client.Put;
 import org.springframework.beans.factory.annotation.Autowired;
 import org.springframework.stereotype.Repository;
@@ -40,6 +42,9 @@ public class HbaseDeadlockDao implements AgentStatDaoV2<DeadlockBo> {
     private HbaseOperations2 hbaseTemplate;
 
     @Autowired
+    private TableNameProvider tableNameProvider;
+
+    @Autowired
     private AgentStatHbaseOperationFactory agentStatHbaseOperationFactory;
 
     @Autowired
@@ -55,9 +60,10 @@ public class HbaseDeadlockDao implements AgentStatDaoV2<DeadlockBo> {
         }
         List<Put> deadlockPuts = this.agentStatHbaseOperationFactory.createPuts(agentId, AgentStatType.DEADLOCK, deadlockBos, this.deadlockSerializer);
         if (!deadlockPuts.isEmpty()) {
-            List<Put> rejectedPuts = this.hbaseTemplate.asyncPut(HBaseTables.AGENT_STAT_VER2, deadlockPuts);
+            TableName agentStatTableName = tableNameProvider.getTableName(HBaseTables.AGENT_STAT_VER2_STR);
+            List<Put> rejectedPuts = this.hbaseTemplate.asyncPut(agentStatTableName, deadlockPuts);
             if (CollectionUtils.isNotEmpty(rejectedPuts)) {
-                this.hbaseTemplate.put(HBaseTables.AGENT_STAT_VER2, rejectedPuts);
+                this.hbaseTemplate.put(agentStatTableName, rejectedPuts);
             }
         }
     }
diff --git a/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/stat/HbaseJvmGcDao.java b/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/stat/HbaseJvmGcDao.java
index 801d52b5f..1010adc28 100644
--- a/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/stat/HbaseJvmGcDao.java
+++ b/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/stat/HbaseJvmGcDao.java
@@ -19,11 +19,13 @@ package com.navercorp.pinpoint.collector.dao.hbase.stat;
 import com.navercorp.pinpoint.collector.dao.AgentStatDaoV2;
 import com.navercorp.pinpoint.common.hbase.HBaseTables;
 import com.navercorp.pinpoint.common.hbase.HbaseOperations2;
+import com.navercorp.pinpoint.common.hbase.TableNameProvider;
 import com.navercorp.pinpoint.common.server.bo.serializer.stat.AgentStatHbaseOperationFactory;
 import com.navercorp.pinpoint.common.server.bo.serializer.stat.JvmGcSerializer;
 import com.navercorp.pinpoint.common.server.bo.stat.AgentStatType;
 import com.navercorp.pinpoint.common.server.bo.stat.JvmGcBo;
 import org.apache.commons.collections.CollectionUtils;
+import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.client.Put;
 import org.springframework.beans.factory.annotation.Autowired;
 import org.springframework.stereotype.Repository;
@@ -40,6 +42,9 @@ public class HbaseJvmGcDao implements AgentStatDaoV2<JvmGcBo> {
     private HbaseOperations2 hbaseTemplate;
 
     @Autowired
+    private TableNameProvider tableNameProvider;
+
+    @Autowired
     private AgentStatHbaseOperationFactory agentStatHbaseOperationFactory;
 
     @Autowired
@@ -55,9 +60,10 @@ public class HbaseJvmGcDao implements AgentStatDaoV2<JvmGcBo> {
         }
         List<Put> jvmGcBoPuts = this.agentStatHbaseOperationFactory.createPuts(agentId, AgentStatType.JVM_GC, jvmGcBos, this.jvmGcSerializer);
         if (!jvmGcBoPuts.isEmpty()) {
-            List<Put> rejectedPuts = this.hbaseTemplate.asyncPut(HBaseTables.AGENT_STAT_VER2, jvmGcBoPuts);
+            TableName agentStatTableName = tableNameProvider.getTableName(HBaseTables.AGENT_STAT_VER2_STR);
+            List<Put> rejectedPuts = this.hbaseTemplate.asyncPut(agentStatTableName, jvmGcBoPuts);
             if (CollectionUtils.isNotEmpty(rejectedPuts)) {
-                this.hbaseTemplate.put(HBaseTables.AGENT_STAT_VER2, rejectedPuts);
+                this.hbaseTemplate.put(agentStatTableName, rejectedPuts);
             }
         }
     }
diff --git a/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/stat/HbaseJvmGcDetailedDao.java b/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/stat/HbaseJvmGcDetailedDao.java
index 62265691f..5649d5972 100644
--- a/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/stat/HbaseJvmGcDetailedDao.java
+++ b/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/stat/HbaseJvmGcDetailedDao.java
@@ -19,11 +19,13 @@ package com.navercorp.pinpoint.collector.dao.hbase.stat;
 import com.navercorp.pinpoint.collector.dao.AgentStatDaoV2;
 import com.navercorp.pinpoint.common.hbase.HBaseTables;
 import com.navercorp.pinpoint.common.hbase.HbaseOperations2;
+import com.navercorp.pinpoint.common.hbase.TableNameProvider;
 import com.navercorp.pinpoint.common.server.bo.serializer.stat.AgentStatHbaseOperationFactory;
 import com.navercorp.pinpoint.common.server.bo.serializer.stat.JvmGcDetailedSerializer;
 import com.navercorp.pinpoint.common.server.bo.stat.AgentStatType;
 import com.navercorp.pinpoint.common.server.bo.stat.JvmGcDetailedBo;
 import org.apache.commons.collections.CollectionUtils;
+import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.client.Put;
 import org.springframework.beans.factory.annotation.Autowired;
 import org.springframework.stereotype.Repository;
@@ -40,6 +42,9 @@ public class HbaseJvmGcDetailedDao implements AgentStatDaoV2<JvmGcDetailedBo> {
     private HbaseOperations2 hbaseTemplate;
 
     @Autowired
+    private TableNameProvider tableNameProvider;
+
+    @Autowired
     private AgentStatHbaseOperationFactory agentStatHbaseOperationFactory;
 
     @Autowired
@@ -55,9 +60,10 @@ public class HbaseJvmGcDetailedDao implements AgentStatDaoV2<JvmGcDetailedBo> {
         }
         List<Put> jvmGcDetailedPuts = this.agentStatHbaseOperationFactory.createPuts(agentId, AgentStatType.JVM_GC_DETAILED, jvmGcDetailedBos, this.jvmGcDetailedSerializer);
         if (!jvmGcDetailedPuts.isEmpty()) {
-            List<Put> rejectedPuts = this.hbaseTemplate.asyncPut(HBaseTables.AGENT_STAT_VER2, jvmGcDetailedPuts);
+            TableName agentStatTableName = tableNameProvider.getTableName(HBaseTables.AGENT_STAT_VER2_STR);
+            List<Put> rejectedPuts = this.hbaseTemplate.asyncPut(agentStatTableName, jvmGcDetailedPuts);
             if (CollectionUtils.isNotEmpty(rejectedPuts)) {
-                this.hbaseTemplate.put(HBaseTables.AGENT_STAT_VER2, rejectedPuts);
+                this.hbaseTemplate.put(agentStatTableName, rejectedPuts);
             }
         }
     }
diff --git a/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/stat/HbaseResponseTimeDao.java b/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/stat/HbaseResponseTimeDao.java
index f55f8e903..10bb27297 100644
--- a/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/stat/HbaseResponseTimeDao.java
+++ b/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/stat/HbaseResponseTimeDao.java
@@ -19,11 +19,13 @@ package com.navercorp.pinpoint.collector.dao.hbase.stat;
 import com.navercorp.pinpoint.collector.dao.AgentStatDaoV2;
 import com.navercorp.pinpoint.common.hbase.HBaseTables;
 import com.navercorp.pinpoint.common.hbase.HbaseOperations2;
+import com.navercorp.pinpoint.common.hbase.TableNameProvider;
 import com.navercorp.pinpoint.common.server.bo.serializer.stat.AgentStatHbaseOperationFactory;
 import com.navercorp.pinpoint.common.server.bo.serializer.stat.ResponseTimeSerializer;
 import com.navercorp.pinpoint.common.server.bo.stat.AgentStatType;
 import com.navercorp.pinpoint.common.server.bo.stat.ResponseTimeBo;
 import org.apache.commons.collections.CollectionUtils;
+import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.client.Put;
 import org.springframework.beans.factory.annotation.Autowired;
 import org.springframework.stereotype.Repository;
@@ -40,6 +42,9 @@ public class HbaseResponseTimeDao implements AgentStatDaoV2<ResponseTimeBo> {
     private HbaseOperations2 hbaseTemplate;
 
     @Autowired
+    private TableNameProvider tableNameProvider;
+
+    @Autowired
     private AgentStatHbaseOperationFactory agentStatHbaseOperationFactory;
 
     @Autowired
@@ -55,9 +60,10 @@ public class HbaseResponseTimeDao implements AgentStatDaoV2<ResponseTimeBo> {
         }
         List<Put> responseTimePuts = this.agentStatHbaseOperationFactory.createPuts(agentId, AgentStatType.RESPONSE_TIME, responseTimeBos, this.responseTimeSerializer);
         if (!responseTimePuts.isEmpty()) {
-            List<Put> rejectedPuts = this.hbaseTemplate.asyncPut(HBaseTables.AGENT_STAT_VER2, responseTimePuts);
+            TableName agentStatTableName = tableNameProvider.getTableName(HBaseTables.AGENT_STAT_VER2_STR);
+            List<Put> rejectedPuts = this.hbaseTemplate.asyncPut(agentStatTableName, responseTimePuts);
             if (CollectionUtils.isNotEmpty(rejectedPuts)) {
-                this.hbaseTemplate.put(HBaseTables.AGENT_STAT_VER2, rejectedPuts);
+                this.hbaseTemplate.put(agentStatTableName, rejectedPuts);
             }
         }
     }
diff --git a/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/stat/HbaseTransactionDao.java b/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/stat/HbaseTransactionDao.java
index 63597cb12..1a54a2df7 100644
--- a/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/stat/HbaseTransactionDao.java
+++ b/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/stat/HbaseTransactionDao.java
@@ -19,11 +19,13 @@ package com.navercorp.pinpoint.collector.dao.hbase.stat;
 import com.navercorp.pinpoint.collector.dao.AgentStatDaoV2;
 import com.navercorp.pinpoint.common.hbase.HBaseTables;
 import com.navercorp.pinpoint.common.hbase.HbaseOperations2;
+import com.navercorp.pinpoint.common.hbase.TableNameProvider;
 import com.navercorp.pinpoint.common.server.bo.serializer.stat.AgentStatHbaseOperationFactory;
 import com.navercorp.pinpoint.common.server.bo.serializer.stat.TransactionSerializer;
 import com.navercorp.pinpoint.common.server.bo.stat.AgentStatType;
 import com.navercorp.pinpoint.common.server.bo.stat.TransactionBo;
 import org.apache.commons.collections.CollectionUtils;
+import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.client.Put;
 import org.springframework.beans.factory.annotation.Autowired;
 import org.springframework.stereotype.Repository;
@@ -40,6 +42,9 @@ public class HbaseTransactionDao implements AgentStatDaoV2<TransactionBo> {
     private HbaseOperations2 hbaseTemplate;
 
     @Autowired
+    private TableNameProvider tableNameProvider;
+
+    @Autowired
     private AgentStatHbaseOperationFactory agentStatHbaseOperationFactory;
 
     @Autowired
@@ -55,9 +60,10 @@ public class HbaseTransactionDao implements AgentStatDaoV2<TransactionBo> {
         }
         List<Put> transactionPuts = this.agentStatHbaseOperationFactory.createPuts(agentId, AgentStatType.TRANSACTION, transactionBos, this.transactionSerializer);
         if (!transactionPuts.isEmpty()) {
-            List<Put> rejectedPuts = this.hbaseTemplate.asyncPut(HBaseTables.AGENT_STAT_VER2, transactionPuts);
+            TableName agentStatTableName = tableNameProvider.getTableName(HBaseTables.AGENT_STAT_VER2_STR);
+            List<Put> rejectedPuts = this.hbaseTemplate.asyncPut(agentStatTableName, transactionPuts);
             if (CollectionUtils.isNotEmpty(rejectedPuts)) {
-                this.hbaseTemplate.put(HBaseTables.AGENT_STAT_VER2, rejectedPuts);
+                this.hbaseTemplate.put(agentStatTableName, rejectedPuts);
             }
         }
     }
diff --git a/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/statistics/BulkIncrementer.java b/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/statistics/BulkIncrementer.java
new file mode 100644
index 000000000..13d7a7d4c
--- /dev/null
+++ b/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/statistics/BulkIncrementer.java
@@ -0,0 +1,51 @@
+/*
+ * Copyright 2018 NAVER Corp.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package com.navercorp.pinpoint.collector.dao.hbase.statistics;
+
+import com.google.common.util.concurrent.AtomicLongMap;
+import com.navercorp.pinpoint.collector.util.AtomicLongMapUtils;
+import com.sematext.hbase.wd.RowKeyDistributorByHashPrefix;
+import org.apache.hadoop.hbase.TableName;
+import org.apache.hadoop.hbase.client.Increment;
+
+import java.util.List;
+import java.util.Map;
+import java.util.Objects;
+
+/**
+ * @author HyunGil Jeong
+ */
+public class BulkIncrementer {
+
+    private final RowKeyMerge rowKeyMerge;
+
+    private final AtomicLongMap<RowInfo> counter = AtomicLongMap.create();
+
+    public BulkIncrementer(RowKeyMerge rowKeyMerge) {
+        this.rowKeyMerge = Objects.requireNonNull(rowKeyMerge, "rowKeyMerge must not be null");
+    }
+
+    public void increment(TableName tableName, RowKey rowKey, ColumnName columnName) {
+        RowInfo rowInfo = new DefaultRowInfo(tableName, rowKey, columnName);
+        counter.incrementAndGet(rowInfo);
+    }
+
+    public Map<TableName, List<Increment>> getIncrements(RowKeyDistributorByHashPrefix rowKeyDistributor) {
+        final Map<RowInfo, Long> snapshot = AtomicLongMapUtils.remove(counter);
+        return rowKeyMerge.createBulkIncrement(snapshot, rowKeyDistributor);
+    }
+}
diff --git a/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/statistics/DefaultRowInfo.java b/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/statistics/DefaultRowInfo.java
index 4e07fec94..a2cdd1913 100644
--- a/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/statistics/DefaultRowInfo.java
+++ b/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/statistics/DefaultRowInfo.java
@@ -16,30 +16,37 @@
 
 package com.navercorp.pinpoint.collector.dao.hbase.statistics;
 
+import org.apache.hadoop.hbase.TableName;
+
+import java.util.Objects;
+
 /**
  * @author emeroad
+ * @author HyunGil Jeong
  */
 public class DefaultRowInfo implements RowInfo {
 
-    private RowKey rowKey;
-    private ColumnName columnName;
+    private final TableName tableName;
+    private final RowKey rowKey;
+    private final ColumnName columnName;
 
-    public DefaultRowInfo(RowKey rowKey, ColumnName columnName) {
-        if (rowKey == null) {
-            throw new NullPointerException("rowKey must not be null");
-        }
-        if (columnName == null) {
-            throw new NullPointerException("columnName must not be null");
-        }
+    public DefaultRowInfo(TableName tableName, RowKey rowKey, ColumnName columnName) {
+        this.tableName = Objects.requireNonNull(tableName, "tableName must not be null");
+        this.rowKey = Objects.requireNonNull(rowKey, "rowKey must not be null");
+        this.columnName = Objects.requireNonNull(columnName, "columnName must not be null");
+    }
 
-        this.rowKey = rowKey;
-        this.columnName = columnName;
+    @Override
+    public TableName getTableName() {
+        return tableName;
     }
 
+    @Override
     public RowKey getRowKey() {
         return rowKey;
     }
 
+    @Override
     public ColumnName getColumnName() {
         return columnName;
     }
@@ -51,15 +58,15 @@ public class DefaultRowInfo implements RowInfo {
 
         DefaultRowInfo that = (DefaultRowInfo) o;
 
-        if (!columnName.equals(that.columnName)) return false;
+        if (!tableName.equals(that.tableName)) return false;
         if (!rowKey.equals(that.rowKey)) return false;
-
-        return true;
+        return columnName.equals(that.columnName);
     }
 
     @Override
     public int hashCode() {
-        int result = rowKey.hashCode();
+        int result = tableName.hashCode();
+        result = 31 * result + rowKey.hashCode();
         result = 31 * result + columnName.hashCode();
         return result;
     }
diff --git a/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/statistics/RowInfo.java b/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/statistics/RowInfo.java
index 312be2e46..6effee0d5 100644
--- a/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/statistics/RowInfo.java
+++ b/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/statistics/RowInfo.java
@@ -16,11 +16,15 @@
 
 package com.navercorp.pinpoint.collector.dao.hbase.statistics;
 
+import org.apache.hadoop.hbase.TableName;
+
 /**
  * @author emeroad
  */
 public interface RowInfo {
 
+    TableName getTableName();
+
     RowKey getRowKey();
 
     ColumnName getColumnName();
diff --git a/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/statistics/RowKeyMerge.java b/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/statistics/RowKeyMerge.java
index e43442c83..f77a94ac7 100644
--- a/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/statistics/RowKeyMerge.java
+++ b/collector/src/main/java/com/navercorp/pinpoint/collector/dao/hbase/statistics/RowKeyMerge.java
@@ -17,14 +17,21 @@
 package com.navercorp.pinpoint.collector.dao.hbase.statistics;
 
 import com.sematext.hbase.wd.RowKeyDistributorByHashPrefix;
+import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.client.Increment;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import java.util.*;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
 
 /**
  * @author emeroad
+ * @author HyunGil Jeong
  */
 public class RowKeyMerge {
     private final Logger logger = LoggerFactory.getLogger(this.getClass());
@@ -37,19 +44,24 @@ public class RowKeyMerge {
         this.family = Arrays.copyOf(family, family.length);
     }
 
-    public  List<Increment> createBulkIncrement(Map<RowInfo, Long> data, RowKeyDistributorByHashPrefix rowKeyDistributorByHashPrefix) {
+    public Map<TableName, List<Increment>> createBulkIncrement(Map<RowInfo, Long> data, RowKeyDistributorByHashPrefix rowKeyDistributorByHashPrefix) {
         if (data.isEmpty()) {
-            return Collections.emptyList();
+            return Collections.emptyMap();
         }
 
-        final Map<RowKey, List<ColumnName>> rowkeyMerge = rowKeyBaseMerge(data);
+        final Map<TableName, List<Increment>> tableIncrementMap = new HashMap<>();
+        final Map<TableName, Map<RowKey, List<ColumnName>>> tableRowKeyMap = mergeRowKeys(data);
 
-        List<Increment> incrementList = new ArrayList<>();
-        for (Map.Entry<RowKey, List<ColumnName>> rowKeyEntry : rowkeyMerge.entrySet()) {
-            Increment increment = createIncrement(rowKeyEntry, rowKeyDistributorByHashPrefix);
-            incrementList.add(increment);
+        for (Map.Entry<TableName, Map<RowKey, List<ColumnName>>> tableRowKeys : tableRowKeyMap.entrySet()) {
+            final TableName tableName = tableRowKeys.getKey();
+            final List<Increment> incrementList = new ArrayList<>();
+            for (Map.Entry<RowKey, List<ColumnName>> rowKeyEntry : tableRowKeys.getValue().entrySet()) {
+                Increment increment = createIncrement(rowKeyEntry, rowKeyDistributorByHashPrefix);
+                incrementList.add(increment);
+            }
+            tableIncrementMap.put(tableName, incrementList);
         }
-        return incrementList;
+        return tableIncrementMap;
     }
 
     private Increment createIncrement(Map.Entry<RowKey, List<ColumnName>> rowKeyEntry, RowKeyDistributorByHashPrefix rowKeyDistributorByHashPrefix) {
@@ -68,8 +80,8 @@ public class RowKeyMerge {
         return increment;
     }
 
-    private Map<RowKey, List<ColumnName>> rowKeyBaseMerge(Map<RowInfo, Long> data) {
-        final Map<RowKey, List<ColumnName>> merge = new HashMap<>();
+    private Map<TableName, Map<RowKey, List<ColumnName>>> mergeRowKeys(Map<RowInfo, Long> data) {
+        final Map<TableName, Map<RowKey, List<ColumnName>>> tables = new HashMap<>();
 
         for (Map.Entry<RowInfo, Long> entry : data.entrySet()) {
             final RowInfo rowInfo = entry.getKey();
@@ -77,16 +89,21 @@ public class RowKeyMerge {
             long callCount = entry.getValue();
             rowInfo.getColumnName().setCallCount(callCount);
 
-            RowKey rowKey = rowInfo.getRowKey();
-            List<ColumnName> oldList = merge.get(rowKey);
-            if (oldList == null) {
-                List<ColumnName> newList = new ArrayList<>();
-                newList.add(rowInfo.getColumnName());
-                merge.put(rowKey, newList);
-            } else {
-                oldList.add(rowInfo.getColumnName());
+            final TableName tableName = rowInfo.getTableName();
+            final RowKey rowKey = rowInfo.getRowKey();
+
+            Map<RowKey, List<ColumnName>> rows = tables.get(tableName);
+            if (rows == null) {
+                rows = new HashMap<>();
+                tables.put(tableName, rows);
+            }
+            List<ColumnName> columnNames = rows.get(rowKey);
+            if (columnNames == null) {
+                columnNames = new ArrayList<>();
+                rows.put(rowKey, columnNames);
             }
+            columnNames.add(rowInfo.getColumnName());
         }
-        return merge;
+        return tables;
     }
 }
diff --git a/collector/src/main/java/com/navercorp/pinpoint/collector/receiver/TCPReceiverBean.java b/collector/src/main/java/com/navercorp/pinpoint/collector/receiver/TCPReceiverBean.java
index 3a55d7d36..80aef7bf0 100644
--- a/collector/src/main/java/com/navercorp/pinpoint/collector/receiver/TCPReceiverBean.java
+++ b/collector/src/main/java/com/navercorp/pinpoint/collector/receiver/TCPReceiverBean.java
@@ -16,6 +16,9 @@
 
 package com.navercorp.pinpoint.collector.receiver;
 
+import com.navercorp.pinpoint.collector.receiver.tcp.DefaultTCPPacketHandlerFactory;
+import com.navercorp.pinpoint.collector.receiver.tcp.TCPPacketHandler;
+import com.navercorp.pinpoint.collector.receiver.tcp.TCPPacketHandlerFactory;
 import com.navercorp.pinpoint.collector.receiver.tcp.TCPReceiver;
 import com.navercorp.pinpoint.common.server.util.AddressFilter;
 import org.springframework.beans.factory.BeanNameAware;
@@ -43,6 +46,7 @@ public class TCPReceiverBean implements InitializingBean, DisposableBean, BeanNa
     private DispatchHandler dispatchHandler;
     private AddressFilter addressFilter;
 
+    private TCPPacketHandlerFactory tcpPacketHandlerFactory;
 
     @Override
     public void afterPropertiesSet() throws Exception {
@@ -55,17 +59,26 @@ public class TCPReceiverBean implements InitializingBean, DisposableBean, BeanNa
         Objects.requireNonNull(addressFilter, "addressFilter must not be null");
         Objects.requireNonNull(executor, "executor must not be null");
 
-        tcpReceiver = createTcpReceiver(beanName, this.bindIp, bindPort, executor, dispatchHandler, addressFilter);
+        tcpReceiver = createTcpReceiver(beanName, this.bindIp, bindPort, executor, dispatchHandler, this.tcpPacketHandlerFactory, addressFilter);
         tcpReceiver.start();
     }
 
 
-    private TCPReceiver createTcpReceiver(String beanName, String bindIp, int port, Executor executor, DispatchHandler dispatchHandler, AddressFilter addressFilter) {
+    private TCPReceiver createTcpReceiver(String beanName, String bindIp, int port, Executor executor,
+                                          DispatchHandler dispatchHandler, TCPPacketHandlerFactory tcpPacketHandlerFactory, AddressFilter addressFilter) {
         InetSocketAddress bindAddress = new InetSocketAddress(bindIp, port);
+        TCPPacketHandler tcpPacketHandler = wrapDispatchHandler(dispatchHandler, tcpPacketHandlerFactory);
 
-        return new TCPReceiver(beanName, dispatchHandler, executor, bindAddress, addressFilter);
+        return new TCPReceiver(beanName, tcpPacketHandler, executor, bindAddress, addressFilter);
     }
 
+    private TCPPacketHandler wrapDispatchHandler(DispatchHandler dispatchHandler, TCPPacketHandlerFactory tcpPacketHandlerFactory) {
+        if (tcpPacketHandlerFactory == null) {
+            // using default Factory
+            tcpPacketHandlerFactory = new DefaultTCPPacketHandlerFactory();
+        }
+        return tcpPacketHandlerFactory.build(dispatchHandler);
+    }
 
 
     @Override
@@ -83,7 +96,11 @@ public class TCPReceiverBean implements InitializingBean, DisposableBean, BeanNa
     }
 
     public void setDispatchHandler(DispatchHandler dispatchHandler) {
-        this.dispatchHandler = Objects.requireNonNull(dispatchHandler, "dispatchHandler must not be null");
+        this.dispatchHandler = dispatchHandler;
+    }
+
+    public void setTcpPacketHandlerFactory(TCPPacketHandlerFactory tcpPacketHandlerFactory) {
+        this.tcpPacketHandlerFactory = tcpPacketHandlerFactory;
     }
 
     public void setBindIp(String bindIp) {
diff --git a/collector/src/main/java/com/navercorp/pinpoint/collector/receiver/tcp/AgentBaseDataReceiver.java b/collector/src/main/java/com/navercorp/pinpoint/collector/receiver/tcp/AgentBaseDataReceiver.java
index e51d30c16..8bf311feb 100644
--- a/collector/src/main/java/com/navercorp/pinpoint/collector/receiver/tcp/AgentBaseDataReceiver.java
+++ b/collector/src/main/java/com/navercorp/pinpoint/collector/receiver/tcp/AgentBaseDataReceiver.java
@@ -18,8 +18,8 @@ package com.navercorp.pinpoint.collector.receiver.tcp;
 
 import com.navercorp.pinpoint.collector.cluster.zookeeper.ZookeeperClusterService;
 import com.navercorp.pinpoint.collector.config.AgentBaseDataReceiverConfiguration;
-import com.navercorp.pinpoint.collector.receiver.DispatchHandler;
 import com.navercorp.pinpoint.collector.receiver.AddressFilterAdaptor;
+import com.navercorp.pinpoint.collector.receiver.DispatchHandler;
 import com.navercorp.pinpoint.common.server.util.AddressFilter;
 import com.navercorp.pinpoint.collector.rpc.handler.AgentLifeCycleHandler;
 import com.navercorp.pinpoint.collector.service.AgentEventService;
@@ -68,8 +68,7 @@ public class AgentBaseDataReceiver {
 
     private final Executor executor;
 
-    private final SendPacketHandler sendPacketHandler;
-    private final RequestPacketHandler requestPacketHandler;
+    private final TCPPacketHandler tcpPacketHandler;
 
 
     @Resource(name = "agentEventService")
@@ -90,11 +89,15 @@ public class AgentBaseDataReceiver {
         this.executor = Objects.requireNonNull(executor, "executor must not be null");
         this.addressFilter = Objects.requireNonNull(addressFilter, "addressFilter must not be null");
 
+        this.tcpPacketHandler = wrapDispatchHandler(dispatchHandler);
+        this.clusterService = service;
+    }
+
+    private TCPPacketHandler wrapDispatchHandler(DispatchHandler dispatchHandler) {
         Objects.requireNonNull(dispatchHandler, "dispatchHandler must not be null");
-        this.sendPacketHandler = new SendPacketHandler(dispatchHandler);
-        this.requestPacketHandler = new RequestPacketHandler(dispatchHandler);
 
-        this.clusterService = service;
+        TCPPacketHandlerFactory tcpPacketHandlerFactory = new DefaultTCPPacketHandlerFactory();
+        return tcpPacketHandlerFactory.build(dispatchHandler);
     }
 
     @PostConstruct
@@ -167,7 +170,7 @@ public class AgentBaseDataReceiver {
         executor.execute(new Runnable() {
             @Override
             public void run() {
-                sendPacketHandler.handle(sendPacket, pinpointSocket);
+                tcpPacketHandler.handleSend(sendPacket, pinpointSocket);
             }
         });
     }
@@ -176,7 +179,7 @@ public class AgentBaseDataReceiver {
         executor.execute(new Runnable() {
             @Override
             public void run() {
-                requestPacketHandler.handle(requestPacket, pinpointSocket);
+                tcpPacketHandler.handleRequest(requestPacket, pinpointSocket);
             }
         });
     }
diff --git a/collector/src/main/java/com/navercorp/pinpoint/collector/receiver/tcp/RequestPacketHandler.java b/collector/src/main/java/com/navercorp/pinpoint/collector/receiver/tcp/DefaultTCPPacketHandler.java
similarity index 54%
rename from collector/src/main/java/com/navercorp/pinpoint/collector/receiver/tcp/RequestPacketHandler.java
rename to collector/src/main/java/com/navercorp/pinpoint/collector/receiver/tcp/DefaultTCPPacketHandler.java
index 034ffbe5e..ebffced25 100644
--- a/collector/src/main/java/com/navercorp/pinpoint/collector/receiver/tcp/RequestPacketHandler.java
+++ b/collector/src/main/java/com/navercorp/pinpoint/collector/receiver/tcp/DefaultTCPPacketHandler.java
@@ -1,11 +1,11 @@
 /*
- * Copyright 2017 NAVER Corp.
+ * Copyright 2018 NAVER Corp.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
  * you may not use this file except in compliance with the License.
  * You may obtain a copy of the License at
  *
- *     http://www.apache.org/licenses/LICENSE-2.0
+ * http://www.apache.org/licenses/LICENSE-2.0
  *
  * Unless required by applicable law or agreed to in writing, software
  * distributed under the License is distributed on an "AS IS" BASIS,
@@ -19,15 +19,13 @@ package com.navercorp.pinpoint.collector.receiver.tcp;
 import com.navercorp.pinpoint.collector.receiver.DispatchHandler;
 import com.navercorp.pinpoint.collector.util.PacketUtils;
 import com.navercorp.pinpoint.rpc.PinpointSocket;
+import com.navercorp.pinpoint.rpc.packet.BasicPacket;
 import com.navercorp.pinpoint.rpc.packet.RequestPacket;
+import com.navercorp.pinpoint.rpc.packet.SendPacket;
 import com.navercorp.pinpoint.thrift.io.DeserializerFactory;
 import com.navercorp.pinpoint.thrift.io.HeaderTBaseDeserializer;
-import com.navercorp.pinpoint.thrift.io.HeaderTBaseDeserializerFactory;
 import com.navercorp.pinpoint.thrift.io.HeaderTBaseSerializer;
-import com.navercorp.pinpoint.thrift.io.HeaderTBaseSerializerFactory;
 import com.navercorp.pinpoint.thrift.io.SerializerFactory;
-import com.navercorp.pinpoint.thrift.io.ThreadLocalHeaderTBaseDeserializerFactory;
-import com.navercorp.pinpoint.thrift.io.ThreadLocalHeaderTBaseSerializerFactory;
 import com.navercorp.pinpoint.thrift.util.SerializationUtils;
 import org.apache.thrift.TBase;
 import org.apache.thrift.TException;
@@ -38,9 +36,9 @@ import java.net.SocketAddress;
 import java.util.Objects;
 
 /**
- * @author Taejin Koo
+ * @author Woonduk Kang(emeroad)
  */
-public class RequestPacketHandler implements PinpointPacketHandler<RequestPacket> {
+public class DefaultTCPPacketHandler implements TCPPacketHandler {
 
     private final Logger logger = LoggerFactory.getLogger(this.getClass());
     private final boolean isDebug = logger.isDebugEnabled();
@@ -50,31 +48,43 @@ public class RequestPacketHandler implements PinpointPacketHandler<RequestPacket
     private final SerializerFactory<HeaderTBaseSerializer> serializerFactory;
     private final DeserializerFactory<HeaderTBaseDeserializer> deserializerFactory;
 
-    public RequestPacketHandler(DispatchHandler dispatchHandler) {
-        this(dispatchHandler, new ThreadLocalHeaderTBaseSerializerFactory<>(new HeaderTBaseSerializerFactory(true, HeaderTBaseSerializerFactory.DEFAULT_UDP_STREAM_MAX_SIZE)));
-    }
-
-    public RequestPacketHandler(DispatchHandler dispatchHandler, SerializerFactory<HeaderTBaseSerializer> serializerFactory) {
-        this(dispatchHandler, serializerFactory, new ThreadLocalHeaderTBaseDeserializerFactory<>(new HeaderTBaseDeserializerFactory()));
-    }
-
-    public RequestPacketHandler(DispatchHandler dispatchHandler, DeserializerFactory<HeaderTBaseDeserializer> deserializerFactory) {
-        this(dispatchHandler, new ThreadLocalHeaderTBaseSerializerFactory<>(new HeaderTBaseSerializerFactory(true, HeaderTBaseSerializerFactory.DEFAULT_UDP_STREAM_MAX_SIZE)), deserializerFactory);
-    }
 
-    public RequestPacketHandler(DispatchHandler dispatchHandler, SerializerFactory<HeaderTBaseSerializer> serializerFactory, DeserializerFactory<HeaderTBaseDeserializer> deserializerFactory) {
+    public DefaultTCPPacketHandler(DispatchHandler dispatchHandler, SerializerFactory<HeaderTBaseSerializer> serializerFactory, DeserializerFactory<HeaderTBaseDeserializer> deserializerFactory) {
         this.dispatchHandler = Objects.requireNonNull(dispatchHandler, "dispatchHandler must not be null");
         this.serializerFactory = Objects.requireNonNull(serializerFactory, "serializerFactory must not be null");
         this.deserializerFactory = Objects.requireNonNull(deserializerFactory, "deserializerFactory must not be null");
     }
 
     @Override
-    public void handle(RequestPacket packet, PinpointSocket pinpointSocket) {
+    public void handleSend(SendPacket packet, PinpointSocket pinpointSocket) {
         Objects.requireNonNull(packet, "packet must not be null");
         Objects.requireNonNull(pinpointSocket, "pinpointSocket must not be null");
-        
-        byte[] payload = packet.getPayload();
+
+        final byte[] payload = getPayload(packet);
+        SocketAddress remoteAddress = pinpointSocket.getRemoteAddress();
+        try {
+            TBase<?, ?> tBase = SerializationUtils.deserialize(payload, deserializerFactory);
+            dispatchHandler.dispatchSendMessage(tBase);
+        } catch (TException e) {
+            handleTException(payload, remoteAddress, e);
+        } catch (Exception e) {
+            // there are cases where invalid headers are received
+            handleException(payload, remoteAddress, e);
+        }
+    }
+
+    public byte[] getPayload(BasicPacket packet) {
+        final byte[] payload = packet.getPayload();
         Objects.requireNonNull(payload, "payload must not be null");
+        return payload;
+    }
+
+    @Override
+    public void handleRequest(RequestPacket packet, PinpointSocket pinpointSocket) {
+        Objects.requireNonNull(packet, "packet must not be null");
+        Objects.requireNonNull(pinpointSocket, "pinpointSocket must not be null");
+
+        final byte[] payload = getPayload(packet);
 
         SocketAddress remoteAddress = pinpointSocket.getRemoteAddress();
         try {
@@ -85,21 +95,28 @@ public class RequestPacketHandler implements PinpointPacketHandler<RequestPacket
                 pinpointSocket.response(packet, resultBytes);
             }
         } catch (TException e) {
-            if (logger.isWarnEnabled()) {
-                logger.warn("packet serialize error. remote:{} cause:{}", remoteAddress, e.getMessage(), e);
-            }
-            if (isDebug) {
-                logger.debug("packet dump hex:{}", PacketUtils.dumpByteArray(payload));
-            }
+            handleTException(payload, remoteAddress, e);
         } catch (Exception e) {
-            // there are cases where invalid headers are received
-            if (logger.isWarnEnabled()) {
-                logger.warn("Unexpected error. remote:{} cause:{}", remoteAddress, e.getMessage(), e);
-            }
-            if (isDebug) {
-                logger.debug("packet dump hex:{}", PacketUtils.dumpByteArray(payload));
-            }
+            handleException(payload, remoteAddress, e);
+        }
+    }
+
+    private void handleTException(byte[] payload, SocketAddress remoteAddress, TException e) {
+        if (logger.isWarnEnabled()) {
+            logger.warn("packet serialize error. remote:{} cause:{}", remoteAddress, e.getMessage(), e);
+        }
+        if (isDebug) {
+            logger.debug("packet dump hex:{}", PacketUtils.dumpByteArray(payload));
         }
     }
 
+    private void handleException(byte[] payload, SocketAddress remoteAddress, Exception e) {
+        // there are cases where invalid headers are received
+        if (logger.isWarnEnabled()) {
+            logger.warn("Unexpected error. remote:{} cause:{}", remoteAddress, e.getMessage(), e);
+        }
+        if (isDebug) {
+            logger.debug("packet dump hex:{}", PacketUtils.dumpByteArray(payload));
+        }
+    }
 }
diff --git a/collector/src/main/java/com/navercorp/pinpoint/collector/receiver/tcp/DefaultTCPPacketHandlerFactory.java b/collector/src/main/java/com/navercorp/pinpoint/collector/receiver/tcp/DefaultTCPPacketHandlerFactory.java
new file mode 100644
index 000000000..f26efa27f
--- /dev/null
+++ b/collector/src/main/java/com/navercorp/pinpoint/collector/receiver/tcp/DefaultTCPPacketHandlerFactory.java
@@ -0,0 +1,85 @@
+/*
+ * Copyright 2018 NAVER Corp.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package com.navercorp.pinpoint.collector.receiver.tcp;
+
+import com.navercorp.pinpoint.collector.receiver.DispatchHandler;
+import com.navercorp.pinpoint.thrift.io.DeserializerFactory;
+import com.navercorp.pinpoint.thrift.io.HeaderTBaseDeserializer;
+import com.navercorp.pinpoint.thrift.io.HeaderTBaseDeserializerFactory;
+import com.navercorp.pinpoint.thrift.io.HeaderTBaseSerializer;
+import com.navercorp.pinpoint.thrift.io.HeaderTBaseSerializerFactory;
+import com.navercorp.pinpoint.thrift.io.SerializerFactory;
+import com.navercorp.pinpoint.thrift.io.ThreadLocalHeaderTBaseDeserializerFactory;
+import com.navercorp.pinpoint.thrift.io.ThreadLocalHeaderTBaseSerializerFactory;
+
+import java.util.Objects;
+
+/**
+ * @author Woonduk Kang(emeroad)
+ */
+public class DefaultTCPPacketHandlerFactory implements TCPPacketHandlerFactory {
+
+    private static final int DEFAULT_UDP_STREAM_MAX_SIZE = HeaderTBaseSerializerFactory.DEFAULT_UDP_STREAM_MAX_SIZE;
+
+    private SerializerFactory<HeaderTBaseSerializer> serializerFactory;
+    private DeserializerFactory<HeaderTBaseDeserializer> deserializerFactory;
+
+
+    public DefaultTCPPacketHandlerFactory() {
+    }
+
+
+    public void setSerializerFactory(SerializerFactory<HeaderTBaseSerializer> serializerFactory) {
+        this.serializerFactory = serializerFactory;
+    }
+
+    public void setDeserializerFactory(DeserializerFactory<HeaderTBaseDeserializer> deserializerFactory) {
+        this.deserializerFactory = deserializerFactory;
+    }
+
+    private DeserializerFactory<HeaderTBaseDeserializer> defaultDeserializerFactory() {
+        final DeserializerFactory<HeaderTBaseDeserializer> deserializerFactory = new HeaderTBaseDeserializerFactory();
+        return new ThreadLocalHeaderTBaseDeserializerFactory<>(deserializerFactory);
+    }
+
+    private SerializerFactory<HeaderTBaseSerializer> defaultSerializerFactory() {
+        final SerializerFactory<HeaderTBaseSerializer> serializerFactory = new HeaderTBaseSerializerFactory(true, DEFAULT_UDP_STREAM_MAX_SIZE);
+        return new ThreadLocalHeaderTBaseSerializerFactory<>(serializerFactory);
+    }
+
+
+    @Override
+    public TCPPacketHandler build(DispatchHandler dispatchHandler) {
+
+        Objects.requireNonNull(dispatchHandler, "dispatchHandler must not be null");
+
+        SerializerFactory<HeaderTBaseSerializer> serializerFactory = this.serializerFactory;
+        if (serializerFactory == null) {
+            serializerFactory = defaultSerializerFactory();
+        }
+        DeserializerFactory<HeaderTBaseDeserializer> deserializerFactory = this.deserializerFactory;
+        if (deserializerFactory == null) {
+            deserializerFactory = defaultDeserializerFactory();
+        }
+        return new DefaultTCPPacketHandler(dispatchHandler, serializerFactory, deserializerFactory);
+    }
+
+
+
+
+
+}
diff --git a/collector/src/main/java/com/navercorp/pinpoint/collector/receiver/tcp/SendPacketHandler.java b/collector/src/main/java/com/navercorp/pinpoint/collector/receiver/tcp/SendPacketHandler.java
deleted file mode 100644
index 0d5f4aba8..000000000
--- a/collector/src/main/java/com/navercorp/pinpoint/collector/receiver/tcp/SendPacketHandler.java
+++ /dev/null
@@ -1,87 +0,0 @@
-/*
- * Copyright 2017 NAVER Corp.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package com.navercorp.pinpoint.collector.receiver.tcp;
-
-import com.navercorp.pinpoint.collector.receiver.DispatchHandler;
-import com.navercorp.pinpoint.collector.util.PacketUtils;
-import com.navercorp.pinpoint.rpc.PinpointSocket;
-import com.navercorp.pinpoint.rpc.packet.SendPacket;
-import com.navercorp.pinpoint.thrift.io.DeserializerFactory;
-import com.navercorp.pinpoint.thrift.io.HeaderTBaseDeserializer;
-import com.navercorp.pinpoint.thrift.io.HeaderTBaseDeserializerFactory;
-import com.navercorp.pinpoint.thrift.io.ThreadLocalHeaderTBaseDeserializerFactory;
-import com.navercorp.pinpoint.thrift.util.SerializationUtils;
-import org.apache.thrift.TBase;
-import org.apache.thrift.TException;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import java.net.SocketAddress;
-import java.util.Objects;
-
-/**
- * @author Taejin Koo
- */
-public class SendPacketHandler implements PinpointPacketHandler<SendPacket> {
-
-    private final Logger logger = LoggerFactory.getLogger(this.getClass());
-    private final boolean isDebug = logger.isDebugEnabled();
-
-    private final DispatchHandler dispatchHandler;
-
-    private final DeserializerFactory<HeaderTBaseDeserializer> deserializerFactory;
-
-    public SendPacketHandler(DispatchHandler dispatchHandler) {
-        this(dispatchHandler, new ThreadLocalHeaderTBaseDeserializerFactory<>(new HeaderTBaseDeserializerFactory()));
-    }
-
-    public SendPacketHandler(DispatchHandler dispatchHandler, DeserializerFactory<HeaderTBaseDeserializer> deserializerFactory) {
-        this.dispatchHandler = Objects.requireNonNull(dispatchHandler, "dispatchHandler must not be null");
-        this.deserializerFactory = Objects.requireNonNull(deserializerFactory, "deserializerFactory must not be null");
-    }
-
-    @Override
-    public void handle(SendPacket packet, PinpointSocket pinpointSocket) {
-        Objects.requireNonNull(packet, "packet must not be null");
-        Objects.requireNonNull(pinpointSocket, "pinpointSocket must not be null");
-        
-        byte[] payload = packet.getPayload();
-        Objects.requireNonNull(payload, "payload must not be null");
-
-        SocketAddress remoteAddress = pinpointSocket.getRemoteAddress();
-        try {
-            TBase<?, ?> tBase = SerializationUtils.deserialize(payload, deserializerFactory);
-            dispatchHandler.dispatchSendMessage(tBase);
-        } catch (TException e) {
-            if (logger.isWarnEnabled()) {
-                logger.warn("packet serialize error. remote:{} cause:{}", remoteAddress, e.getMessage(), e);
-            }
-            if (isDebug) {
-                logger.debug("packet dump hex:{}", PacketUtils.dumpByteArray(payload));
-            }
-        } catch (Exception e) {
-            // there are cases where invalid headers are received
-            if (logger.isWarnEnabled()) {
-                logger.warn("Unexpected error. remote:{} cause:{}", remoteAddress, e.getMessage(), e);
-            }
-            if (isDebug) {
-                logger.debug("packet dump hex:{}", PacketUtils.dumpByteArray(payload));
-            }
-        }
-    }
-
-}
diff --git a/web/src/test/java/com/navercorp/pinpoint/web/applicationmap/appender/histogram/ParallelNodeHistogramAppenderTest.java b/collector/src/main/java/com/navercorp/pinpoint/collector/receiver/tcp/TCPPacketHandler.java
similarity index 50%
rename from web/src/test/java/com/navercorp/pinpoint/web/applicationmap/appender/histogram/ParallelNodeHistogramAppenderTest.java
rename to collector/src/main/java/com/navercorp/pinpoint/collector/receiver/tcp/TCPPacketHandler.java
index 7f1b77892..cbf0e036d 100644
--- a/web/src/test/java/com/navercorp/pinpoint/web/applicationmap/appender/histogram/ParallelNodeHistogramAppenderTest.java
+++ b/collector/src/main/java/com/navercorp/pinpoint/collector/receiver/tcp/TCPPacketHandler.java
@@ -1,11 +1,11 @@
 /*
- * Copyright 2017 NAVER Corp.
+ * Copyright 2018 NAVER Corp.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
  * you may not use this file except in compliance with the License.
  * You may obtain a copy of the License at
  *
- *     http://www.apache.org/licenses/LICENSE-2.0
+ * http://www.apache.org/licenses/LICENSE-2.0
  *
  * Unless required by applicable law or agreed to in writing, software
  * distributed under the License is distributed on an "AS IS" BASIS,
@@ -14,15 +14,18 @@
  * limitations under the License.
  */
 
-package com.navercorp.pinpoint.web.applicationmap.appender.histogram;
+package com.navercorp.pinpoint.collector.receiver.tcp;
+
+import com.navercorp.pinpoint.rpc.PinpointSocket;
+import com.navercorp.pinpoint.rpc.packet.RequestPacket;
+import com.navercorp.pinpoint.rpc.packet.SendPacket;
 
 /**
- * @author HyunGil Jeong
+ * @author Woonduk Kang(emeroad)
  */
-public class ParallelNodeHistogramAppenderTest extends NodeHistogramAppenderTestBase {
+public interface TCPPacketHandler {
+
+    void handleSend(SendPacket sendPacket, PinpointSocket pinpointSocket);
 
-    @Override
-    protected NodeHistogramAppenderFactory createNodeHistogramAppenderFactory() {
-        return new NodeHistogramAppenderFactory("parallel", 16);
-    }
+    void handleRequest(RequestPacket requestPacket, PinpointSocket pinpointSocket);
 }
diff --git a/collector/src/main/java/com/navercorp/pinpoint/collector/receiver/tcp/PinpointPacketHandler.java b/collector/src/main/java/com/navercorp/pinpoint/collector/receiver/tcp/TCPPacketHandlerFactory.java
similarity index 63%
rename from collector/src/main/java/com/navercorp/pinpoint/collector/receiver/tcp/PinpointPacketHandler.java
rename to collector/src/main/java/com/navercorp/pinpoint/collector/receiver/tcp/TCPPacketHandlerFactory.java
index 4b537e6df..708a382a9 100644
--- a/collector/src/main/java/com/navercorp/pinpoint/collector/receiver/tcp/PinpointPacketHandler.java
+++ b/collector/src/main/java/com/navercorp/pinpoint/collector/receiver/tcp/TCPPacketHandlerFactory.java
@@ -1,11 +1,11 @@
 /*
- * Copyright 2017 NAVER Corp.
+ * Copyright 2018 NAVER Corp.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
  * you may not use this file except in compliance with the License.
  * You may obtain a copy of the License at
  *
- *     http://www.apache.org/licenses/LICENSE-2.0
+ * http://www.apache.org/licenses/LICENSE-2.0
  *
  * Unless required by applicable law or agreed to in writing, software
  * distributed under the License is distributed on an "AS IS" BASIS,
@@ -16,14 +16,11 @@
 
 package com.navercorp.pinpoint.collector.receiver.tcp;
 
-import com.navercorp.pinpoint.rpc.PinpointSocket;
-import com.navercorp.pinpoint.rpc.packet.BasicPacket;
+import com.navercorp.pinpoint.collector.receiver.DispatchHandler;
 
 /**
- * @author Taejin Koo
+ * @author Woonduk Kang(emeroad)
  */
-public interface PinpointPacketHandler<P extends BasicPacket> {
-
-    void handle(P packet, PinpointSocket pinpointSocket);
-
+public interface TCPPacketHandlerFactory {
+    TCPPacketHandler build(DispatchHandler dispatchHandler);
 }
diff --git a/collector/src/main/java/com/navercorp/pinpoint/collector/receiver/tcp/TCPReceiver.java b/collector/src/main/java/com/navercorp/pinpoint/collector/receiver/tcp/TCPReceiver.java
index 9409db0c2..fdf036e79 100644
--- a/collector/src/main/java/com/navercorp/pinpoint/collector/receiver/tcp/TCPReceiver.java
+++ b/collector/src/main/java/com/navercorp/pinpoint/collector/receiver/tcp/TCPReceiver.java
@@ -16,7 +16,6 @@
 
 package com.navercorp.pinpoint.collector.receiver.tcp;
 
-import com.navercorp.pinpoint.collector.receiver.DispatchHandler;
 import com.navercorp.pinpoint.collector.receiver.AddressFilterAdaptor;
 import com.navercorp.pinpoint.common.server.util.AddressFilter;
 import com.navercorp.pinpoint.rpc.PinpointSocket;
@@ -52,11 +51,10 @@ public class TCPReceiver {
 
     private final Executor executor;
 
-    private final SendPacketHandler sendPacketHandler;
-    private final RequestPacketHandler requestPacketHandler;
+    private final TCPPacketHandler tcpPacketHandler;
 
 
-    public TCPReceiver(String name, DispatchHandler dispatchHandler, Executor executor, InetSocketAddress bindAddress, AddressFilter addressFilter) {
+    public TCPReceiver(String name, TCPPacketHandler tcpPacketHandler, Executor executor, InetSocketAddress bindAddress, AddressFilter addressFilter) {
         this.name = Objects.requireNonNull(name, "name must not be null");
         this.logger = LoggerFactory.getLogger(name);
 
@@ -65,9 +63,8 @@ public class TCPReceiver {
         this.addressFilter = Objects.requireNonNull(addressFilter, "addressFilter must not be null");
         this.executor = Objects.requireNonNull(executor, "executor must not be null");
 
-        Objects.requireNonNull(dispatchHandler, "dispatchHandler must not be null");
-        this.sendPacketHandler = new SendPacketHandler(dispatchHandler);
-        this.requestPacketHandler = new RequestPacketHandler(dispatchHandler);
+        this.tcpPacketHandler = Objects.requireNonNull(tcpPacketHandler, "tcpPacketHandler must not be null");
+
     }
 
     public void start() {
@@ -118,7 +115,7 @@ public class TCPReceiver {
         executor.execute(new Runnable() {
             @Override
             public void run() {
-                sendPacketHandler.handle(sendPacket, pinpointSocket);
+                tcpPacketHandler.handleSend(sendPacket, pinpointSocket);
             }
         });
     }
@@ -127,7 +124,7 @@ public class TCPReceiver {
         executor.execute(new Runnable() {
             @Override
             public void run() {
-                requestPacketHandler.handle(requestPacket, pinpointSocket);
+                tcpPacketHandler.handleRequest(requestPacket, pinpointSocket);
             }
         });
     }
diff --git a/collector/src/main/resources-local/log4j.xml b/collector/src/main/resources-local/log4j.xml
index 437a20705..f4f4653d5 100644
--- a/collector/src/main/resources-local/log4j.xml
+++ b/collector/src/main/resources-local/log4j.xml
@@ -24,7 +24,7 @@
     </logger>
 
     <logger name="com.navercorp.pinpoint.collector.dao" additivity="false">
-        <level value="TRACE"/>
+        <level value="DEBUG"/>
         <appender-ref ref="console"/>
     </logger>
 
diff --git a/collector/src/main/resources/applicationContext-collector.xml b/collector/src/main/resources/applicationContext-collector.xml
index 938d380fc..eb8eca3e7 100644
--- a/collector/src/main/resources/applicationContext-collector.xml
+++ b/collector/src/main/resources/applicationContext-collector.xml
@@ -351,14 +351,26 @@
         <constructor-arg value="#{hTable.MAP_STATISTICS_CALLEE_VER2_CF_COUNTER}"/>
     </bean>
 
+    <bean id="callerBulkIncrementer" class="com.navercorp.pinpoint.collector.dao.hbase.statistics.BulkIncrementer">
+        <constructor-arg ref="callerMerge"/>
+    </bean>
+
     <bean id="calleeMerge" class="com.navercorp.pinpoint.collector.dao.hbase.statistics.RowKeyMerge">
         <constructor-arg value="#{hTable.MAP_STATISTICS_CALLER_VER2_CF_COUNTER}"/>
     </bean>
 
+    <bean id="calleeBulkIncrementer" class="com.navercorp.pinpoint.collector.dao.hbase.statistics.BulkIncrementer">
+        <constructor-arg ref="calleeMerge"/>
+    </bean>
+
     <bean id="selfMerge" class="com.navercorp.pinpoint.collector.dao.hbase.statistics.RowKeyMerge">
         <constructor-arg value="#{hTable.MAP_STATISTICS_SELF_VER2_CF_COUNTER}"/>
     </bean>
 
+    <bean id="selfBulkIncrementer" class="com.navercorp.pinpoint.collector.dao.hbase.statistics.BulkIncrementer">
+        <constructor-arg ref="selfMerge"/>
+    </bean>
+
     <bean id="timeSlot" class="com.navercorp.pinpoint.common.util.DefaultTimeSlot">
     </bean>
 
diff --git a/collector/src/main/resources/applicationContext-hbase.xml b/collector/src/main/resources/applicationContext-hbase.xml
index 0918686b2..f655153ab 100644
--- a/collector/src/main/resources/applicationContext-hbase.xml
+++ b/collector/src/main/resources/applicationContext-hbase.xml
@@ -52,6 +52,10 @@
 
     <bean class="org.apache.hadoop.util.ShutdownHookManagerProxy"/>
 
+    <bean id="tableNameProvider" class="com.navercorp.pinpoint.common.hbase.HbaseTableNameProvider">
+        <constructor-arg index="0" value="${hbase.namespace:}"/>
+    </bean>
+
     <bean id="asyncOperation" class="com.navercorp.pinpoint.common.hbase.HBaseAsyncOperationFactory" factory-method="create">
         <constructor-arg type="org.apache.hadoop.hbase.client.Connection" value="#{connectionFactory.getConnection()}"/>
         <constructor-arg type="org.apache.hadoop.conf.Configuration" ref="hbaseConfiguration"/>
diff --git a/collector/src/main/resources/hbase.properties b/collector/src/main/resources/hbase.properties
index 81624d3ec..65d5398f2 100644
--- a/collector/src/main/resources/hbase.properties
+++ b/collector/src/main/resources/hbase.properties
@@ -4,6 +4,9 @@ hbase.client.port=2181
 # hbase default:/hbase
 hbase.zookeeper.znode.parent=/hbase
 
+# hbase namespace to use default:default - you may leave this empty to use the default namespace
+hbase.namespace=
+
 # hbase timeout option==================================================================================
 # hbase default:true
 hbase.ipc.client.tcpnodelay=true
diff --git a/collector/src/main/resources/servlet-context.xml b/collector/src/main/resources/servlet-context.xml
index 98d8bc205..806d582e1 100644
--- a/collector/src/main/resources/servlet-context.xml
+++ b/collector/src/main/resources/servlet-context.xml
@@ -12,7 +12,7 @@
     <mvc:annotation-driven />
     <tx:annotation-driven />
     <context:component-scan
-        base-package="com.navercorp.pinpoint.collector.manage.controller">
+        base-package="com.navercorp.pinpoint.collector.manage.controller, com.navercorp.pinpoint.collector.controller">
     </context:component-scan>
 
     <bean id="jsonView"
diff --git a/collector/src/test/java/com/navercorp/pinpoint/collector/dao/hbase/statistics/BulkIncrementerTest.java b/collector/src/test/java/com/navercorp/pinpoint/collector/dao/hbase/statistics/BulkIncrementerTest.java
new file mode 100644
index 000000000..c6d6171c7
--- /dev/null
+++ b/collector/src/test/java/com/navercorp/pinpoint/collector/dao/hbase/statistics/BulkIncrementerTest.java
@@ -0,0 +1,513 @@
+/*
+ * Copyright 2018 NAVER Corp.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package com.navercorp.pinpoint.collector.dao.hbase.statistics;
+
+import com.google.common.collect.Lists;
+import com.navercorp.pinpoint.common.util.BytesUtils;
+import com.sematext.hbase.wd.RowKeyDistributorByHashPrefix;
+import org.apache.hadoop.hbase.TableName;
+import org.apache.hadoop.hbase.client.Increment;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.junit.Assert;
+import org.junit.Before;
+import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.mockito.Mock;
+import org.mockito.junit.MockitoJUnitRunner;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+import org.springframework.util.CollectionUtils;
+
+import java.nio.ByteBuffer;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.NavigableMap;
+import java.util.Random;
+import java.util.concurrent.Callable;
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.FutureTask;
+import java.util.concurrent.TimeUnit;
+
+import static org.mockito.ArgumentMatchers.any;
+import static org.mockito.Mockito.when;
+
+/**
+ * @author HyunGil Jeong
+ */
+@RunWith(MockitoJUnitRunner.class)
+public class BulkIncrementerTest {
+
+    private static final byte[] CF = Bytes.toBytes("CF");
+
+    private final BulkIncrementer bulkIncrementer = new BulkIncrementer(new RowKeyMerge(CF));
+
+    @Mock
+    private RowKeyDistributorByHashPrefix rowKeyDistributor;
+
+    @Before
+    public void setUp() {
+        when(rowKeyDistributor.getDistributedKey(any(byte[].class))).then(invocation -> invocation.getArgument(0));
+    }
+
+    @Test
+    public void singleTable() {
+        // Given
+        TableName tableA = TableName.valueOf("A");
+        TestDataSet testDataSetA_0_0 = new TestDataSet(tableA, 0, 0, 100);
+        TestDataSet testDataSetA_0_1 = new TestDataSet(tableA, 0, 1, 200);
+
+        List<TestData> testDatas = new ArrayList<>();
+        testDatas.addAll(testDataSetA_0_0.getTestDatas());
+        testDatas.addAll(testDataSetA_0_1.getTestDatas());
+        Collections.shuffle(testDatas);
+
+        // When
+        for (TestData testData : testDatas) {
+            bulkIncrementer.increment(testData.getTableName(), testData.getRowKey(), testData.getColumnName());
+        }
+
+        // Then
+        Map<TableName, List<Increment>> incrementMap = bulkIncrementer.getIncrements(rowKeyDistributor);
+        TestVerifier verifier = new TestVerifier(incrementMap);
+        verifier.verify(testDataSetA_0_0);
+        verifier.verify(testDataSetA_0_1);
+    }
+
+    @Test
+    public void multipleTables() {
+        // Given
+        TableName tableA = TableName.valueOf("a", "A");
+        TableName tableB = TableName.valueOf("b", "A");
+        TestDataSet testDataSetA_0_0 = new TestDataSet(tableA, 0, 0, 100);
+        TestDataSet testDataSetA_0_1 = new TestDataSet(tableA, 0, 1, 200);
+        TestDataSet testDataSetA_1_0 = new TestDataSet(tableA, 1, 0, 300);
+        TestDataSet testDataSetA_1_1 = new TestDataSet(tableA, 1, 1, 400);
+        TestDataSet testDataSetB_0_0 = new TestDataSet(tableB, 0, 0, 500);
+        TestDataSet testDataSetB_0_1 = new TestDataSet(tableB, 0, 1, 600);
+        TestDataSet testDataSetB_1_0 = new TestDataSet(tableB, 1, 0, 700);
+        TestDataSet testDataSetB_1_1 = new TestDataSet(tableB, 1, 1, 800);
+
+        List<TestData> testDatas = new ArrayList<>();
+        testDatas.addAll(testDataSetA_0_0.getTestDatas());
+        testDatas.addAll(testDataSetA_0_1.getTestDatas());
+        testDatas.addAll(testDataSetA_1_0.getTestDatas());
+        testDatas.addAll(testDataSetA_1_1.getTestDatas());
+        testDatas.addAll(testDataSetB_0_0.getTestDatas());
+        testDatas.addAll(testDataSetB_0_1.getTestDatas());
+        testDatas.addAll(testDataSetB_1_0.getTestDatas());
+        testDatas.addAll(testDataSetB_1_1.getTestDatas());
+        Collections.shuffle(testDatas);
+
+        // When
+        for (TestData testData : testDatas) {
+            bulkIncrementer.increment(testData.getTableName(), testData.getRowKey(), testData.getColumnName());
+        }
+
+        // Then
+        Map<TableName, List<Increment>> incrementMap = bulkIncrementer.getIncrements(rowKeyDistributor);
+        TestVerifier verifier = new TestVerifier(incrementMap);
+        verifier.verify(testDataSetA_0_0);
+        verifier.verify(testDataSetA_0_1);
+        verifier.verify(testDataSetA_1_0);
+        verifier.verify(testDataSetA_1_1);
+        verifier.verify(testDataSetB_0_0);
+        verifier.verify(testDataSetB_0_1);
+        verifier.verify(testDataSetB_1_0);
+        verifier.verify(testDataSetB_1_1);
+    }
+
+    @Test
+    public void singleTableConcurrent() throws Exception {
+        // Given
+        TableName tableA = TableName.valueOf("A");
+        TestDataSet testDataSetA_0_0 = new TestDataSet(tableA, 0, 0, 1000000);
+        TestDataSet testDataSetA_0_1 = new TestDataSet(tableA, 0, 1, 1000001);
+
+        List<TestData> testDatas = new ArrayList<>();
+        testDatas.addAll(testDataSetA_0_0.getTestDatas());
+        testDatas.addAll(testDataSetA_0_1.getTestDatas());
+        Collections.shuffle(testDatas);
+
+        // When
+        final int numIncrementers = 16;
+        List<List<TestData>> testDataPartitions = Lists.partition(testDatas, testDatas.size() / (numIncrementers - 1));
+        final CountDownLatch completeLatch = new CountDownLatch(testDataPartitions.size());
+        final CountDownLatch flusherLatch = new CountDownLatch(1);
+
+        FutureTask<Map<TableName, List<Increment>>> flushTask = new FutureTask<>(new Flusher(completeLatch, flusherLatch));
+        new Thread(flushTask, "Flusher").start();
+
+        int counter = 0;
+        for (List<TestData> testDataPartition : testDataPartitions) {
+            Incrementer incrementer = new Incrementer(completeLatch, testDataPartition);
+            new Thread(incrementer, "Incrementer-" + counter++).start();
+        }
+
+        flusherLatch.await(30L, TimeUnit.SECONDS);
+
+        // Then
+        Map<TableName, List<Increment>> incrementMap = flushTask.get(5L, TimeUnit.SECONDS);
+        TestVerifier verifier = new TestVerifier(incrementMap);
+        verifier.verify(testDataSetA_0_0);
+        verifier.verify(testDataSetA_0_1);
+    }
+
+    @Test
+    public void multipleTablesConcurrent() throws Exception {
+        // Given
+        final int numTables = 50;
+        List<TableName> tableNames = new ArrayList<>(numTables);
+        for (int i = 0; i < numTables; i++) {
+            tableNames.add(TableName.valueOf(i + ""));
+        }
+        final int numRowIds = 100;
+        final int numColumnIds = 20;
+        final int numTestDataSets = numTables * numRowIds * numColumnIds;
+
+        final int maxCallCount = 200;
+        final Random random = new Random();
+        List<TestDataSet> testDataSets = new ArrayList<>(numTestDataSets);
+        for (TableName tableName : tableNames) {
+            for (int i = 0; i < numRowIds; i++) {
+                for (int j = 0; j < numColumnIds; j++) {
+                    int callCount = random.nextInt(maxCallCount - 100) + 100;
+                    TestDataSet testDataSet = new TestDataSet(tableName, i, j, callCount);
+                    testDataSets.add(testDataSet);
+                }
+            }
+        }
+        final int maxNumTestDatas = numTestDataSets * maxCallCount;
+        List<TestData> testDatas = new ArrayList<>(maxNumTestDatas);
+        for (TestDataSet testDataSet : testDataSets) {
+            testDatas.addAll(testDataSet.getTestDatas());
+        }
+        Collections.shuffle(testDatas);
+
+        // When
+        final int numIncrementers = 16;
+        List<List<TestData>> testDataPartitions = Lists.partition(testDatas, testDatas.size() / (numIncrementers - 1));
+        final CountDownLatch incrementorLatch = new CountDownLatch(testDataPartitions.size());
+        final CountDownLatch flusherLatch = new CountDownLatch(1);
+
+        FutureTask<Map<TableName, List<Increment>>> flushTask = new FutureTask<>(new Flusher(incrementorLatch, flusherLatch));
+        new Thread(flushTask, "Flusher").start();
+
+        int counter = 0;
+        for (List<TestData> testDataPartition : testDataPartitions) {
+            Incrementer incrementer = new Incrementer(incrementorLatch, testDataPartition);
+            new Thread(incrementer, "Incrementer-" + counter++).start();
+        }
+
+        flusherLatch.await(30L, TimeUnit.SECONDS);
+
+        // Then
+        Map<TableName, List<Increment>> incrementMap = flushTask.get(5L, TimeUnit.SECONDS);
+        TestVerifier verifier = new TestVerifier(incrementMap);
+        for (TestDataSet testDataSet : testDataSets) {
+            verifier.verify(testDataSet);
+        }
+    }
+
+    private class Incrementer implements Runnable {
+
+        private final Logger logger = LoggerFactory.getLogger(this.getClass());
+
+        private final CountDownLatch completeLatch;
+        private final List<TestData> testDatas;
+
+        private Incrementer(CountDownLatch completeLatch, List<TestData> testDatas) {
+            this.completeLatch = completeLatch;
+            this.testDatas = testDatas;
+        }
+
+        @Override
+        public void run() {
+            for (TestData testData : testDatas) {
+                bulkIncrementer.increment(testData.getTableName(), testData.getRowKey(), testData.getColumnName());
+            }
+            completeLatch.countDown();
+        }
+    }
+
+    private class Flusher implements Callable<Map<TableName, List<Increment>>> {
+
+        private final Logger logger = LoggerFactory.getLogger(this.getClass());
+
+        private final CountDownLatch awaitLatch;
+        private final CountDownLatch completeLatch;
+
+        private Flusher(CountDownLatch awaitLatch, CountDownLatch completeLatch) {
+            this.awaitLatch = awaitLatch;
+            this.completeLatch = completeLatch;
+        }
+
+        private void flushToMap(Map<TableName, List<Increment>> resultMap) {
+            Map<TableName, List<Increment>> incrementMap = bulkIncrementer.getIncrements(rowKeyDistributor);
+            for (Map.Entry<TableName, List<Increment>> incrementMapEntry : incrementMap.entrySet()) {
+                TableName tableName = incrementMapEntry.getKey();
+                List<Increment> increments = resultMap.computeIfAbsent(tableName, k -> new ArrayList<>());
+                increments.addAll(incrementMapEntry.getValue());
+            }
+        }
+
+        @Override
+        public Map<TableName, List<Increment>> call() {
+            Map<TableName, List<Increment>> resultMap = new HashMap<>();
+            try {
+                do {
+                    flushToMap(resultMap);
+                } while (!awaitLatch.await(10L, TimeUnit.MILLISECONDS));
+                flushToMap(resultMap);
+            } catch (InterruptedException e) {
+                Thread.currentThread().interrupt();
+                return Collections.emptyMap();
+            } finally {
+                completeLatch.countDown();
+            }
+            return resultMap;
+        }
+    }
+
+    private static class TestVerifier {
+
+        // Map<table, Map<row, Map<column, count>>>
+        private final Map<TableName, Map<ByteBuffer, Map<ByteBuffer, Long>>> resultMap;
+
+        public TestVerifier(Map<TableName, List<Increment>> incrementMap) {
+            this.resultMap = new HashMap<>();
+            for (Map.Entry<TableName, List<Increment>> incrementMapEntry : incrementMap.entrySet()) {
+                TableName tableName = incrementMapEntry.getKey();
+                List<Increment> increments = incrementMapEntry.getValue();
+                resultMap.put(tableName, convertIncrements(increments));
+            }
+        }
+
+        private Map<ByteBuffer, Map<ByteBuffer, Long>> convertIncrements(List<Increment> increments) {
+            if (CollectionUtils.isEmpty(increments)) {
+                return Collections.emptyMap();
+            }
+            Map<ByteBuffer, Map<ByteBuffer, Long>> convertedMap = new HashMap<>();
+            for (Increment increment : increments) {
+                ByteBuffer rowKey = ByteBuffer.wrap(increment.getRow());
+
+                Map<ByteBuffer, Long> convertedKeyValueMap = convertedMap.computeIfAbsent(rowKey, key -> new HashMap<>());
+                NavigableMap<byte[], Long> keyValues = increment.getFamilyMapOfLongs().get(CF);
+                for (Map.Entry<byte[], Long> keyValue : keyValues.entrySet()) {
+                    ByteBuffer key = ByteBuffer.wrap(keyValue.getKey());
+                    Long value = keyValue.getValue();
+                    if (value != null) {
+                        convertedKeyValueMap.merge(key, value, (val, prev) -> val + prev);
+                    }
+                }
+            }
+            return convertedMap;
+        }
+
+        public void verify(TestDataSet testDataSet) {
+            TableName expectedTableName = testDataSet.getTableName();
+            RowKey expectedRowKey = testDataSet.getRowKey();
+            ColumnName expectedColumnName = testDataSet.getColumnName();
+            long expectedCount = testDataSet.getCount();
+            Map<ByteBuffer, Map<ByteBuffer, Long>> rows = resultMap.get(expectedTableName);
+            if (rows == null) {
+                Assert.fail("Expected rows not found for " + testDataSet);
+            }
+            Map<ByteBuffer, Long> keyValues = rows.get(ByteBuffer.wrap(expectedRowKey.getRowKey()));
+            if (keyValues == null) {
+                Assert.fail("Expected row not found for " + testDataSet);
+            }
+            Long actualCount = keyValues.get(ByteBuffer.wrap(expectedColumnName.getColumnName()));
+            if (actualCount == null) {
+                Assert.fail("Expected column not found for " + testDataSet);
+            }
+            Assert.assertEquals("Expected counts do not match for " + testDataSet, expectedCount, (long) actualCount);
+        }
+    }
+
+    private static class TestDataSet {
+
+        private final TableName tableName;
+        private final TestRowKey rowKey;
+        private final TestColumnName columnName;
+        private final int count;
+        private List<TestData> testDatas;
+
+        private TestDataSet(TableName tableName, int rowId, int columnId, int count) {
+            this.tableName = tableName;
+            this.rowKey = new TestRowKey(rowId);
+            this.columnName = new TestColumnName(columnId);
+            this.count = count;
+        }
+
+        public TableName getTableName() {
+            return tableName;
+        }
+
+        public RowKey getRowKey() {
+            return rowKey;
+        }
+
+        public ColumnName getColumnName() {
+            return columnName;
+        }
+
+        public int getCount() {
+            return count;
+        }
+
+        public List<TestData> getTestDatas() {
+            if (testDatas == null) {
+                if (count < 1) {
+                    testDatas = Collections.emptyList();
+                } else {
+                    testDatas = new ArrayList<>(count);
+                    for (int i = 0; i < count; i++) {
+                        TestData testData = new TestData(this.tableName, this.rowKey, this.columnName);
+                        testDatas.add(testData);
+                    }
+                }
+            }
+            return testDatas;
+        }
+
+        @Override
+        public String toString() {
+            final StringBuilder sb = new StringBuilder("TestDataSet{");
+            sb.append("tableName=").append(tableName);
+            sb.append(", row=").append(rowKey.getId());
+            sb.append(", column=").append(columnName.getId());
+            sb.append(", count=").append(count);
+            sb.append('}');
+            return sb.toString();
+        }
+    }
+
+    private static class TestData {
+
+        private final TableName tableName;
+        private final RowKey rowKey;
+        private final ColumnName columnName;
+
+        private TestData(TableName tableName, RowKey rowKey, ColumnName columnName) {
+            this.tableName = tableName;
+            this.rowKey = rowKey;
+            this.columnName = columnName;
+        }
+
+        public TableName getTableName() {
+            return tableName;
+        }
+
+        public RowKey getRowKey() {
+            return rowKey;
+        }
+
+        public ColumnName getColumnName() {
+            return columnName;
+        }
+    }
+
+    private static class TestRowKey implements RowKey {
+
+        private final int id;
+
+        private TestRowKey(int id) {
+            this.id = id;
+        }
+
+        public int getId() {
+            return id;
+        }
+
+        @Override
+        public byte[] getRowKey() {
+            return BytesUtils.intToVar32(id);
+        }
+
+        @Override
+        public boolean equals(Object o) {
+            if (this == o) return true;
+            if (o == null || getClass() != o.getClass()) return false;
+
+            TestRowKey that = (TestRowKey) o;
+
+            return id == that.id;
+        }
+
+        @Override
+        public int hashCode() {
+            return id;
+        }
+    }
+
+    private static class TestColumnName implements ColumnName {
+
+        private final int id;
+        private long count;
+
+        private TestColumnName(int id) {
+            this.id = id;
+        }
+
+        public int getId() {
+            return id;
+        }
+
+        @Override
+        public byte[] getColumnName() {
+            return BytesUtils.intToVar32(id);
+        }
+
+        @Override
+        public long getCallCount() {
+            return count;
+        }
+
+        @Override
+        public void setCallCount(long callCount) {
+            this.count = callCount;
+        }
+
+        @Override
+        public boolean equals(Object o) {
+            if (this == o) return true;
+            if (o == null || getClass() != o.getClass()) return false;
+
+            TestColumnName that = (TestColumnName) o;
+
+            return id == that.id;
+        }
+
+        @Override
+        public int hashCode() {
+            return id;
+        }
+
+        @Override
+        public String toString() {
+            final StringBuilder sb = new StringBuilder("TestColumnName{");
+            sb.append("id=").append(id);
+            sb.append(", count=").append(count);
+            sb.append('}');
+            return sb.toString();
+        }
+    }
+
+}
diff --git a/collector/src/test/java/com/navercorp/pinpoint/collector/receiver/DataReceiverGroupTest.java b/collector/src/test/java/com/navercorp/pinpoint/collector/receiver/DataReceiverGroupTest.java
index 442e88cb9..69168b012 100644
--- a/collector/src/test/java/com/navercorp/pinpoint/collector/receiver/DataReceiverGroupTest.java
+++ b/collector/src/test/java/com/navercorp/pinpoint/collector/receiver/DataReceiverGroupTest.java
@@ -90,7 +90,7 @@ public class DataReceiverGroupTest {
         }
     }
 
-    public TCPReceiverBean createTcpReceiverBean(DataReceiverGroupConfiguration mockConfig, DispatchHandler dispatchHandler) {
+    private TCPReceiverBean createTcpReceiverBean(DataReceiverGroupConfiguration mockConfig, DispatchHandler dispatchHandler) {
         TCPReceiverBean tcpReceiverBean = new TCPReceiverBean();
         tcpReceiverBean.setBeanName("tcpReceiver");
         tcpReceiverBean.setBindIp(mockConfig.getTcpBindIp());
@@ -102,7 +102,7 @@ public class DataReceiverGroupTest {
         return tcpReceiverBean;
     }
 
-    public UDPReceiverBean createUdpReceiverBean(DataReceiverGroupConfiguration mockConfig, DispatchHandler dispatchHandler) {
+    private UDPReceiverBean createUdpReceiverBean(DataReceiverGroupConfiguration mockConfig, DispatchHandler dispatchHandler) {
         UDPReceiverBean udpReceiverBean = new UDPReceiverBean();
         udpReceiverBean.setBeanName("udpReceiver");
         udpReceiverBean.setBindIp(mockConfig.getUdpBindIp());
diff --git a/commons-hbase/pom.xml b/commons-hbase/pom.xml
index 3119ec05c..2d7d8933b 100644
--- a/commons-hbase/pom.xml
+++ b/commons-hbase/pom.xml
@@ -18,7 +18,10 @@
     </properties>
 
     <dependencies>
-
+        <dependency>
+            <groupId>com.navercorp.pinpoint</groupId>
+            <artifactId>pinpoint-annotations</artifactId>
+        </dependency>
         <dependency>
             <groupId>com.navercorp.pinpoint</groupId>
             <artifactId>pinpoint-commons</artifactId>
diff --git a/commons-hbase/src/main/java/com/navercorp/pinpoint/common/hbase/HBaseTables.java b/commons-hbase/src/main/java/com/navercorp/pinpoint/common/hbase/HBaseTables.java
index 4946d474c..aecdcbf88 100644
--- a/commons-hbase/src/main/java/com/navercorp/pinpoint/common/hbase/HBaseTables.java
+++ b/commons-hbase/src/main/java/com/navercorp/pinpoint/common/hbase/HBaseTables.java
@@ -32,59 +32,89 @@ public final class HBaseTables {
     // Time delta (in milliseconds) we can store in each row of AgentStatV2
     public static final int AGENT_STAT_TIMESPAN_MS = 5 * 60 * 1000;
 
-    public static final TableName APPLICATION_TRACE_INDEX = TableName.valueOf("ApplicationTraceIndex");
+
+    public static final String APPLICATION_TRACE_INDEX_STR = "ApplicationTraceIndex";
+    @Deprecated
+    public static final TableName APPLICATION_TRACE_INDEX = TableName.valueOf(APPLICATION_TRACE_INDEX_STR);
     public static final byte[] APPLICATION_TRACE_INDEX_CF_TRACE = Bytes.toBytes("I"); // applicationIndex
     public static final int APPLICATION_TRACE_INDEX_ROW_DISTRIBUTE_SIZE = 1; // applicationIndex hash size
 
-    public static final TableName AGENT_STAT_VER2 = TableName.valueOf("AgentStatV2");
-
+    public static final String AGENT_STAT_VER2_STR = "AgentStatV2";
+    @Deprecated
+    public static final TableName AGENT_STAT_VER2 = TableName.valueOf(AGENT_STAT_VER2_STR);
     public static final byte[] AGENT_STAT_CF_STATISTICS = Bytes.toBytes("S"); // agent statistics column family
 
-    public static final TableName TRACE_V2 = TableName.valueOf("TraceV2");
+    public static final String TRACE_V2_STR = "TraceV2";
+    @Deprecated
+    public static final TableName TRACE_V2 = TableName.valueOf(TRACE_V2_STR);
     public static final byte[] TRACE_V2_CF_SPAN = Bytes.toBytes("S");  //Span
 
-    public static final TableName APPLICATION_INDEX = TableName.valueOf("ApplicationIndex");
+    public static final String APPLICATION_INDEX_STR = "ApplicationIndex";
+    @Deprecated
+    public static final TableName APPLICATION_INDEX = TableName.valueOf(APPLICATION_INDEX_STR);
     public static final byte[] APPLICATION_INDEX_CF_AGENTS = Bytes.toBytes("Agents");
 
-    public static final TableName AGENTINFO = TableName.valueOf("AgentInfo");
+    public static final String AGENTINFO_STR = "AgentInfo";
+    @Deprecated
+    public static final TableName AGENTINFO = TableName.valueOf(AGENTINFO_STR);
     public static final byte[] AGENTINFO_CF_INFO = Bytes.toBytes("Info");
     public static final byte[] AGENTINFO_CF_INFO_IDENTIFIER = Bytes.toBytes("i");
     public static final byte[] AGENTINFO_CF_INFO_SERVER_META_DATA = Bytes.toBytes("m");
     public static final byte[] AGENTINFO_CF_INFO_JVM = Bytes.toBytes("j");
 
-    public static final TableName AGENT_LIFECYCLE = TableName.valueOf("AgentLifeCycle");
+    public static final String AGENT_LIFECYCLE_STR = "AgentLifeCycle";
+    @Deprecated
+    public static final TableName AGENT_LIFECYCLE = TableName.valueOf(AGENT_LIFECYCLE_STR);
     public static final byte[] AGENT_LIFECYCLE_CF_STATUS = Bytes.toBytes("S"); // agent lifecycle column family
     public static final byte[] AGENT_LIFECYCLE_CF_STATUS_QUALI_STATES = Bytes.toBytes("states"); // qualifier for agent lifecycle states
 
-    public static final TableName AGENT_EVENT = TableName.valueOf("AgentEvent");
+    public static final String AGENT_EVENT_STR = "AgentEvent";
+    @Deprecated
+    public static final TableName AGENT_EVENT = TableName.valueOf(AGENT_EVENT_STR);
     public static final byte[] AGENT_EVENT_CF_EVENTS = Bytes.toBytes("E"); // agent events column family
 
-    public static final TableName SQL_METADATA_VER2 = TableName.valueOf("SqlMetaData_Ver2");
+    public static final String SQL_METADATA_VER2_STR = "SqlMetaData_Ver2";
+    @Deprecated
+    public static final TableName SQL_METADATA_VER2 = TableName.valueOf(SQL_METADATA_VER2_STR);
     public static final byte[] SQL_METADATA_VER2_CF_SQL = Bytes.toBytes("Sql");
     public static final byte[] SQL_METADATA_VER2_CF_SQL_QUALI_SQLSTATEMENT = Bytes.toBytes("P_sql_statement");
 
-    public static final TableName STRING_METADATA = TableName.valueOf("StringMetaData");
+    public static final String STRING_METADATA_STR = "StringMetaData";
+    @Deprecated
+    public static final TableName STRING_METADATA = TableName.valueOf(STRING_METADATA_STR);
     public static final byte[] STRING_METADATA_CF_STR = Bytes.toBytes("Str");
     public static final byte[] STRING_METADATA_CF_STR_QUALI_STRING = Bytes.toBytes("P_string");
 
-    public static final TableName API_METADATA = TableName.valueOf("ApiMetaData");
+    public static final String API_METADATA_STR = "ApiMetaData";
+    @Deprecated
+    public static final TableName API_METADATA = TableName.valueOf(API_METADATA_STR);
     public static final byte[] API_METADATA_CF_API = Bytes.toBytes("Api");
     public static final byte[] API_METADATA_CF_API_QUALI_SIGNATURE = Bytes.toBytes("P_api_signature");
 
-    public static final TableName MAP_STATISTICS_CALLER_VER2 = TableName.valueOf("ApplicationMapStatisticsCaller_Ver2");
+    public static final String MAP_STATISTICS_CALLER_VER2_STR = "ApplicationMapStatisticsCaller_Ver2";
+    @Deprecated
+    public static final TableName MAP_STATISTICS_CALLER_VER2 = TableName.valueOf(MAP_STATISTICS_CALLER_VER2_STR);
     public static final byte[] MAP_STATISTICS_CALLER_VER2_CF_COUNTER = Bytes.toBytes("C");
 
-    public static final TableName MAP_STATISTICS_CALLEE_VER2 = TableName.valueOf("ApplicationMapStatisticsCallee_Ver2");
+    public static final String MAP_STATISTICS_CALLEE_VER2_STR = "ApplicationMapStatisticsCallee_Ver2";
+    @Deprecated
+    public static final TableName MAP_STATISTICS_CALLEE_VER2 = TableName.valueOf(MAP_STATISTICS_CALLEE_VER2_STR);
     public static final byte[] MAP_STATISTICS_CALLEE_VER2_CF_COUNTER = Bytes.toBytes("C");
 
-    public static final TableName MAP_STATISTICS_SELF_VER2 = TableName.valueOf("ApplicationMapStatisticsSelf_Ver2");
+    public static final String MAP_STATISTICS_SELF_VER2_STR = "ApplicationMapStatisticsSelf_Ver2";
+    @Deprecated
+    public static final TableName MAP_STATISTICS_SELF_VER2 = TableName.valueOf(MAP_STATISTICS_SELF_VER2_STR);
     public static final byte[] MAP_STATISTICS_SELF_VER2_CF_COUNTER = Bytes.toBytes("C");
 
-    public static final TableName HOST_APPLICATION_MAP_VER2 = TableName.valueOf("HostApplicationMap_Ver2");
+    public static final String HOST_APPLICATION_MAP_VER2_STR = "HostApplicationMap_Ver2";
+    @Deprecated
+    public static final TableName HOST_APPLICATION_MAP_VER2 = TableName.valueOf(HOST_APPLICATION_MAP_VER2_STR);
     public static final byte[] HOST_APPLICATION_MAP_VER2_CF_MAP = Bytes.toBytes("M");
 
     public static final int APPLICATION_STAT_TIMESPAN_MS = 5 * 60 * 1000;
-    public static final TableName APPLICATION_STAT_AGGRE = TableName.valueOf("ApplicationStatAggre");
+    public static final String APPLICATION_STAT_AGGRE_STR = "ApplicationStatAggre";
+    @Deprecated
+    public static final TableName APPLICATION_STAT_AGGRE = TableName.valueOf(APPLICATION_STAT_AGGRE_STR);
     public static final byte[] APPLICATION_STAT_CF_STATISTICS = Bytes.toBytes("S");
 
 }
diff --git a/commons-hbase/src/main/java/com/navercorp/pinpoint/common/hbase/HbaseTableNameProvider.java b/commons-hbase/src/main/java/com/navercorp/pinpoint/common/hbase/HbaseTableNameProvider.java
new file mode 100644
index 000000000..988bfdc71
--- /dev/null
+++ b/commons-hbase/src/main/java/com/navercorp/pinpoint/common/hbase/HbaseTableNameProvider.java
@@ -0,0 +1,62 @@
+/*
+ * Copyright 2017 NAVER Corp.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package com.navercorp.pinpoint.common.hbase;
+
+import com.navercorp.pinpoint.common.annotations.VisibleForTesting;
+import com.navercorp.pinpoint.common.hbase.namespace.HbaseNamespaceValidator;
+import com.navercorp.pinpoint.common.hbase.namespace.NamespaceValidator;
+import com.navercorp.pinpoint.common.hbase.util.HbaseTableNameCache;
+import com.navercorp.pinpoint.common.util.StringUtils;
+import org.apache.hadoop.hbase.NamespaceDescriptor;
+import org.apache.hadoop.hbase.TableName;
+
+import java.util.Objects;
+
+/**
+ * @author HyunGil Jeong
+ */
+public class HbaseTableNameProvider implements TableNameProvider {
+
+    private static final HbaseTableNameCache CACHE = new HbaseTableNameCache();
+
+    private final String namespace;
+
+    public HbaseTableNameProvider(String namespace) {
+        this(namespace, HbaseNamespaceValidator.INSTANCE);
+    }
+
+    @VisibleForTesting
+    HbaseTableNameProvider(String namespace, NamespaceValidator namespaceValidator) {
+        Objects.requireNonNull(namespaceValidator, "namespaceValidator must not be null");
+        this.namespace = requireValidation(namespace, namespaceValidator);
+    }
+
+    private String requireValidation(String namespace, NamespaceValidator namespaceValidator) {
+        if (StringUtils.isEmpty(namespace)) {
+            return NamespaceDescriptor.DEFAULT_NAMESPACE_NAME_STR;
+        }
+        if (!namespaceValidator.validate(namespace)) {
+            throw new IllegalArgumentException("Invalid namespace : " + namespace);
+        }
+        return namespace;
+    }
+
+    @Override
+    public TableName getTableName(String tableName) {
+        return CACHE.get(namespace, tableName);
+    }
+}
diff --git a/web/src/test/java/com/navercorp/pinpoint/web/applicationmap/appender/server/SerialServerInfoAppenderTest.java b/commons-hbase/src/main/java/com/navercorp/pinpoint/common/hbase/TableNameProvider.java
similarity index 64%
rename from web/src/test/java/com/navercorp/pinpoint/web/applicationmap/appender/server/SerialServerInfoAppenderTest.java
rename to commons-hbase/src/main/java/com/navercorp/pinpoint/common/hbase/TableNameProvider.java
index cf184a5b3..f646deac1 100644
--- a/web/src/test/java/com/navercorp/pinpoint/web/applicationmap/appender/server/SerialServerInfoAppenderTest.java
+++ b/commons-hbase/src/main/java/com/navercorp/pinpoint/common/hbase/TableNameProvider.java
@@ -1,5 +1,5 @@
 /*
- * Copyright 2017 NAVER Corp.
+ * Copyright 2018 NAVER Corp.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
  * you may not use this file except in compliance with the License.
@@ -14,15 +14,14 @@
  * limitations under the License.
  */
 
-package com.navercorp.pinpoint.web.applicationmap.appender.server;
+package com.navercorp.pinpoint.common.hbase;
+
+import org.apache.hadoop.hbase.TableName;
 
 /**
  * @author HyunGil Jeong
  */
-public class SerialServerInfoAppenderTest extends ServerInfoAppenderTestBase {
+public interface TableNameProvider {
 
-    @Override
-    protected ServerInfoAppenderFactory createServerInfoAppenderFactory() {
-        return new ServerInfoAppenderFactory("serial", 1);
-    }
+    TableName getTableName(String tableName);
 }
diff --git a/commons-hbase/src/main/java/com/navercorp/pinpoint/common/hbase/namespace/HbaseNamespaceValidator.java b/commons-hbase/src/main/java/com/navercorp/pinpoint/common/hbase/namespace/HbaseNamespaceValidator.java
new file mode 100644
index 000000000..8f26c7497
--- /dev/null
+++ b/commons-hbase/src/main/java/com/navercorp/pinpoint/common/hbase/namespace/HbaseNamespaceValidator.java
@@ -0,0 +1,43 @@
+/*
+ * Copyright 2018 NAVER Corp.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package com.navercorp.pinpoint.common.hbase.namespace;
+
+import org.apache.hadoop.hbase.TableName;
+
+import java.util.regex.Pattern;
+
+/**
+ * @author HyunGil Jeong
+ */
+public class HbaseNamespaceValidator implements NamespaceValidator {
+
+    public static final NamespaceValidator INSTANCE = new HbaseNamespaceValidator();
+
+    private static final Pattern VALID_NAMESPACE = Pattern.compile(TableName.VALID_NAMESPACE_REGEX);
+
+    private HbaseNamespaceValidator() {
+
+    }
+
+    @Override
+    public boolean validate(String namespace) {
+        if (namespace == null) {
+            return false;
+        }
+        return VALID_NAMESPACE.matcher(namespace).matches();
+    }
+}
diff --git a/web/src/test/java/com/navercorp/pinpoint/web/applicationmap/appender/server/ParallelServerInfoAppenderTest.java b/commons-hbase/src/main/java/com/navercorp/pinpoint/common/hbase/namespace/NamespaceValidator.java
similarity index 64%
rename from web/src/test/java/com/navercorp/pinpoint/web/applicationmap/appender/server/ParallelServerInfoAppenderTest.java
rename to commons-hbase/src/main/java/com/navercorp/pinpoint/common/hbase/namespace/NamespaceValidator.java
index 53a9d91e9..5b06db6f8 100644
--- a/web/src/test/java/com/navercorp/pinpoint/web/applicationmap/appender/server/ParallelServerInfoAppenderTest.java
+++ b/commons-hbase/src/main/java/com/navercorp/pinpoint/common/hbase/namespace/NamespaceValidator.java
@@ -1,5 +1,5 @@
 /*
- * Copyright 2017 NAVER Corp.
+ * Copyright 2018 NAVER Corp.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
  * you may not use this file except in compliance with the License.
@@ -14,15 +14,12 @@
  * limitations under the License.
  */
 
-package com.navercorp.pinpoint.web.applicationmap.appender.server;
+package com.navercorp.pinpoint.common.hbase.namespace;
 
 /**
  * @author HyunGil Jeong
  */
-public class ParallelServerInfoAppenderTest extends ServerInfoAppenderTestBase {
+public interface NamespaceValidator {
 
-    @Override
-    protected ServerInfoAppenderFactory createServerInfoAppenderFactory() {
-        return new ServerInfoAppenderFactory("parallel", 16);
-    }
+    boolean validate(String namespace);
 }
diff --git a/commons-hbase/src/main/java/com/navercorp/pinpoint/common/hbase/util/HbaseTableNameCache.java b/commons-hbase/src/main/java/com/navercorp/pinpoint/common/hbase/util/HbaseTableNameCache.java
new file mode 100644
index 000000000..1f836c448
--- /dev/null
+++ b/commons-hbase/src/main/java/com/navercorp/pinpoint/common/hbase/util/HbaseTableNameCache.java
@@ -0,0 +1,96 @@
+/*
+ * Copyright 2018 NAVER Corp.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package com.navercorp.pinpoint.common.hbase.util;
+
+import com.navercorp.pinpoint.common.util.StringUtils;
+import org.apache.hadoop.hbase.NamespaceDescriptor;
+import org.apache.hadoop.hbase.TableName;
+
+import java.util.Map;
+import java.util.Objects;
+import java.util.concurrent.ConcurrentHashMap;
+
+/**
+ * @author HyunGil Jeong
+ */
+public class HbaseTableNameCache {
+
+    private final Map<TableNameKey, TableName> tableNameCache = new ConcurrentHashMap<>();
+
+    public TableName get(String qualifier) {
+        return get(NamespaceDescriptor.DEFAULT_NAMESPACE_NAME_STR, qualifier);
+    }
+
+    public TableName get(String namespace, String qualifier) {
+        Objects.requireNonNull(qualifier, "qualifier must not be null");
+        String nonEmptyNamespace = StringUtils.isEmpty(namespace) ? NamespaceDescriptor.DEFAULT_NAMESPACE_NAME_STR : namespace;
+
+        TableNameKey tableNameKey = new TableNameKey(nonEmptyNamespace, qualifier);
+        TableName tableName = tableNameCache.get(tableNameKey);
+        if (tableName != null) {
+            return tableName;
+        }
+        tableName = TableName.valueOf(tableNameKey.getNamespace(), tableNameKey.getQualifier());
+        TableName prevTableName = tableNameCache.putIfAbsent(tableNameKey, tableName);
+        if (prevTableName != null) {
+            return prevTableName;
+        }
+        return tableName;
+    }
+
+    private static class TableNameKey {
+
+        private final String namespace;
+        private final String qualifier;
+
+        private TableNameKey(String namespace, String qualifier) {
+            this.namespace = Objects.requireNonNull(namespace, "namespace must not be null");
+            this.qualifier = Objects.requireNonNull(qualifier, "qualifier must not be null");
+        }
+
+        public String getNamespace() {
+            return namespace;
+        }
+
+        public String getQualifier() {
+            return qualifier;
+        }
+
+        @Override
+        public boolean equals(Object o) {
+            if (this == o) return true;
+            if (o == null || getClass() != o.getClass()) return false;
+
+            TableNameKey that = (TableNameKey) o;
+
+            if (!namespace.equals(that.namespace)) return false;
+            return qualifier.equals(that.qualifier);
+        }
+
+        @Override
+        public int hashCode() {
+            int result = namespace.hashCode();
+            result = 31 * result + qualifier.hashCode();
+            return result;
+        }
+
+        @Override
+        public String toString() {
+            return namespace + ":" + qualifier;
+        }
+    }
+}
diff --git a/commons-hbase/src/test/java/com/navercorp/pinpoint/common/hbase/HbaseTableNameProviderTest.java b/commons-hbase/src/test/java/com/navercorp/pinpoint/common/hbase/HbaseTableNameProviderTest.java
new file mode 100644
index 000000000..b4fc2b5f2
--- /dev/null
+++ b/commons-hbase/src/test/java/com/navercorp/pinpoint/common/hbase/HbaseTableNameProviderTest.java
@@ -0,0 +1,98 @@
+/*
+ * Copyright 2018 NAVER Corp.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package com.navercorp.pinpoint.common.hbase;
+
+import com.navercorp.pinpoint.common.hbase.namespace.NamespaceValidator;
+import org.apache.hadoop.hbase.NamespaceDescriptor;
+import org.apache.hadoop.hbase.TableName;
+import org.junit.Assert;
+import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.mockito.Mock;
+import org.mockito.junit.MockitoJUnitRunner;
+
+import static org.mockito.ArgumentMatchers.anyString;
+import static org.mockito.Mockito.never;
+import static org.mockito.Mockito.only;
+import static org.mockito.Mockito.verify;
+import static org.mockito.Mockito.when;
+
+/**
+ * @author HyunGil Jeong
+ */
+@RunWith(MockitoJUnitRunner.class)
+public class HbaseTableNameProviderTest {
+
+    private static final String TABLE_QUALIFIER = "testTable";
+
+    @Mock
+    private NamespaceValidator namespaceValidator;
+
+    @Test
+    public void nullNamespaceShouldReturnDefaultNamespace() {
+        // Given
+        final String nullNamespace = null;
+        // When
+        final TableNameProvider tableNameProvider = new HbaseTableNameProvider(nullNamespace, namespaceValidator);
+        final TableName tableName = tableNameProvider.getTableName(TABLE_QUALIFIER);
+        // Then
+        verify(namespaceValidator, never()).validate(anyString());
+        Assert.assertEquals(NamespaceDescriptor.DEFAULT_NAMESPACE_NAME_STR, tableName.getNamespaceAsString());
+        Assert.assertEquals(TABLE_QUALIFIER, tableName.getQualifierAsString());
+    }
+
+    @Test
+    public void emptyNamespaceShouldReturnDefaultNamespace() {
+        // Given
+        final String emptyNamespace = "";
+        // When
+        final TableNameProvider tableNameProvider = new HbaseTableNameProvider(emptyNamespace, namespaceValidator);
+        final TableName tableName = tableNameProvider.getTableName(TABLE_QUALIFIER);
+        // Then
+        verify(namespaceValidator, never()).validate(anyString());
+        Assert.assertEquals(NamespaceDescriptor.DEFAULT_NAMESPACE_NAME_STR, tableName.getNamespaceAsString());
+        Assert.assertEquals(TABLE_QUALIFIER, tableName.getQualifierAsString());
+    }
+
+    @Test
+    public void validNamespace() {
+        // Given
+        final String validNamespace = "namespace";
+        when(namespaceValidator.validate(validNamespace)).thenReturn(true);
+        // When
+        final TableNameProvider tableNameProvider = new HbaseTableNameProvider(validNamespace, namespaceValidator);
+        final TableName tableName = tableNameProvider.getTableName(TABLE_QUALIFIER);
+        // Then
+        verify(namespaceValidator, only()).validate(validNamespace);
+        Assert.assertEquals(validNamespace, tableName.getNamespaceAsString());
+        Assert.assertEquals(TABLE_QUALIFIER, tableName.getQualifierAsString());
+    }
+
+    @Test
+    public void invalidNamespace() {
+        // Given
+        final String invalidNamespace = "invalidNamespace";
+        when(namespaceValidator.validate(invalidNamespace)).thenReturn(false);
+        // When
+        try {
+            new HbaseTableNameProvider(invalidNamespace, namespaceValidator);
+            Assert.fail("Expected IllegalArgumentException to be thrown");
+        } catch (IllegalArgumentException e) {
+            verify(namespaceValidator, only()).validate(invalidNamespace);
+        }
+    }
+}
diff --git a/commons-hbase/src/test/java/com/navercorp/pinpoint/common/hbase/namespace/HbaseNamespaceValidatorTest.java b/commons-hbase/src/test/java/com/navercorp/pinpoint/common/hbase/namespace/HbaseNamespaceValidatorTest.java
new file mode 100644
index 000000000..484e800cd
--- /dev/null
+++ b/commons-hbase/src/test/java/com/navercorp/pinpoint/common/hbase/namespace/HbaseNamespaceValidatorTest.java
@@ -0,0 +1,51 @@
+/*
+ * Copyright 2018 NAVER Corp.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package com.navercorp.pinpoint.common.hbase.namespace;
+
+import org.junit.Assert;
+import org.junit.Test;
+
+/**
+ * @author HyunGil Jeong
+ */
+public class HbaseNamespaceValidatorTest {
+
+    @Test
+    public void testValidate() {
+        String valid1 = "a";
+        String valid2 = "_a0";
+        String valid3 = "0_Z";
+        String valid4 = "C09_";
+
+        String invalid1 = "";
+        String invalid2 = "-";
+        String invalid3 = "abc-1";
+        String invalid4 = "abc!";
+        String invalid5 = null;
+
+        Assert.assertTrue(valid1 + " should be valid.", HbaseNamespaceValidator.INSTANCE.validate(valid1));
+        Assert.assertTrue(valid2 + " should be valid.", HbaseNamespaceValidator.INSTANCE.validate(valid2));
+        Assert.assertTrue(valid3 + " should be valid.", HbaseNamespaceValidator.INSTANCE.validate(valid3));
+        Assert.assertTrue(valid4 + " should be valid.", HbaseNamespaceValidator.INSTANCE.validate(valid4));
+
+        Assert.assertFalse(invalid1 + " should be invalid.", HbaseNamespaceValidator.INSTANCE.validate(invalid1));
+        Assert.assertFalse(invalid2 + " should be invalid.", HbaseNamespaceValidator.INSTANCE.validate(invalid2));
+        Assert.assertFalse(invalid3 + " should be invalid.", HbaseNamespaceValidator.INSTANCE.validate(invalid3));
+        Assert.assertFalse(invalid4 + " should be invalid.", HbaseNamespaceValidator.INSTANCE.validate(invalid4));
+        Assert.assertFalse(invalid4 + " should be invalid.", HbaseNamespaceValidator.INSTANCE.validate(invalid5));
+    }
+}
diff --git a/commons-hbase/src/test/java/com/navercorp/pinpoint/common/hbase/util/HbaseTableNameCacheTest.java b/commons-hbase/src/test/java/com/navercorp/pinpoint/common/hbase/util/HbaseTableNameCacheTest.java
new file mode 100644
index 000000000..c81e7ca38
--- /dev/null
+++ b/commons-hbase/src/test/java/com/navercorp/pinpoint/common/hbase/util/HbaseTableNameCacheTest.java
@@ -0,0 +1,71 @@
+/*
+ * Copyright 2018 NAVER Corp.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package com.navercorp.pinpoint.common.hbase.util;
+
+import org.apache.hadoop.hbase.NamespaceDescriptor;
+import org.apache.hadoop.hbase.TableName;
+import org.junit.Assert;
+import org.junit.Test;
+
+/**
+ * @author HyunGil Jeong
+ */
+public class HbaseTableNameCacheTest {
+
+    private final HbaseTableNameCache cache = new HbaseTableNameCache();
+
+    @Test
+    public void emptyNamespaceShouldReturnDefaultNamespace() {
+        // Given
+        final String nullNamespace = null;
+        final String emptyNamespace = "";
+        final String qualifier = "table";
+        // When
+        TableName tableName1 = cache.get(qualifier);
+        TableName tableName2 = cache.get(nullNamespace, qualifier);
+        TableName tableName3 = cache.get(emptyNamespace, qualifier);
+        // Then
+        Assert.assertEquals(NamespaceDescriptor.DEFAULT_NAMESPACE_NAME_STR, tableName1.getNamespaceAsString());
+        Assert.assertEquals(qualifier, tableName1.getQualifierAsString());
+        Assert.assertEquals(NamespaceDescriptor.DEFAULT_NAMESPACE_NAME_STR, tableName2.getNamespaceAsString());
+        Assert.assertEquals(qualifier, tableName2.getQualifierAsString());
+        Assert.assertEquals(NamespaceDescriptor.DEFAULT_NAMESPACE_NAME_STR, tableName3.getNamespaceAsString());
+        Assert.assertEquals(qualifier, tableName3.getQualifierAsString());
+    }
+
+    @Test
+    public void specifiedNamespace() {
+        // Given
+        final String namespace = "namespace";
+        final String qualifier = "table";
+        // When
+        TableName tableName = cache.get(namespace, qualifier);
+        // Then
+        Assert.assertEquals(namespace, tableName.getNamespaceAsString());
+        Assert.assertEquals(qualifier, tableName.getQualifierAsString());
+    }
+
+    @Test(expected = NullPointerException.class)
+    public void nullQualifierShouldThrowException() {
+        // Given
+        final String nullQualifier = null;
+        // When
+        cache.get(nullQualifier);
+        // Then
+        Assert.fail();
+    }
+}
diff --git a/commons/src/main/java/com/navercorp/pinpoint/common/util/JvmVersion.java b/commons/src/main/java/com/navercorp/pinpoint/common/util/JvmVersion.java
index 65e88a061..dad0c5ce1 100644
--- a/commons/src/main/java/com/navercorp/pinpoint/common/util/JvmVersion.java
+++ b/commons/src/main/java/com/navercorp/pinpoint/common/util/JvmVersion.java
@@ -29,6 +29,7 @@ public enum JvmVersion {
     JAVA_6(1.6f, 50),
     JAVA_7(1.7f, 51),
     JAVA_8(1.8f, 52),
+    JAVA_9(9f, 53),
     UNSUPPORTED(-1, -1);
 
     private final float version;
diff --git a/commons/src/test/java/com/navercorp/pinpoint/common/util/JvmVersionTest.java b/commons/src/test/java/com/navercorp/pinpoint/common/util/JvmVersionTest.java
index 4e2a18b03..d49a9192b 100644
--- a/commons/src/test/java/com/navercorp/pinpoint/common/util/JvmVersionTest.java
+++ b/commons/src/test/java/com/navercorp/pinpoint/common/util/JvmVersionTest.java
@@ -21,8 +21,6 @@ import static com.navercorp.pinpoint.common.util.JvmVersion.*;
 
 import org.junit.Test;
 
-import com.navercorp.pinpoint.common.util.JvmVersion;
-
 /**
  * @author hyungil.jeong
  */
@@ -35,30 +33,42 @@ public class JvmVersionTest {
         assertFalse(JAVA_5.onOrAfter(JAVA_6));
         assertFalse(JAVA_5.onOrAfter(JAVA_7));
         assertFalse(JAVA_5.onOrAfter(JAVA_8));
+        assertFalse(JAVA_5.onOrAfter(JAVA_9));
         assertFalse(JAVA_5.onOrAfter(UNSUPPORTED));
         // JDK 6
         assertTrue(JAVA_6.onOrAfter(JAVA_5));
         assertTrue(JAVA_6.onOrAfter(JAVA_6));
         assertFalse(JAVA_6.onOrAfter(JAVA_7));
         assertFalse(JAVA_6.onOrAfter(JAVA_8));
+        assertFalse(JAVA_6.onOrAfter(JAVA_9));
         assertFalse(JAVA_6.onOrAfter(UNSUPPORTED));
         // JDK 7
         assertTrue(JAVA_7.onOrAfter(JAVA_5));
         assertTrue(JAVA_7.onOrAfter(JAVA_6));
         assertTrue(JAVA_7.onOrAfter(JAVA_7));
         assertFalse(JAVA_7.onOrAfter(JAVA_8));
+        assertFalse(JAVA_7.onOrAfter(JAVA_9));
         assertFalse(JAVA_7.onOrAfter(UNSUPPORTED));
         // JDK 8
         assertTrue(JAVA_8.onOrAfter(JAVA_5));
         assertTrue(JAVA_8.onOrAfter(JAVA_6));
         assertTrue(JAVA_8.onOrAfter(JAVA_7));
         assertTrue(JAVA_8.onOrAfter(JAVA_8));
+        assertFalse(JAVA_8.onOrAfter(JAVA_9));
         assertFalse(JAVA_8.onOrAfter(UNSUPPORTED));
+        // JDK 9
+        assertTrue(JAVA_9.onOrAfter(JAVA_5));
+        assertTrue(JAVA_9.onOrAfter(JAVA_6));
+        assertTrue(JAVA_9.onOrAfter(JAVA_7));
+        assertTrue(JAVA_9.onOrAfter(JAVA_8));
+        assertTrue(JAVA_9.onOrAfter(JAVA_9));
+        assertFalse(JAVA_9.onOrAfter(UNSUPPORTED));
         // Unsupported
         assertFalse(UNSUPPORTED.onOrAfter(JAVA_5));
         assertFalse(UNSUPPORTED.onOrAfter(JAVA_6));
         assertFalse(UNSUPPORTED.onOrAfter(JAVA_7));
         assertFalse(UNSUPPORTED.onOrAfter(JAVA_8));
+        assertFalse(UNSUPPORTED.onOrAfter(JAVA_9));
         assertFalse(UNSUPPORTED.onOrAfter(UNSUPPORTED));
     }
 
@@ -66,56 +76,65 @@ public class JvmVersionTest {
     public void testGetFromDoubleVersion() {
         // JDK 5
         final JvmVersion java_5 = JvmVersion.getFromVersion(1.5f);
-        assertSame(java_5, JAVA_5);
+        assertSame(JAVA_5, java_5);
         // JDK 6
         final JvmVersion java_6 = JvmVersion.getFromVersion(1.6f);
-        assertSame(java_6, JAVA_6);
+        assertSame(JAVA_6, java_6);
         // JDK 7
         final JvmVersion java_7 = JvmVersion.getFromVersion(1.7f);
-        assertSame(java_7, JAVA_7);
+        assertSame(JAVA_7, java_7);
         // JDK 8
         final JvmVersion java_8 = JvmVersion.getFromVersion(1.8f);
-        assertSame(java_8, JAVA_8);
+        assertSame(JAVA_8, java_8);
+        // JDK 9
+        final JvmVersion java_9 = JvmVersion.getFromVersion(9f);
+        assertSame(JAVA_9, java_9);
         // Unsupported
         final JvmVersion java_unsupported = JvmVersion.getFromVersion(0.9f);
-        assertSame(java_unsupported, UNSUPPORTED);
+        assertSame(UNSUPPORTED, java_unsupported);
     }
 
     @Test
     public void testGetFromStringVersion() {
         // JDK 5
         final JvmVersion java_5 = JvmVersion.getFromVersion("1.5");
-        assertSame(java_5, JAVA_5);
+        assertSame(JAVA_5, java_5);
         // JDK 6
         final JvmVersion java_6 = JvmVersion.getFromVersion("1.6");
-        assertSame(java_6, JAVA_6);
+        assertSame(JAVA_6, java_6);
         // JDK 7
         final JvmVersion java_7 = JvmVersion.getFromVersion("1.7");
-        assertSame(java_7, JAVA_7);
+        assertSame(JAVA_7, java_7);
         // JDK 8
         final JvmVersion java_8 = JvmVersion.getFromVersion("1.8");
-        assertSame(java_8, JAVA_8);
+        assertSame(JAVA_8, java_8);
+        // JDK 9
+        final JvmVersion java_9 = JvmVersion.getFromVersion("9");
+        assertSame(JAVA_9, java_9);
         // Unsupported
         final JvmVersion java_unsupported = JvmVersion.getFromVersion("abc");
-        assertSame(java_unsupported, UNSUPPORTED);
+        assertSame(UNSUPPORTED, java_unsupported);
     }
 
     @Test
     public void testGetFromClassVersion() {
         // JDK 5
         final JvmVersion java_5 = JvmVersion.getFromClassVersion(49);
-        assertSame(java_5, JAVA_5);
+        assertSame(JAVA_5, java_5);
         // JDK 6
         final JvmVersion java_6 = JvmVersion.getFromClassVersion(50);
-        assertSame(java_6, JAVA_6);
+        assertSame(JAVA_6, java_6);
         // JDK 7
         final JvmVersion java_7 = JvmVersion.getFromClassVersion(51);
-        assertSame(java_7, JAVA_7);
+        assertSame(JAVA_7, java_7);
         // JDK 8
         final JvmVersion java_8 = JvmVersion.getFromClassVersion(52);
-        assertSame(java_8, JAVA_8);
+        assertSame(JAVA_8, java_8);
+        // JDK 9
+        final JvmVersion java_9 = JvmVersion.getFromClassVersion(53);
+        assertSame(JAVA_9, java_9);
         // Unsupported
         final JvmVersion java_unsupported = JvmVersion.getFromClassVersion(-1);
-        assertSame(java_unsupported, UNSUPPORTED);
+        assertSame(UNSUPPORTED, java_unsupported);
     }
 }
diff --git a/flink/src/main/java/com/navercorp/pinpoint/flink/dao/hbase/ActiveTraceDao.java b/flink/src/main/java/com/navercorp/pinpoint/flink/dao/hbase/ActiveTraceDao.java
index 622696592..a3bf7bbe8 100644
--- a/flink/src/main/java/com/navercorp/pinpoint/flink/dao/hbase/ActiveTraceDao.java
+++ b/flink/src/main/java/com/navercorp/pinpoint/flink/dao/hbase/ActiveTraceDao.java
@@ -17,6 +17,7 @@ package com.navercorp.pinpoint.flink.dao.hbase;
 
 import com.navercorp.pinpoint.common.hbase.HBaseTables;
 import com.navercorp.pinpoint.common.hbase.HbaseTemplate2;
+import com.navercorp.pinpoint.common.hbase.TableNameProvider;
 import com.navercorp.pinpoint.common.server.bo.serializer.stat.ApplicationStatHbaseOperationFactory;
 import com.navercorp.pinpoint.common.server.bo.serializer.stat.join.ActiveTraceSerializer;
 import com.navercorp.pinpoint.common.server.bo.stat.join.JoinStatBo;
@@ -40,12 +41,13 @@ public class ActiveTraceDao {
     private final HbaseTemplate2 hbaseTemplate2;
     private final ApplicationStatHbaseOperationFactory applicationStatHbaseOperationFactory;
     private final ActiveTraceSerializer activeTraceSerializer;
-    private final TableName APPLICATION_STAT_AGGRE = HBaseTables.APPLICATION_STAT_AGGRE;
+    private final TableNameProvider tableNameProvider;
 
-    public ActiveTraceDao(HbaseTemplate2 hbaseTemplate2, ApplicationStatHbaseOperationFactory applicationStatHbaseOperationFactory, ActiveTraceSerializer activeTraceSerializer) {
+    public ActiveTraceDao(HbaseTemplate2 hbaseTemplate2, ApplicationStatHbaseOperationFactory applicationStatHbaseOperationFactory, ActiveTraceSerializer activeTraceSerializer, TableNameProvider tableNameProvider) {
         this.hbaseTemplate2 = Objects.requireNonNull(hbaseTemplate2, "hbaseTemplate2 must not be null");
         this.applicationStatHbaseOperationFactory = Objects.requireNonNull(applicationStatHbaseOperationFactory, "applicationStatHbaseOperationFactory must not be null");
         this.activeTraceSerializer = Objects.requireNonNull(activeTraceSerializer, "activeTraceSerializer must not be null");
+        this.tableNameProvider = Objects.requireNonNull(tableNameProvider, "tableNameProvider must not be null");
     }
 
     public void insert(String id, long timestamp, List<JoinStatBo> joinActiveTraceBoList, StatType statType) {
@@ -54,9 +56,10 @@ public class ActiveTraceDao {
         }
         List<Put> activeTracePuts = applicationStatHbaseOperationFactory.createPuts(id, joinActiveTraceBoList, statType, activeTraceSerializer);
         if (!activeTracePuts.isEmpty()) {
-            List<Put> rejectedPuts = hbaseTemplate2.asyncPut(APPLICATION_STAT_AGGRE, activeTracePuts);
+            TableName applicationStatAggreTableName = tableNameProvider.getTableName(HBaseTables.APPLICATION_STAT_AGGRE_STR);
+            List<Put> rejectedPuts = hbaseTemplate2.asyncPut(applicationStatAggreTableName, activeTracePuts);
             if (CollectionUtils.isNotEmpty(rejectedPuts)) {
-                hbaseTemplate2.put(APPLICATION_STAT_AGGRE, rejectedPuts);
+                hbaseTemplate2.put(applicationStatAggreTableName, rejectedPuts);
             }
         }
     }
diff --git a/flink/src/main/java/com/navercorp/pinpoint/flink/dao/hbase/CpuLoadDao.java b/flink/src/main/java/com/navercorp/pinpoint/flink/dao/hbase/CpuLoadDao.java
index 86207dadf..27df9716d 100644
--- a/flink/src/main/java/com/navercorp/pinpoint/flink/dao/hbase/CpuLoadDao.java
+++ b/flink/src/main/java/com/navercorp/pinpoint/flink/dao/hbase/CpuLoadDao.java
@@ -17,6 +17,7 @@ package com.navercorp.pinpoint.flink.dao.hbase;
 
 import com.navercorp.pinpoint.common.hbase.HBaseTables;
 import com.navercorp.pinpoint.common.hbase.HbaseTemplate2;
+import com.navercorp.pinpoint.common.hbase.TableNameProvider;
 import com.navercorp.pinpoint.common.server.bo.serializer.stat.ApplicationStatHbaseOperationFactory;
 import com.navercorp.pinpoint.common.server.bo.serializer.stat.join.CpuLoadSerializer;
 import com.navercorp.pinpoint.common.server.bo.stat.join.JoinStatBo;
@@ -40,12 +41,13 @@ public class CpuLoadDao {
     private final HbaseTemplate2 hbaseTemplate2;
     private final ApplicationStatHbaseOperationFactory applicationStatHbaseOperationFactory;
     private final CpuLoadSerializer cpuLoadSerializer;
-    private final TableName APPLICATION_STAT_AGGRE = HBaseTables.APPLICATION_STAT_AGGRE;
+    private final TableNameProvider tableNameProvider;
 
-    public CpuLoadDao(HbaseTemplate2 hbaseTemplate2, ApplicationStatHbaseOperationFactory applicationStatHbaseOperationFactory, CpuLoadSerializer cpuLoadSerializer) {
+    public CpuLoadDao(HbaseTemplate2 hbaseTemplate2, ApplicationStatHbaseOperationFactory applicationStatHbaseOperationFactory, CpuLoadSerializer cpuLoadSerializer, TableNameProvider tableNameProvider) {
         this.hbaseTemplate2 = Objects.requireNonNull(hbaseTemplate2, "hbaseTemplate2 must not be null");
         this.applicationStatHbaseOperationFactory = Objects.requireNonNull(applicationStatHbaseOperationFactory, "applicationStatHbaseOperationFactory must not be null");
         this.cpuLoadSerializer = Objects.requireNonNull(cpuLoadSerializer, "cpuLoadSerializer must not be null");
+        this.tableNameProvider = Objects.requireNonNull(tableNameProvider, "tableNameProvider must not be null");
     }
 
     public void insert(String id, long timestamp, List<JoinStatBo> joinCpuLoadBoList, StatType statType) {
@@ -54,9 +56,10 @@ public class CpuLoadDao {
         }
         List<Put> cpuLoadPuts = applicationStatHbaseOperationFactory.createPuts(id, joinCpuLoadBoList, statType, cpuLoadSerializer);
         if (!cpuLoadPuts.isEmpty()) {
-            List<Put> rejectedPuts = hbaseTemplate2.asyncPut(APPLICATION_STAT_AGGRE, cpuLoadPuts);
+            TableName applicationStatAggreTableName = tableNameProvider.getTableName(HBaseTables.APPLICATION_STAT_AGGRE_STR);
+            List<Put> rejectedPuts = hbaseTemplate2.asyncPut(applicationStatAggreTableName, cpuLoadPuts);
             if (CollectionUtils.isNotEmpty(rejectedPuts)) {
-                hbaseTemplate2.put(APPLICATION_STAT_AGGRE, rejectedPuts);
+                hbaseTemplate2.put(applicationStatAggreTableName, rejectedPuts);
             }
         }
     }
diff --git a/flink/src/main/java/com/navercorp/pinpoint/flink/dao/hbase/DataSourceDao.java b/flink/src/main/java/com/navercorp/pinpoint/flink/dao/hbase/DataSourceDao.java
index 3ba3577b6..94d16ef6d 100644
--- a/flink/src/main/java/com/navercorp/pinpoint/flink/dao/hbase/DataSourceDao.java
+++ b/flink/src/main/java/com/navercorp/pinpoint/flink/dao/hbase/DataSourceDao.java
@@ -17,6 +17,7 @@ package com.navercorp.pinpoint.flink.dao.hbase;
 
 import com.navercorp.pinpoint.common.hbase.HBaseTables;
 import com.navercorp.pinpoint.common.hbase.HbaseTemplate2;
+import com.navercorp.pinpoint.common.hbase.TableNameProvider;
 import com.navercorp.pinpoint.common.server.bo.serializer.stat.ApplicationStatHbaseOperationFactory;
 import com.navercorp.pinpoint.common.server.bo.serializer.stat.join.DataSourceSerializer;
 import com.navercorp.pinpoint.common.server.bo.stat.join.JoinStatBo;
@@ -40,12 +41,13 @@ public class DataSourceDao {
     private final HbaseTemplate2 hbaseTemplate2;
     private final ApplicationStatHbaseOperationFactory applicationStatHbaseOperationFactory;
     private final DataSourceSerializer dataSourceSerializer;
-    private final TableName APPLICATION_STAT_AGGRE = HBaseTables.APPLICATION_STAT_AGGRE;
+    private final TableNameProvider tableNameProvider;
 
-    public DataSourceDao(HbaseTemplate2 hbaseTemplate2, ApplicationStatHbaseOperationFactory applicationStatHbaseOperationFactory, DataSourceSerializer dataSourceSerializer) {
+    public DataSourceDao(HbaseTemplate2 hbaseTemplate2, ApplicationStatHbaseOperationFactory applicationStatHbaseOperationFactory, DataSourceSerializer dataSourceSerializer, TableNameProvider tableNameProvider) {
         this.hbaseTemplate2 = Objects.requireNonNull(hbaseTemplate2, "hbaseTemplate2 must not be null");
         this.applicationStatHbaseOperationFactory = Objects.requireNonNull(applicationStatHbaseOperationFactory, "applicationStatHbaseOperationFactory must not be null");
         this.dataSourceSerializer = Objects.requireNonNull(dataSourceSerializer, "dataSourceSerializer must not be null");
+        this.tableNameProvider = Objects.requireNonNull(tableNameProvider, "tableNameProvider must not be null");
     }
 
     public void insert(String id, long timestamp, List<JoinStatBo> joinResponseTimeBoList, StatType statType) {
@@ -54,9 +56,10 @@ public class DataSourceDao {
         }
         List<Put> responseTimePuts = applicationStatHbaseOperationFactory.createPuts(id, joinResponseTimeBoList, statType, dataSourceSerializer);
         if (!responseTimePuts.isEmpty()) {
-            List<Put> rejectedPuts = hbaseTemplate2.asyncPut(APPLICATION_STAT_AGGRE, responseTimePuts);
+            TableName applicationStatAggreTableName = tableNameProvider.getTableName(HBaseTables.APPLICATION_STAT_AGGRE_STR);
+            List<Put> rejectedPuts = hbaseTemplate2.asyncPut(applicationStatAggreTableName, responseTimePuts);
             if (CollectionUtils.isNotEmpty(rejectedPuts)) {
-                hbaseTemplate2.put(APPLICATION_STAT_AGGRE, rejectedPuts);
+                hbaseTemplate2.put(applicationStatAggreTableName, rejectedPuts);
             }
         }
     }
diff --git a/flink/src/main/java/com/navercorp/pinpoint/flink/dao/hbase/MemoryDao.java b/flink/src/main/java/com/navercorp/pinpoint/flink/dao/hbase/MemoryDao.java
index 564697ef2..01e861ec9 100644
--- a/flink/src/main/java/com/navercorp/pinpoint/flink/dao/hbase/MemoryDao.java
+++ b/flink/src/main/java/com/navercorp/pinpoint/flink/dao/hbase/MemoryDao.java
@@ -17,6 +17,7 @@ package com.navercorp.pinpoint.flink.dao.hbase;
 
 import com.navercorp.pinpoint.common.hbase.HBaseTables;
 import com.navercorp.pinpoint.common.hbase.HbaseTemplate2;
+import com.navercorp.pinpoint.common.hbase.TableNameProvider;
 import com.navercorp.pinpoint.common.server.bo.serializer.stat.ApplicationStatHbaseOperationFactory;
 import com.navercorp.pinpoint.common.server.bo.serializer.stat.join.MemorySerializer;
 import com.navercorp.pinpoint.common.server.bo.stat.join.JoinStatBo;
@@ -40,12 +41,13 @@ public class MemoryDao {
     private final HbaseTemplate2 hbaseTemplate2;
     private final ApplicationStatHbaseOperationFactory applicationStatHbaseOperationFactory;
     private final MemorySerializer memorySerializer;
-    private final TableName APPLICATION_STAT_AGGRE = HBaseTables.APPLICATION_STAT_AGGRE;
+    private final TableNameProvider tableNameProvider;
 
-    public MemoryDao(HbaseTemplate2 hbaseTemplate2, ApplicationStatHbaseOperationFactory applicationStatHbaseOperationFactory, MemorySerializer memorySerializer) {
+    public MemoryDao(HbaseTemplate2 hbaseTemplate2, ApplicationStatHbaseOperationFactory applicationStatHbaseOperationFactory, MemorySerializer memorySerializer, TableNameProvider tableNameProvider) {
         this.hbaseTemplate2 = Objects.requireNonNull(hbaseTemplate2, "hbaseTemplate2 must not be null");
         this.applicationStatHbaseOperationFactory = Objects.requireNonNull(applicationStatHbaseOperationFactory, "applicationStatHbaseOperationFactory must not be null");
         this.memorySerializer = Objects.requireNonNull(memorySerializer, "memorySerializer must not be null");
+        this.tableNameProvider = Objects.requireNonNull(tableNameProvider, "tableNameProvider must not be null");
     }
 
     public void insert(String id, long timestamp, List<JoinStatBo> joinMemoryBoList, StatType statType) {
@@ -54,9 +56,10 @@ public class MemoryDao {
         }
         List<Put> memoryPuts = applicationStatHbaseOperationFactory.createPuts(id, joinMemoryBoList, statType, memorySerializer);
         if (!memoryPuts.isEmpty()) {
-            List<Put> rejectedPuts = hbaseTemplate2.asyncPut(APPLICATION_STAT_AGGRE, memoryPuts);
+            TableName applicationStatAggreTableName = tableNameProvider.getTableName(HBaseTables.APPLICATION_STAT_AGGRE_STR);
+            List<Put> rejectedPuts = hbaseTemplate2.asyncPut(applicationStatAggreTableName, memoryPuts);
             if (CollectionUtils.isNotEmpty(rejectedPuts)) {
-                hbaseTemplate2.put(APPLICATION_STAT_AGGRE, rejectedPuts);
+                hbaseTemplate2.put(applicationStatAggreTableName, rejectedPuts);
             }
         }
     }
diff --git a/flink/src/main/java/com/navercorp/pinpoint/flink/dao/hbase/ResponseTimeDao.java b/flink/src/main/java/com/navercorp/pinpoint/flink/dao/hbase/ResponseTimeDao.java
index 70afa9ded..8e582f131 100644
--- a/flink/src/main/java/com/navercorp/pinpoint/flink/dao/hbase/ResponseTimeDao.java
+++ b/flink/src/main/java/com/navercorp/pinpoint/flink/dao/hbase/ResponseTimeDao.java
@@ -17,6 +17,7 @@ package com.navercorp.pinpoint.flink.dao.hbase;
 
 import com.navercorp.pinpoint.common.hbase.HBaseTables;
 import com.navercorp.pinpoint.common.hbase.HbaseTemplate2;
+import com.navercorp.pinpoint.common.hbase.TableNameProvider;
 import com.navercorp.pinpoint.common.server.bo.serializer.stat.ApplicationStatHbaseOperationFactory;
 import com.navercorp.pinpoint.common.server.bo.serializer.stat.join.ResponseTimeSerializer;
 import com.navercorp.pinpoint.common.server.bo.stat.join.JoinStatBo;
@@ -40,12 +41,13 @@ public class ResponseTimeDao {
     private final HbaseTemplate2 hbaseTemplate2;
     private final ApplicationStatHbaseOperationFactory applicationStatHbaseOperationFactory;
     private final ResponseTimeSerializer responseTimeSerializer;
-    private final TableName APPLICATION_STAT_AGGRE = HBaseTables.APPLICATION_STAT_AGGRE;
+    private final TableNameProvider tableNameProvider;
 
-    public ResponseTimeDao(HbaseTemplate2 hbaseTemplate2, ApplicationStatHbaseOperationFactory applicationStatHbaseOperationFactory, ResponseTimeSerializer responseTimeSerializer) {
+    public ResponseTimeDao(HbaseTemplate2 hbaseTemplate2, ApplicationStatHbaseOperationFactory applicationStatHbaseOperationFactory, ResponseTimeSerializer responseTimeSerializer, TableNameProvider tableNameProvider) {
         this.hbaseTemplate2 = Objects.requireNonNull(hbaseTemplate2, "hbaseTemplate2 must not be null");
         this.applicationStatHbaseOperationFactory = Objects.requireNonNull(applicationStatHbaseOperationFactory, "applicationStatHbaseOperationFactory must not be null");
         this.responseTimeSerializer = Objects.requireNonNull(responseTimeSerializer, "responseTimeSerializer must not be null");
+        this.tableNameProvider = Objects.requireNonNull(tableNameProvider, "tableNameProvider must not be null");
     }
 
     public void insert(String id, long timestamp, List<JoinStatBo> joinResponseTimeBoList, StatType statType) {
@@ -54,9 +56,10 @@ public class ResponseTimeDao {
         }
         List<Put> responseTimePuts = applicationStatHbaseOperationFactory.createPuts(id, joinResponseTimeBoList, statType, responseTimeSerializer);
         if (!responseTimePuts.isEmpty()) {
-            List<Put> rejectedPuts = hbaseTemplate2.asyncPut(APPLICATION_STAT_AGGRE, responseTimePuts);
+            TableName applicationStatAggreTableName = tableNameProvider.getTableName(HBaseTables.APPLICATION_STAT_AGGRE_STR);
+            List<Put> rejectedPuts = hbaseTemplate2.asyncPut(applicationStatAggreTableName, responseTimePuts);
             if (CollectionUtils.isNotEmpty(rejectedPuts)) {
-                hbaseTemplate2.put(APPLICATION_STAT_AGGRE, rejectedPuts);
+                hbaseTemplate2.put(applicationStatAggreTableName, rejectedPuts);
             }
         }
     }
diff --git a/flink/src/main/java/com/navercorp/pinpoint/flink/dao/hbase/StatisticsDao.java b/flink/src/main/java/com/navercorp/pinpoint/flink/dao/hbase/StatisticsDao.java
index bcb69e77d..8ab4e06e2 100644
--- a/flink/src/main/java/com/navercorp/pinpoint/flink/dao/hbase/StatisticsDao.java
+++ b/flink/src/main/java/com/navercorp/pinpoint/flink/dao/hbase/StatisticsDao.java
@@ -15,14 +15,12 @@
  */
 package com.navercorp.pinpoint.flink.dao.hbase;
 
-import com.navercorp.pinpoint.common.hbase.HBaseTables;
 import com.navercorp.pinpoint.common.server.bo.stat.join.*;
 import com.navercorp.pinpoint.flink.Bootstrap;
 import org.apache.flink.api.common.io.OutputFormat;
 import org.apache.flink.api.java.tuple.Tuple3;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.util.CollectionUtil;
-import org.apache.hadoop.hbase.TableName;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -37,7 +35,6 @@ public class StatisticsDao implements OutputFormat<Tuple3<String, JoinStatBo, Lo
     private final Logger logger = LoggerFactory.getLogger(this.getClass());
 
     private static final long serialVersionUID = 1L;
-    private transient TableName APPLICATION_STAT_AGGRE;
     private transient CpuLoadDao cpuLoadDao;
     private transient MemoryDao memoryDao;
     private transient TransactionDao transactionDao;
@@ -51,7 +48,6 @@ public class StatisticsDao implements OutputFormat<Tuple3<String, JoinStatBo, Lo
 
     @Override
     public void configure(Configuration parameters) {
-        this.APPLICATION_STAT_AGGRE = HBaseTables.APPLICATION_STAT_AGGRE;
         Bootstrap bootstrap = Bootstrap.getInstance();
         cpuLoadDao = bootstrap.getCpuLoadDao();
         memoryDao = bootstrap.getMemoryDao();
diff --git a/flink/src/main/java/com/navercorp/pinpoint/flink/dao/hbase/TransactionDao.java b/flink/src/main/java/com/navercorp/pinpoint/flink/dao/hbase/TransactionDao.java
index 0e18e69bb..4af9aa03a 100644
--- a/flink/src/main/java/com/navercorp/pinpoint/flink/dao/hbase/TransactionDao.java
+++ b/flink/src/main/java/com/navercorp/pinpoint/flink/dao/hbase/TransactionDao.java
@@ -17,6 +17,7 @@ package com.navercorp.pinpoint.flink.dao.hbase;
 
 import com.navercorp.pinpoint.common.hbase.HBaseTables;
 import com.navercorp.pinpoint.common.hbase.HbaseTemplate2;
+import com.navercorp.pinpoint.common.hbase.TableNameProvider;
 import com.navercorp.pinpoint.common.server.bo.serializer.stat.ApplicationStatHbaseOperationFactory;
 import com.navercorp.pinpoint.common.server.bo.serializer.stat.join.TransactionSerializer;
 import com.navercorp.pinpoint.common.server.bo.stat.join.JoinStatBo;
@@ -37,16 +38,16 @@ import java.util.Objects;
 public class TransactionDao {
     private final Logger logger = LoggerFactory.getLogger(this.getClass());
 
-    private static final TableName APPLICATION_STAT_AGGRE = HBaseTables.APPLICATION_STAT_AGGRE;
-
     private final HbaseTemplate2 hbaseTemplate2;
     private final ApplicationStatHbaseOperationFactory applicationStatHbaseOperationFactory;
     private final TransactionSerializer transactionSerializer;
+    private final TableNameProvider tableNameProvider;
 
-    public TransactionDao(HbaseTemplate2 hbaseTemplate2, ApplicationStatHbaseOperationFactory applicationStatHbaseOperationFactory, TransactionSerializer transactionSerializer) {
+    public TransactionDao(HbaseTemplate2 hbaseTemplate2, ApplicationStatHbaseOperationFactory applicationStatHbaseOperationFactory, TransactionSerializer transactionSerializer, TableNameProvider tableNameProvider) {
         this.hbaseTemplate2 = Objects.requireNonNull(hbaseTemplate2, "hbaseTemplate2 must not be null");
         this.applicationStatHbaseOperationFactory = Objects.requireNonNull(applicationStatHbaseOperationFactory, "applicationStatHbaseOperationFactory must not be null");
         this.transactionSerializer = Objects.requireNonNull(transactionSerializer, "transactionSerializer must not be null");
+        this.tableNameProvider = Objects.requireNonNull(tableNameProvider, "tableNameProvider must not be null");
     }
 
     public void insert(String id, long timestamp, List<JoinStatBo> joinTransactionBoList, StatType statType) {
@@ -55,9 +56,10 @@ public class TransactionDao {
         }
         List<Put> transactionPuts = applicationStatHbaseOperationFactory.createPuts(id, joinTransactionBoList, statType, transactionSerializer);
         if (!transactionPuts.isEmpty()) {
-            List<Put> rejectedPuts = hbaseTemplate2.asyncPut(APPLICATION_STAT_AGGRE, transactionPuts);
+            TableName applicationStatAggreTableName = tableNameProvider.getTableName(HBaseTables.APPLICATION_STAT_AGGRE_STR);
+            List<Put> rejectedPuts = hbaseTemplate2.asyncPut(applicationStatAggreTableName, transactionPuts);
             if (CollectionUtils.isNotEmpty(rejectedPuts)) {
-                hbaseTemplate2.put(APPLICATION_STAT_AGGRE, rejectedPuts);
+                hbaseTemplate2.put(applicationStatAggreTableName, rejectedPuts);
             }
         }
     }
diff --git a/flink/src/main/java/com/navercorp/pinpoint/flink/process/ApplicationCache.java b/flink/src/main/java/com/navercorp/pinpoint/flink/process/ApplicationCache.java
index c24a5930a..ac1aaddbc 100644
--- a/flink/src/main/java/com/navercorp/pinpoint/flink/process/ApplicationCache.java
+++ b/flink/src/main/java/com/navercorp/pinpoint/flink/process/ApplicationCache.java
@@ -17,10 +17,12 @@ package com.navercorp.pinpoint.flink.process;
 
 import com.navercorp.pinpoint.common.hbase.HBaseTables;
 import com.navercorp.pinpoint.common.hbase.HbaseTemplate2;
+import com.navercorp.pinpoint.common.hbase.TableNameProvider;
 import com.navercorp.pinpoint.common.server.util.RowKeyUtils;
 import com.navercorp.pinpoint.common.util.TimeUtils;
 import com.navercorp.pinpoint.web.mapper.AgentInfoMapper;
 import com.navercorp.pinpoint.web.vo.AgentInfo;
+import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.client.Get;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.slf4j.Logger;
@@ -42,9 +44,11 @@ public class ApplicationCache {
 
     private final transient HbaseTemplate2 hbaseTemplate2;
 
+    private final transient TableNameProvider tableNameProvider;
 
-    public ApplicationCache(HbaseTemplate2 hbaseTemplate2) {
+    public ApplicationCache(HbaseTemplate2 hbaseTemplate2, TableNameProvider tableNameProvider) {
         this.hbaseTemplate2 = Objects.requireNonNull(hbaseTemplate2, "hbaseTemplate must not be null");
+        this.tableNameProvider = Objects.requireNonNull(tableNameProvider, "tableNameProvider must not be null");
     }
 
     @Cacheable(value="applicationId", key=SPEL_KEY)
@@ -57,7 +61,8 @@ public class ApplicationCache {
         get.addColumn(HBaseTables.AGENTINFO_CF_INFO, HBaseTables.AGENTINFO_CF_INFO_IDENTIFIER);
         AgentInfo agentInfo = null;
         try {
-            agentInfo = hbaseTemplate2.get(HBaseTables.AGENTINFO, get, agentInfoMapper);
+            TableName tableName = tableNameProvider.getTableName(HBaseTables.AGENTINFO_STR);
+            agentInfo = hbaseTemplate2.get(tableName, get, agentInfoMapper);
         } catch (Exception e) {
             logger.error("can't found application id({})", agentId, e);
         }
diff --git a/flink/src/main/java/com/navercorp/pinpoint/flink/receiver/FlinkPacketHandlerFactory.java b/flink/src/main/java/com/navercorp/pinpoint/flink/receiver/FlinkPacketHandlerFactory.java
new file mode 100644
index 000000000..63e11efe9
--- /dev/null
+++ b/flink/src/main/java/com/navercorp/pinpoint/flink/receiver/FlinkPacketHandlerFactory.java
@@ -0,0 +1,49 @@
+/*
+ * Copyright 2018 NAVER Corp.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package com.navercorp.pinpoint.flink.receiver;
+
+import com.navercorp.pinpoint.collector.receiver.DispatchHandler;
+import com.navercorp.pinpoint.collector.receiver.tcp.DefaultTCPPacketHandler;
+import com.navercorp.pinpoint.collector.receiver.tcp.TCPPacketHandler;
+import com.navercorp.pinpoint.collector.receiver.tcp.TCPPacketHandlerFactory;
+import com.navercorp.pinpoint.thrift.io.DeserializerFactory;
+import com.navercorp.pinpoint.thrift.io.FlinkHeaderTBaseDeserializerFactory;
+import com.navercorp.pinpoint.thrift.io.FlinkHeaderTBaseSerializerFactory;
+import com.navercorp.pinpoint.thrift.io.HeaderTBaseDeserializer;
+import com.navercorp.pinpoint.thrift.io.HeaderTBaseSerializer;
+import com.navercorp.pinpoint.thrift.io.SerializerFactory;
+import com.navercorp.pinpoint.thrift.io.ThreadLocalHeaderTBaseDeserializerFactory;
+import com.navercorp.pinpoint.thrift.io.ThreadLocalHeaderTBaseSerializerFactory;
+
+import java.util.Objects;
+
+/**
+ * @author Woonduk Kang(emeroad)
+ */
+public class FlinkPacketHandlerFactory implements TCPPacketHandlerFactory {
+    @Override
+    public TCPPacketHandler build(DispatchHandler dispatchHandler) {
+        Objects.requireNonNull(dispatchHandler, "dispatchHandler must not be null");
+
+        SerializerFactory<HeaderTBaseSerializer> serializerFactory = new FlinkHeaderTBaseSerializerFactory();
+        SerializerFactory<HeaderTBaseSerializer> cachedSerializer = new ThreadLocalHeaderTBaseSerializerFactory<>(serializerFactory);
+
+        DeserializerFactory<HeaderTBaseDeserializer> deserializerFactory = new FlinkHeaderTBaseDeserializerFactory();
+        DeserializerFactory<HeaderTBaseDeserializer> cachedDeserializer = new ThreadLocalHeaderTBaseDeserializerFactory<>(deserializerFactory);
+        return new DefaultTCPPacketHandler(dispatchHandler, cachedSerializer, cachedDeserializer);
+    }
+}
diff --git a/flink/src/main/resources/applicationContext-flink.xml b/flink/src/main/resources/applicationContext-flink.xml
index 1d83e38ed..1d6fcd21d 100644
--- a/flink/src/main/resources/applicationContext-flink.xml
+++ b/flink/src/main/resources/applicationContext-flink.xml
@@ -45,7 +45,8 @@
     <bean id="tbaseFlatMapper" class="com.navercorp.pinpoint.flink.process.TBaseFlatMapper"/>
 
     <bean id="applicationCache" class="com.navercorp.pinpoint.flink.process.ApplicationCache">
-        <constructor-arg ref="hbaseTemplate"/>
+        <constructor-arg index="0" ref="hbaseTemplate"/>
+        <constructor-arg index="1" ref="tableNameProvider"/>
     </bean>
 
 
@@ -86,36 +87,42 @@
         <constructor-arg index="0" ref="hbaseTemplate"/>
         <constructor-arg index="1" ref="applicationStatHbaseOperationFactory"/>
         <constructor-arg index="2" ref="cpuLoadSerializer"/>
+        <constructor-arg index="3" ref="tableNameProvider"/>
     </bean>
 
     <bean id="memoryDao" class="com.navercorp.pinpoint.flink.dao.hbase.MemoryDao">
         <constructor-arg index="0" ref="hbaseTemplate"/>
         <constructor-arg index="1" ref="applicationStatHbaseOperationFactory"/>
         <constructor-arg index="2" ref="memorySerializer"/>
+        <constructor-arg index="3" ref="tableNameProvider"/>
     </bean>
 
     <bean id="transactionDao" class="com.navercorp.pinpoint.flink.dao.hbase.TransactionDao">
         <constructor-arg index="0" ref="hbaseTemplate"/>
         <constructor-arg index="1" ref="applicationStatHbaseOperationFactory"/>
         <constructor-arg index="2" ref="transactionSerializer"/>
+        <constructor-arg index="3" ref="tableNameProvider"/>
     </bean>
 
     <bean id="activeTraceDao" class="com.navercorp.pinpoint.flink.dao.hbase.ActiveTraceDao">
         <constructor-arg index="0" ref="hbaseTemplate"/>
         <constructor-arg index="1" ref="applicationStatHbaseOperationFactory"/>
         <constructor-arg index="2" ref="activeTraceSerializer"/>
+        <constructor-arg index="3" ref="tableNameProvider"/>
     </bean>
 
     <bean id="responseTimeDao" class="com.navercorp.pinpoint.flink.dao.hbase.ResponseTimeDao">
         <constructor-arg index="0" ref="hbaseTemplate"/>
         <constructor-arg index="1" ref="applicationStatHbaseOperationFactory"/>
         <constructor-arg index="2" ref="responseTimeSerializer"/>
+        <constructor-arg index="3" ref="tableNameProvider"/>
     </bean>
 
     <bean id="dataSourceDao" class="com.navercorp.pinpoint.flink.dao.hbase.DataSourceDao">
         <constructor-arg index="0" ref="hbaseTemplate"/>
         <constructor-arg index="1" ref="applicationStatHbaseOperationFactory"/>
         <constructor-arg index="2" ref="dataSourceSerializer"/>
+        <constructor-arg index="3" ref="tableNameProvider"/>
     </bean>
 
     <bean id="handlerManager" class="com.navercorp.pinpoint.collector.manage.HandlerManager">
@@ -128,6 +135,8 @@
         <constructor-arg ref="handlerManager"/>
     </bean>
 
+    <bean id="flinkPacketHandlerFactory" class="com.navercorp.pinpoint.flink.receiver.FlinkPacketHandlerFactory"/>
+
     <bean id="flinkServerRegister" class="com.navercorp.pinpoint.flink.cluster.FlinkServerRegister" lazy-init="true">
         <constructor-arg ref="flinkConfiguration"/>
     </bean>
@@ -163,6 +172,7 @@
         <property name="addressFilter" ref="addressFilter"/>
         <property name="dispatchHandler" ref="tcpDispatchHandlerWrapper"/>
         <property name="executor" ref="flinkWorker"/>
+        <property name="tcpPacketHandlerFactory" ref="flinkPacketHandlerFactory"/>
     </bean>
 
 
diff --git a/flink/src/main/resources/applicationContext-hbase.xml b/flink/src/main/resources/applicationContext-hbase.xml
index db97b39cc..571a7ecdf 100644
--- a/flink/src/main/resources/applicationContext-hbase.xml
+++ b/flink/src/main/resources/applicationContext-hbase.xml
@@ -81,6 +81,10 @@
         <constructor-arg ref="hbaseConfiguration" index="0"></constructor-arg>
     </bean>
 
+    <bean id="tableNameProvider" class="com.navercorp.pinpoint.common.hbase.HbaseTableNameProvider">
+        <constructor-arg index="0" value="${hbase.namespace:}"/>
+    </bean>
+
     <!--<bean id="applicationTraceIndexDistributor" class="com.sematext.hbase.wd.RowKeyDistributorByHashPrefix">-->
         <!--<constructor-arg ref="applicationTraceIndex"/>-->
     <!--</bean>-->
diff --git a/flink/src/main/resources/hbase.properties b/flink/src/main/resources/hbase.properties
index 8dbd9ff50..cc39a0669 100644
--- a/flink/src/main/resources/hbase.properties
+++ b/flink/src/main/resources/hbase.properties
@@ -21,6 +21,9 @@ hbase.client.port=2181
 # hbase default:/hbase
 hbase.zookeeper.znode.parent=/hbase
 
+# hbase namespace to use default:default - you may leave this empty to use the default namespace
+hbase.namespace=
+
 # hbase timeout option==================================================================================
 # hbase default:true
 hbase.ipc.client.tcpnodelay=true
diff --git a/plugins/postgresql-jdbc/src/main/java/com/navercorp/pinpoint/plugin/jdbc/postgresql/PostgreSqlConfig.java b/plugins/postgresql-jdbc/src/main/java/com/navercorp/pinpoint/plugin/jdbc/postgresql/PostgreSqlConfig.java
index 833263f8a..534d9ac9b 100644
--- a/plugins/postgresql-jdbc/src/main/java/com/navercorp/pinpoint/plugin/jdbc/postgresql/PostgreSqlConfig.java
+++ b/plugins/postgresql-jdbc/src/main/java/com/navercorp/pinpoint/plugin/jdbc/postgresql/PostgreSqlConfig.java
@@ -15,28 +15,24 @@
 package com.navercorp.pinpoint.plugin.jdbc.postgresql;
 
 import com.navercorp.pinpoint.bootstrap.config.ProfilerConfig;
+import com.navercorp.pinpoint.bootstrap.plugin.jdbc.JdbcConfig;
 
 /**
  * @author Brad Hong
  *
  */
-public class PostgreSqlConfig {
-    private final boolean pluginEnable;
+public class PostgreSqlConfig extends JdbcConfig {
     private final boolean profileSetAutoCommit;
     private final boolean profileCommit;
     private final boolean profileRollback;
-    private final int maxSqlBindValueSize; 
 
     public PostgreSqlConfig(ProfilerConfig config) {
-        this.pluginEnable = config.readBoolean("profiler.jdbc.postgresql", false);
+        super(config.readBoolean("profiler.jdbc.postgresql", false),
+                config.readBoolean("profiler.jdbc.postgresql.tracesqlbindvalue", config.isTraceSqlBindValue()),
+                config.getMaxSqlBindValueSize());
         this.profileSetAutoCommit = config.readBoolean("profiler.jdbc.postgresql.setautocommit", false);
         this.profileCommit = config.readBoolean("profiler.jdbc.postgresql.commit", false);
         this.profileRollback = config.readBoolean("profiler.jdbc.postgresql.rollback", false);
-        this.maxSqlBindValueSize = config.readInt("profiler.jdbc.maxsqlbindvaluesize", 1024);
-    }
-
-    public boolean isPluginEnable() {
-        return pluginEnable;
     }
 
     public boolean isProfileSetAutoCommit() {
@@ -51,12 +47,8 @@ public class PostgreSqlConfig {
         return profileRollback;
     }
     
-    public int getMaxSqlBindValueSize() {
-        return maxSqlBindValueSize;
-    }
-    
     @Override
     public String toString() {
-        return "PostgreSqlConfig [pluginEnable="+ pluginEnable + ",profileSetAutoCommit=" + profileSetAutoCommit + ", profileCommit=" + profileCommit + ", profileRollback=" + profileRollback + ", maxSqlBindValueSize=" + maxSqlBindValueSize + "]";
+        return "PostgreSqlConfig [" + super.toString() + ", profileSetAutoCommit=" + profileSetAutoCommit + ", profileCommit=" + profileCommit + ", profileRollback=" + profileRollback + "]";
     }
 }
diff --git a/plugins/postgresql-jdbc/src/main/java/com/navercorp/pinpoint/plugin/jdbc/postgresql/PostgreSqlPlugin.java b/plugins/postgresql-jdbc/src/main/java/com/navercorp/pinpoint/plugin/jdbc/postgresql/PostgreSqlPlugin.java
index 6c7f732b2..79e35d54b 100644
--- a/plugins/postgresql-jdbc/src/main/java/com/navercorp/pinpoint/plugin/jdbc/postgresql/PostgreSqlPlugin.java
+++ b/plugins/postgresql-jdbc/src/main/java/com/navercorp/pinpoint/plugin/jdbc/postgresql/PostgreSqlPlugin.java
@@ -193,8 +193,10 @@ public class PostgreSqlPlugin implements ProfilerPlugin, TransformTemplateAware
                     InstrumentUtils.findMethod(target, "executeUpdate")
                             .addScopedInterceptor(preparedStatementInterceptor, va(maxBindValueSize), POSTGRESQL_SCOPE);
 
-                    for (InstrumentMethod method : declaredMethods) {
-                        method.addScopedInterceptor("com.navercorp.pinpoint.bootstrap.plugin.jdbc.interceptor.PreparedStatementBindVariableInterceptor", POSTGRESQL_SCOPE, ExecutionPolicy.BOUNDARY);
+                    if (config.isTraceSqlBindValue()) {
+                        for (InstrumentMethod method : declaredMethods) {
+                            method.addScopedInterceptor("com.navercorp.pinpoint.bootstrap.plugin.jdbc.interceptor.PreparedStatementBindVariableInterceptor", POSTGRESQL_SCOPE, ExecutionPolicy.BOUNDARY);
+                        }
                     }
                 }
 
@@ -230,8 +232,10 @@ public class PostgreSqlPlugin implements ProfilerPlugin, TransformTemplateAware
                     InstrumentUtils.findMethod(target, "executeUpdate")
                             .addScopedInterceptor(preparedStatementInterceptor, va(maxBindValueSize), POSTGRESQL_SCOPE);
 
-                    for (InstrumentMethod method : declaredMethods) {
-                        method.addScopedInterceptor("com.navercorp.pinpoint.bootstrap.plugin.jdbc.interceptor.PreparedStatementBindVariableInterceptor", POSTGRESQL_SCOPE, ExecutionPolicy.BOUNDARY);
+                    if (config.isTraceSqlBindValue()) {
+                        for (InstrumentMethod method : declaredMethods) {
+                            method.addScopedInterceptor("com.navercorp.pinpoint.bootstrap.plugin.jdbc.interceptor.PreparedStatementBindVariableInterceptor", POSTGRESQL_SCOPE, ExecutionPolicy.BOUNDARY);
+                        }
                     }
                 }
 
@@ -380,10 +384,12 @@ public class PostgreSqlPlugin implements ProfilerPlugin, TransformTemplateAware
                 InstrumentUtils.findMethod(target, "execute", "java.lang.String")
                         .addScopedInterceptor(executeUpdateInterceptor, POSTGRESQL_SCOPE);
 
-                final PreparedStatementBindingMethodFilter excludes = PreparedStatementBindingMethodFilter.excludes("setRowId", "setNClob", "setSQLXML");
-                final List<InstrumentMethod> declaredMethods = target.getDeclaredMethods(excludes);
-                for (InstrumentMethod method : declaredMethods) {
-                    method.addScopedInterceptor("com.navercorp.pinpoint.bootstrap.plugin.jdbc.interceptor.PreparedStatementBindVariableInterceptor", POSTGRESQL_SCOPE, ExecutionPolicy.BOUNDARY);
+                if (config.isTraceSqlBindValue()) {
+                    final PreparedStatementBindingMethodFilter excludes = PreparedStatementBindingMethodFilter.excludes("setRowId", "setNClob", "setSQLXML");
+                    final List<InstrumentMethod> declaredMethods = target.getDeclaredMethods(excludes);
+                    for (InstrumentMethod method : declaredMethods) {
+                        method.addScopedInterceptor("com.navercorp.pinpoint.bootstrap.plugin.jdbc.interceptor.PreparedStatementBindVariableInterceptor", POSTGRESQL_SCOPE, ExecutionPolicy.BOUNDARY);
+                    }
                 }
 
                 return target.toBytecode();
diff --git a/profiler/src/main/java/com/navercorp/pinpoint/profiler/DefaultAgent.java b/profiler/src/main/java/com/navercorp/pinpoint/profiler/DefaultAgent.java
index 9187fa6f0..b5b60b5d8 100644
--- a/profiler/src/main/java/com/navercorp/pinpoint/profiler/DefaultAgent.java
+++ b/profiler/src/main/java/com/navercorp/pinpoint/profiler/DefaultAgent.java
@@ -25,8 +25,10 @@ import com.navercorp.pinpoint.bootstrap.logging.PLogger;
 import com.navercorp.pinpoint.bootstrap.logging.PLoggerBinder;
 import com.navercorp.pinpoint.bootstrap.logging.PLoggerFactory;
 import com.navercorp.pinpoint.common.service.ServiceTypeRegistryService;
+import com.navercorp.pinpoint.common.util.Assert;
 import com.navercorp.pinpoint.profiler.context.module.ApplicationContext;
 import com.navercorp.pinpoint.profiler.context.module.DefaultApplicationContext;
+import com.navercorp.pinpoint.profiler.context.module.DefaultModuleFactoryProvider;
 import com.navercorp.pinpoint.profiler.util.SystemPropertyDumper;
 import com.navercorp.pinpoint.profiler.interceptor.registry.DefaultInterceptorRegistryBinder;
 import com.navercorp.pinpoint.profiler.interceptor.registry.InterceptorRegistryBinder;
@@ -122,9 +124,12 @@ public class DefaultAgent implements Agent {
     }
 
     protected ApplicationContext newApplicationContext(AgentOption agentOption, InterceptorRegistryBinder interceptorRegistryBinder) {
-        return new DefaultApplicationContext(agentOption, interceptorRegistryBinder);
-    }
+        Assert.requireNonNull(agentOption, "agentOption must not be null");
+        ProfilerConfig profilerConfig = Assert.requireNonNull(agentOption.getProfilerConfig(), "profilerConfig must not be null");
 
+        DefaultModuleFactoryProvider moduleFactoryProvider = new DefaultModuleFactoryProvider(profilerConfig.getInjectionModuleFactoryClazzName());
+        return new DefaultApplicationContext(agentOption, interceptorRegistryBinder, moduleFactoryProvider);
+    }
 
     protected ApplicationContext getApplicationContext() {
         return applicationContext;
diff --git a/profiler/src/main/java/com/navercorp/pinpoint/profiler/context/module/DefaultApplicationContext.java b/profiler/src/main/java/com/navercorp/pinpoint/profiler/context/module/DefaultApplicationContext.java
index 5fc22c309..fdf86f058 100644
--- a/profiler/src/main/java/com/navercorp/pinpoint/profiler/context/module/DefaultApplicationContext.java
+++ b/profiler/src/main/java/com/navercorp/pinpoint/profiler/context/module/DefaultApplicationContext.java
@@ -69,7 +69,6 @@ public class DefaultApplicationContext implements ApplicationContext {
     private final DataSender statDataSender;
 
     private final AgentInformation agentInformation;
-    private final AgentOption agentOption;
     private final ServerMetaDataRegistryService serverMetaDataRegistryService;
 
     private final ServiceTypeRegistryService serviceTypeRegistryService;
@@ -82,12 +81,12 @@ public class DefaultApplicationContext implements ApplicationContext {
 
     private final Injector injector;
 
-    public DefaultApplicationContext(AgentOption agentOption, final InterceptorRegistryBinder interceptorRegistryBinder) {
-        this(agentOption, interceptorRegistryBinder, new ApplicationContextModuleFactory());
+    public DefaultApplicationContext(AgentOption agentOption, final InterceptorRegistryBinder interceptorRegistryBinder, ModuleFactoryProvider moduleFactoryProvider) {
+        this(agentOption, interceptorRegistryBinder, moduleFactoryProvider.get());
     }
 
     public DefaultApplicationContext(AgentOption agentOption, final InterceptorRegistryBinder interceptorRegistryBinder, ModuleFactory moduleFactory) {
-        this.agentOption = Assert.requireNonNull(agentOption, "agentOption must not be null");
+        Assert.requireNonNull(agentOption, "agentOption must not be null");
         this.profilerConfig = Assert.requireNonNull(agentOption.getProfilerConfig(), "profilerConfig must not be null");
         Assert.requireNonNull(moduleFactory, "moduleFactory must not be null");
 
diff --git a/profiler/src/main/java/com/navercorp/pinpoint/profiler/context/module/DefaultModuleFactoryProvider.java b/profiler/src/main/java/com/navercorp/pinpoint/profiler/context/module/DefaultModuleFactoryProvider.java
new file mode 100644
index 000000000..53267cdfa
--- /dev/null
+++ b/profiler/src/main/java/com/navercorp/pinpoint/profiler/context/module/DefaultModuleFactoryProvider.java
@@ -0,0 +1,68 @@
+/*
+ * Copyright 2018 NAVER Corp.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package com.navercorp.pinpoint.profiler.context.module;
+
+import com.navercorp.pinpoint.bootstrap.config.ProfilerConfig;
+import com.navercorp.pinpoint.common.util.Assert;
+import com.navercorp.pinpoint.common.util.StringUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.lang.reflect.Constructor;
+
+/**
+ * @author Taejin Koo
+ */
+public class DefaultModuleFactoryProvider implements ModuleFactoryProvider {
+
+    private static final Logger LOGGER = LoggerFactory.getLogger(DefaultModuleFactoryProvider.class);
+
+    private final String moduleFactoryClazzName;
+
+    public DefaultModuleFactoryProvider(String moduleFactoryClazzName) {
+        this.moduleFactoryClazzName = moduleFactoryClazzName;
+    }
+
+    public DefaultModuleFactoryProvider(ProfilerConfig profilerConfig) {
+        Assert.requireNonNull(profilerConfig, "profilerConfig must not be null");
+        this.moduleFactoryClazzName = profilerConfig.getInjectionModuleFactoryClazzName();
+    }
+
+    @Override
+    public ModuleFactory get() {
+        if (StringUtils.isEmpty(moduleFactoryClazzName) || ApplicationContextModuleFactory.class.getName().equals(moduleFactoryClazzName)) {
+            return new ApplicationContextModuleFactory();
+        } else {
+            ClassLoader classLoader = getClassLoader(DefaultModuleFactoryProvider.class.getClassLoader());
+            try {
+                final Class<? extends ModuleFactory> moduleFactoryClass =
+                        (Class<? extends ModuleFactory>) Class.forName(moduleFactoryClazzName, true, classLoader);
+                Constructor<? extends ModuleFactory> constructor = moduleFactoryClass.getConstructor();
+                return constructor.newInstance();
+            } catch (Exception e) {
+                LOGGER.warn("{} clazz not found", moduleFactoryClazzName);
+            }
+        }
+        return null;
+    }
+
+    private ClassLoader getClassLoader(ClassLoader classLoader) {
+        Assert.requireNonNull(classLoader, "can't find classLoader");
+        return classLoader;
+    }
+
+}
diff --git a/profiler/src/main/java/com/navercorp/pinpoint/profiler/context/module/ModuleFactoryProvider.java b/profiler/src/main/java/com/navercorp/pinpoint/profiler/context/module/ModuleFactoryProvider.java
new file mode 100644
index 000000000..f3f42c0e5
--- /dev/null
+++ b/profiler/src/main/java/com/navercorp/pinpoint/profiler/context/module/ModuleFactoryProvider.java
@@ -0,0 +1,26 @@
+/*
+ * Copyright 2018 NAVER Corp.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package com.navercorp.pinpoint.profiler.context.module;
+
+/**
+ * @author Taejin Koo
+ */
+public interface ModuleFactoryProvider {
+
+    ModuleFactory get();
+
+}
diff --git a/profiler/src/test/java/com/navercorp/pinpoint/profiler/context/graph/DependencyGraph.java b/profiler/src/test/java/com/navercorp/pinpoint/profiler/context/graph/DependencyGraph.java
index 8a371db0b..49087bfad 100644
--- a/profiler/src/test/java/com/navercorp/pinpoint/profiler/context/graph/DependencyGraph.java
+++ b/profiler/src/test/java/com/navercorp/pinpoint/profiler/context/graph/DependencyGraph.java
@@ -28,6 +28,7 @@ import com.navercorp.pinpoint.common.Charsets;
 import com.navercorp.pinpoint.common.service.DefaultAnnotationKeyRegistryService;
 import com.navercorp.pinpoint.common.service.DefaultServiceTypeRegistryService;
 import com.navercorp.pinpoint.profiler.context.module.DefaultApplicationContext;
+import com.navercorp.pinpoint.profiler.context.module.DefaultModuleFactoryProvider;
 import com.navercorp.pinpoint.profiler.interceptor.registry.InterceptorRegistryBinder;
 import com.navercorp.pinpoint.profiler.util.TestInterceptorRegistryBinder;
 import org.slf4j.Logger;
@@ -81,7 +82,7 @@ public class DependencyGraph {
                 "mockAgent", "mockApplicationName", profilerConfig, new URL[0],
                 null, new DefaultServiceTypeRegistryService(), new DefaultAnnotationKeyRegistryService());
 
-        return new DefaultApplicationContext(agentOption, binder);
+        return new DefaultApplicationContext(agentOption, binder, new DefaultModuleFactoryProvider(""));
     }
 
     private String currentWorkingDir() {
diff --git a/profiler/src/test/java/com/navercorp/pinpoint/profiler/context/module/DefaultApplicationContextTest.java b/profiler/src/test/java/com/navercorp/pinpoint/profiler/context/module/DefaultApplicationContextTest.java
index e7f13987d..870de2258 100644
--- a/profiler/src/test/java/com/navercorp/pinpoint/profiler/context/module/DefaultApplicationContextTest.java
+++ b/profiler/src/test/java/com/navercorp/pinpoint/profiler/context/module/DefaultApplicationContextTest.java
@@ -89,7 +89,7 @@ public class DefaultApplicationContextTest {
         AgentOption agentOption = new DefaultAgentOption(instrumentation, "mockAgent", "mockApplicationName", profilerConfig, new URL[0],
                 null, new DefaultServiceTypeRegistryService(), new DefaultAnnotationKeyRegistryService());
 
-        return new DefaultApplicationContext(agentOption, binder);
+        return new DefaultApplicationContext(agentOption, binder, new DefaultModuleFactoryProvider(""));
     }
 
 }
\ No newline at end of file
diff --git a/profiler/src/test/java/com/navercorp/pinpoint/profiler/context/module/ModuleFactoryProviderTest.java b/profiler/src/test/java/com/navercorp/pinpoint/profiler/context/module/ModuleFactoryProviderTest.java
new file mode 100644
index 000000000..5dfcd5473
--- /dev/null
+++ b/profiler/src/test/java/com/navercorp/pinpoint/profiler/context/module/ModuleFactoryProviderTest.java
@@ -0,0 +1,63 @@
+/*
+ * Copyright 2018 NAVER Corp.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package com.navercorp.pinpoint.profiler.context.module;
+
+import com.google.inject.Module;
+import com.navercorp.pinpoint.bootstrap.AgentOption;
+import com.navercorp.pinpoint.profiler.interceptor.registry.InterceptorRegistryBinder;
+import org.junit.Assert;
+import org.junit.Test;
+
+/**
+ * @author Taejin Koo
+ */
+public class ModuleFactoryProviderTest {
+
+    @Test
+    public void test() {
+        ModuleFactoryProvider provider = new DefaultModuleFactoryProvider(TestModuleFactory.class.getName());
+        ModuleFactory moduleFactory = provider.get();
+
+        Assert.assertEquals(TestModuleFactory.class, moduleFactory.getClass());
+    }
+
+    @Test
+    public void test2() {
+        ModuleFactoryProvider provider = new DefaultModuleFactoryProvider("");
+        ModuleFactory moduleFactory = provider.get();
+
+        Assert.assertEquals(ApplicationContextModuleFactory.class, moduleFactory.getClass());
+    }
+
+    @Test
+    public void test3() {
+        ModuleFactoryProvider provider = new DefaultModuleFactoryProvider("abcde");
+        ModuleFactory moduleFactory = provider.get();
+
+        Assert.assertNull(moduleFactory);
+    }
+
+    public static class TestModuleFactory implements ModuleFactory {
+
+        @Override
+        public Module newModule(AgentOption agentOption, InterceptorRegistryBinder interceptorRegistryBinder) {
+            return null;
+        }
+
+    }
+
+}
diff --git a/profiler/src/test/java/com/navercorp/pinpoint/profiler/monitor/CollectJobTest.java b/profiler/src/test/java/com/navercorp/pinpoint/profiler/monitor/CollectJobTest.java
index bc14c804c..c3db8862b 100644
--- a/profiler/src/test/java/com/navercorp/pinpoint/profiler/monitor/CollectJobTest.java
+++ b/profiler/src/test/java/com/navercorp/pinpoint/profiler/monitor/CollectJobTest.java
@@ -23,6 +23,7 @@ import com.navercorp.pinpoint.bootstrap.config.ProfilerConfig;
 import com.navercorp.pinpoint.common.service.DefaultAnnotationKeyRegistryService;
 import com.navercorp.pinpoint.common.service.DefaultServiceTypeRegistryService;
 import com.navercorp.pinpoint.profiler.context.module.DefaultApplicationContext;
+import com.navercorp.pinpoint.profiler.context.module.DefaultModuleFactoryProvider;
 import com.navercorp.pinpoint.profiler.interceptor.registry.InterceptorRegistryBinder;
 import com.navercorp.pinpoint.profiler.monitor.collector.AgentStatMetricCollector;
 import com.navercorp.pinpoint.profiler.sender.DataSender;
@@ -69,7 +70,7 @@ public class CollectJobTest {
         AgentOption agentOption = new DefaultAgentOption(instrumentation, "mockAgent", "mockApplicationName", profilerConfig, new URL[0],
                 null, new DefaultServiceTypeRegistryService(), new DefaultAnnotationKeyRegistryService());
 
-        return new DefaultApplicationContext(agentOption, binder);
+        return new DefaultApplicationContext(agentOption, binder, new DefaultModuleFactoryProvider(""));
     }
 
 
diff --git a/quickstart/bin/start-collector.sh b/quickstart/bin/start-collector.sh
index 5cbea0a9c..de6e13875 100755
--- a/quickstart/bin/start-collector.sh
+++ b/quickstart/bin/start-collector.sh
@@ -1,11 +1,5 @@
 #!/usr/bin/env bash
 
-UNAME=`uname`
-OS_TYPE="linux";
-if [[ "$UNAME" == "Darwin" ]]; then
-        OS_TYPE="mac"
-fi
-
 this="${BASH_SOURCE-$0}"
 while [ -h "$this" ]; do
   ls=`ls -ld "$this"`
@@ -118,56 +112,49 @@ function func_init_log
 
 function func_check_running_pinpoint_collector
 {
-	port=$( func_read_properties "$KEY_PORT" )
-
-        if [[ "$OS_TYPE" == 'mac' ]]; then
-		main_port_num=`lsof -p $pid | grep TCP | grep $port | wc -l `
-                process_tcp_port_num=`lsof -p $pid | grep TCP | wc -l `
-                process_udp_port_num=`lsof -p $pid |  grep UDP | wc -l `
-        else
-		main_port_num=`netstat -anp 2>/dev/null | grep $pid/java | grep tcp | grep $port | wc -l `
-                process_tcp_port_num=`netstat -anp 2>/dev/null | grep $pid/java | grep tcp | wc -l `
-                process_udp_port_num=`netstat -anp 2>/dev/null | grep $pid/java | grep udp | wc -l `
-        fi
-	
-        if [[ $main_port_num -eq 1 && $process_tcp_port_num -ge 2 && $process_udp_port_num -ge 2 ]]; then
-                echo "true"
-        else
-                echo "false"
-        fi
+    port=$( func_read_properties "$KEY_PORT" )
+    check_url="http://localhost:"$port"/serverTime.pinpoint"
+
+    process_status=`curl $check_url 2>/dev/null | grep 'currentServerTime'`
+
+    if [ -z $process_status ]; then
+        echo "false"
+    else
+        echo "true"
+    fi
 }
 
 function func_start_pinpoint_collector
 {
-		version=$( func_read_properties "$KEY_VERSION" )
-        pid=`nohup ${bin}/../../mvnw -f $COLLECTOR_DIR/pom.xml clean package tomcat7:run -D$IDENTIFIER -Dmaven.pinpoint.version=$version > $LOGS_DIR/$LOG_FILE 2>&1 & echo $!`
-        echo $pid > $PID_DIR/$PID_FILE
-
-        echo "---$COLLECTOR_IDENTIFIER initialization started. pid=$pid.---"
-
-        end_count=0
-	check_running_pinpoint_collector=$( func_check_running_pinpoint_collector )
-        while [ "$check_running_pinpoint_collector" == "false" ]; do
-                wait_time=`expr $end_count \* $UNIT_TIME`
-                echo "starting $COLLECTOR_IDENTIFIER. $wait_time /$CLOSE_WAIT_TIME sec(close wait limit)."
-				
-                if [ $end_count -ge $CHECK_COUNT ]; then
-                        break
-                fi
+    version=$( func_read_properties "$KEY_VERSION" )
+    pid=`nohup ${bin}/../../mvnw -f $COLLECTOR_DIR/pom.xml clean package tomcat7:run -D$IDENTIFIER -Dmaven.pinpoint.version=$version > $LOGS_DIR/$LOG_FILE 2>&1 & echo $!`
+    echo $pid > $PID_DIR/$PID_FILE
 
-                sleep $UNIT_TIME
-                end_count=`expr $end_count + 1`
-				
-               check_running_pinpoint_collector=$( func_check_running_pinpoint_collector )
-        done
+    echo "---$COLLECTOR_IDENTIFIER initialization started. pid=$pid.---"g
+
+    end_count=0
+    check_running_pinpoint_collector=$( func_check_running_pinpoint_collector )
+    while [ "$check_running_pinpoint_collector" == "false" ]; do
+        wait_time=`expr $end_count \* $UNIT_TIME`
+        echo "starting $COLLECTOR_IDENTIFIER. $wait_time /$CLOSE_WAIT_TIME sec(close wait limit)."
 
-        if [[ "$check_running_pinpoint_collector" == "true" ]]; then
-                echo "---$COLLECTOR_IDENTIFIER initialization completed. pid=$pid.---"
-                tail -f  $LOGS_DIR/$LOG_FILE
-        else
-                echo "---$COLLECTOR_IDENTIFIER initialization failed. pid=$pid.---"
-                kill -9 $pid
+        if [ $end_count -ge $CHECK_COUNT ]; then
+            break
         fi
+
+        sleep $UNIT_TIME
+        end_count=`expr $end_count + 1`
+
+        check_running_pinpoint_collector=$( func_check_running_pinpoint_collector )
+    done
+
+    if [[ "$check_running_pinpoint_collector" == "true" ]]; then
+        echo "---$COLLECTOR_IDENTIFIER initialization completed. pid=$pid.---"
+        tail -f  $LOGS_DIR/$LOG_FILE
+    else
+        echo "---$COLLECTOR_IDENTIFIER initialization failed. pid=$pid.---"
+        kill -9 $pid
+    fi
 }
 
 func_check_process
diff --git a/quickstart/bin/start-testapp.sh b/quickstart/bin/start-testapp.sh
index 1ad198c3e..f02ec4b9c 100755
--- a/quickstart/bin/start-testapp.sh
+++ b/quickstart/bin/start-testapp.sh
@@ -141,9 +141,9 @@ function func_init_agent
 
 function func_start_pinpoint_testapp
 {
-		version=$( func_read_properties "$KEY_VERSION" )
+        version=$( func_read_properties "$KEY_VERSION" )
         maven_opt=$MAVEN_OPTS
-		pinpoint_agent=$AGENT_BOOTSTRAP_DIR/pinpoint-bootstrap-$version.jar
+        pinpoint_agent=$AGENT_BOOTSTRAP_DIR/pinpoint-bootstrap-$version.jar
         pinpoint_opt="-javaagent:$pinpoint_agent -Dpinpoint.agentId=test-agent -Dpinpoint.applicationName=TESTAPP"
         export MAVEN_OPTS=$pinpoint_opt
 
diff --git a/quickstart/bin/start-web.sh b/quickstart/bin/start-web.sh
index 6cacd05cf..167f18f8a 100755
--- a/quickstart/bin/start-web.sh
+++ b/quickstart/bin/start-web.sh
@@ -111,37 +111,37 @@ function func_init_log
 
 function func_start_pinpoint_web
 {
-	version=$( func_read_properties "$KEY_VERSION" )
-	port=$( func_read_properties "$KEY_PORT" ) 
-        pid=`nohup ${bin}/../../mvnw -f $WEB_DIR/pom.xml clean package tomcat7:run -D$IDENTIFIER -Dmaven.pinpoint.version=$version > $LOGS_DIR/$LOG_FILE 2>&1 & echo $!`
-	check_url="http://localhost:"$port"/serverTime.pinpoint"
-        echo $pid > $PID_DIR/$PID_FILE
-
-        echo "---$WEB_IDENTIFIER initialization started. pid=$pid.---"
-
-        process_status=`curl $check_url 2>/dev/null | grep 'currentServerTime'`
-        end_count=0
-
-        while [ -z $process_status ]; do
-                wait_time=`expr $end_count \* $UNIT_TIME`
-                echo "starting $WEB_IDENTIFIER. $wait_time /$CLOSE_WAIT_TIME sec(close wait limit)."
-
-                if [ $end_count -ge $CHECK_COUNT ]; then
-                        break
-                fi
-
-                sleep $UNIT_TIME
-                end_count=`expr $end_count + 1`
-                process_status=`curl $check_url 2>/dev/null | grep 'currentServerTime'`
-        done
-
-        if [ -z $process_status ]; then
-                echo "---$WEB_IDENTIFIER initialization failed. pid=$pid.---"
-                kill -9 $pid
-        else
-                echo "---$WEB_IDENTIFIER initialization completed. pid=$pid.---"
-                tail -f  $LOGS_DIR/$LOG_FILE
-        fi
+    version=$( func_read_properties "$KEY_VERSION" )
+    port=$( func_read_properties "$KEY_PORT" )
+    pid=`nohup ${bin}/../../mvnw -f $WEB_DIR/pom.xml clean package tomcat7:run -D$IDENTIFIER -Dmaven.pinpoint.version=$version > $LOGS_DIR/$LOG_FILE 2>&1 & echo $!`
+    check_url="http://localhost:"$port"/serverTime.pinpoint"
+    echo $pid > $PID_DIR/$PID_FILE
+
+    echo "---$WEB_IDENTIFIER initialization started. pid=$pid.---"
+
+    process_status=`curl $check_url 2>/dev/null | grep 'currentServerTime'`
+    end_count=0
+
+    while [ -z $process_status ]; do
+        wait_time=`expr $end_count \* $UNIT_TIME`
+        echo "starting $WEB_IDENTIFIER. $wait_time /$CLOSE_WAIT_TIME sec(close wait limit)."
+
+        if [ $end_count -ge $CHECK_COUNT ]; then
+            break
+            fi
+
+            sleep $UNIT_TIME
+            end_count=`expr $end_count + 1`
+            process_status=`curl $check_url 2>/dev/null | grep 'currentServerTime'`
+    done
+
+    if [ -z $process_status ]; then
+        echo "---$WEB_IDENTIFIER initialization failed. pid=$pid.---"
+        kill -9 $pid
+    else
+        echo "---$WEB_IDENTIFIER initialization completed. pid=$pid.---"
+        tail -f  $LOGS_DIR/$LOG_FILE
+    fi
 }
 
 func_check_process
diff --git a/quickstart/web/src/main/resources/pinpoint-web.properties b/quickstart/web/src/main/resources/pinpoint-web.properties
index dd4e39aa2..400f15753 100644
--- a/quickstart/web/src/main/resources/pinpoint-web.properties
+++ b/quickstart/web/src/main/resources/pinpoint-web.properties
@@ -35,21 +35,14 @@ web.hbase.selectAllSpans.limit=500
 
 web.activethread.activeAgent.duration.days=7
 
-# server map link selector mode = v1 or v2 (default = v1)
-web.servermap.selector.mode=v2
-
-# server map link creator mode = serial or parallel (default = serial)
-# only applicable when web.servermap.selector.mode=v2
-web.servermap.creator.mode=serial
-web.servermap.creator.parallel.maxthreads=16
-
-# server map builder mode = v1 or v2 (default = v1)
-web.servermap.builder.mode=v2
-
-# server map appender mode = serial or parallel (default = serial)
-# only applicable when web.servermap.builder.mode=v2
-web.servermap.appender.mode=serial
-web.servermap.appender.parallel.maxthreads=16
+# number of server map link select worker threads
+web.servermap.creator.worker.threadSize=16
+# capacity of server map link select worker queue
+web.servermap.creator.worker.queueSize=512
+# number of server node appender worker threads
+web.servermap.appender.worker.threadSize=16
+# capacity of server node appender worker queue
+web.servermap.appender.worker.queueSize=512
 
 # see RFC 6454: The Web Origin Concept(https://tools.ietf.org/html/rfc6454) for more details
 # 1. Allow only same origin requests (value : websocket.allowedOrigins=)
diff --git a/web/src/main/java/com/navercorp/pinpoint/web/applicationmap/appender/histogram/ParallelNodeHistogramAppender.java b/web/src/main/java/com/navercorp/pinpoint/web/applicationmap/appender/histogram/DefaultNodeHistogramAppender.java
similarity index 79%
rename from web/src/main/java/com/navercorp/pinpoint/web/applicationmap/appender/histogram/ParallelNodeHistogramAppender.java
rename to web/src/main/java/com/navercorp/pinpoint/web/applicationmap/appender/histogram/DefaultNodeHistogramAppender.java
index b86941a11..476eee528 100644
--- a/web/src/main/java/com/navercorp/pinpoint/web/applicationmap/appender/histogram/ParallelNodeHistogramAppender.java
+++ b/web/src/main/java/com/navercorp/pinpoint/web/applicationmap/appender/histogram/DefaultNodeHistogramAppender.java
@@ -1,5 +1,5 @@
 /*
- * Copyright 2017 NAVER Corp.
+ * Copyright 2018 NAVER Corp.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
  * you may not use this file except in compliance with the License.
@@ -17,43 +17,34 @@
 package com.navercorp.pinpoint.web.applicationmap.appender.histogram;
 
 import com.navercorp.pinpoint.common.trace.ServiceType;
+import com.navercorp.pinpoint.web.applicationmap.histogram.NodeHistogram;
 import com.navercorp.pinpoint.web.applicationmap.link.LinkList;
 import com.navercorp.pinpoint.web.applicationmap.nodes.Node;
 import com.navercorp.pinpoint.web.applicationmap.nodes.NodeList;
-import com.navercorp.pinpoint.web.applicationmap.histogram.NodeHistogram;
 import com.navercorp.pinpoint.web.vo.Application;
 import com.navercorp.pinpoint.web.vo.Range;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
 import org.springframework.util.CollectionUtils;
 
 import java.util.ArrayList;
 import java.util.Collection;
 import java.util.List;
+import java.util.Objects;
 import java.util.concurrent.CompletableFuture;
-import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executor;
 import java.util.function.Supplier;
 
 /**
  * @author HyunGil Jeong
  */
-public class ParallelNodeHistogramAppender implements NodeHistogramAppender {
-
-    private final Logger logger = LoggerFactory.getLogger(this.getClass());
+public class DefaultNodeHistogramAppender implements NodeHistogramAppender {
 
     private final NodeHistogramFactory nodeHistogramFactory;
 
-    private final ExecutorService executorService;
+    private final Executor executor;
 
-    public ParallelNodeHistogramAppender(NodeHistogramFactory nodeHistogramFactory, ExecutorService executorService) {
-        if (nodeHistogramFactory == null) {
-            throw new NullPointerException("nodeHistogramFactory must not be null");
-        }
-        if (executorService == null) {
-            throw new NullPointerException("executorService must not be null");
-        }
-        this.nodeHistogramFactory = nodeHistogramFactory;
-        this.executorService = executorService;
+    public DefaultNodeHistogramAppender(NodeHistogramFactory nodeHistogramFactory, Executor executor) {
+        this.nodeHistogramFactory = Objects.requireNonNull(nodeHistogramFactory, "nodeHistogramFactory must not be null");
+        this.executor = Objects.requireNonNull(executor, "executor must not be null");
     }
 
     @Override
@@ -66,11 +57,7 @@ public class ParallelNodeHistogramAppender implements NodeHistogramAppender {
             return;
         }
         CompletableFuture[] futures = getNodeHistogramFutures(range, nodes, linkList);
-        try {
-            CompletableFuture.allOf(futures).join();
-        } catch (Exception e) {
-            logger.error("Error appending node histograms", e);
-        }
+        CompletableFuture.allOf(futures).join();
     }
 
     private CompletableFuture[] getNodeHistogramFutures(Range range, Collection<Node> nodes, LinkList linkList) {
@@ -86,6 +73,7 @@ public class ParallelNodeHistogramAppender implements NodeHistogramAppender {
         CompletableFuture<NodeHistogram> nodeHistogramFuture;
         final Application application = node.getApplication();
         final ServiceType serviceType = application.getServiceType();
+
         if (serviceType.isWas()) {
             // for WAS nodes, set their own response time histogram
             final Application wasNode = node.getApplication();
@@ -94,7 +82,7 @@ public class ParallelNodeHistogramAppender implements NodeHistogramAppender {
                 public NodeHistogram get() {
                     return nodeHistogramFactory.createWasNodeHistogram(wasNode, range);
                 }
-            }, executorService);
+            }, executor);
         } else if (serviceType.isTerminal() || serviceType.isUnknown()) {
             nodeHistogramFuture = CompletableFuture.completedFuture(nodeHistogramFactory.createTerminalNodeHistogram(application, range, linkList));
         } else if (serviceType.isQueue()) {
diff --git a/web/src/main/java/com/navercorp/pinpoint/web/applicationmap/appender/histogram/NodeHistogramAppenderFactory.java b/web/src/main/java/com/navercorp/pinpoint/web/applicationmap/appender/histogram/NodeHistogramAppenderFactory.java
index 32ed79ad9..f619d66a3 100644
--- a/web/src/main/java/com/navercorp/pinpoint/web/applicationmap/appender/histogram/NodeHistogramAppenderFactory.java
+++ b/web/src/main/java/com/navercorp/pinpoint/web/applicationmap/appender/histogram/NodeHistogramAppenderFactory.java
@@ -16,17 +16,12 @@
 
 package com.navercorp.pinpoint.web.applicationmap.appender.histogram;
 
-import com.navercorp.pinpoint.common.util.PinpointThreadFactory;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
 import org.springframework.beans.factory.annotation.Autowired;
-import org.springframework.beans.factory.annotation.Value;
+import org.springframework.beans.factory.annotation.Qualifier;
 import org.springframework.stereotype.Component;
 
-import javax.annotation.PreDestroy;
-import java.util.concurrent.ExecutorService;
-import java.util.concurrent.Executors;
-import java.util.concurrent.TimeUnit;
+import java.util.Objects;
+import java.util.concurrent.Executor;
 
 /**
  * @author HyunGil Jeong
@@ -34,40 +29,14 @@ import java.util.concurrent.TimeUnit;
 @Component
 public class NodeHistogramAppenderFactory {
 
-    private final Logger logger = LoggerFactory.getLogger(this.getClass());
-
-    private final String mode;
-    private final ExecutorService executorService;
+    private final Executor executor;
 
     @Autowired
-    public NodeHistogramAppenderFactory(
-            @Value("#{pinpointWebProps['web.servermap.appender.mode'] ?: 'serial'}") String mode,
-            @Value("#{pinpointWebProps['web.servermap.appender.parallel.maxthreads'] ?: 16}") int maxThreads) {
-        logger.info("NodeHistogramAppender mode : {}", mode);
-        this.mode = mode;
-        if (this.mode.equalsIgnoreCase("parallel")) {
-            executorService = Executors.newFixedThreadPool(maxThreads, new PinpointThreadFactory("Pinpoint-node-histogram-appender", true));
-        } else {
-            executorService = null;
-        }
+    public NodeHistogramAppenderFactory(@Qualifier("nodeHistogramAppendExecutor") Executor executor) {
+        this.executor = Objects.requireNonNull(executor, "executor must not be null");
     }
 
     public NodeHistogramAppender create(NodeHistogramFactory nodeHistogramFactory) {
-        if (mode.equalsIgnoreCase("parallel")) {
-            return new ParallelNodeHistogramAppender(nodeHistogramFactory, executorService);
-        }
-        return new SerialNodeHistogramAppender(nodeHistogramFactory);
-    }
-
-    @PreDestroy
-    public void preDestroy() {
-        if (executorService != null) {
-            executorService.shutdown();
-            try {
-                executorService.awaitTermination(10, TimeUnit.SECONDS);
-            } catch (InterruptedException e) {
-                Thread.currentThread().interrupt();
-            }
-        }
+        return new DefaultNodeHistogramAppender(nodeHistogramFactory, executor);
     }
 }
diff --git a/web/src/main/java/com/navercorp/pinpoint/web/applicationmap/appender/histogram/SerialNodeHistogramAppender.java b/web/src/main/java/com/navercorp/pinpoint/web/applicationmap/appender/histogram/SerialNodeHistogramAppender.java
deleted file mode 100644
index 3d00e2c48..000000000
--- a/web/src/main/java/com/navercorp/pinpoint/web/applicationmap/appender/histogram/SerialNodeHistogramAppender.java
+++ /dev/null
@@ -1,70 +0,0 @@
-/*
- * Copyright 2017 NAVER Corp.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package com.navercorp.pinpoint.web.applicationmap.appender.histogram;
-
-import com.navercorp.pinpoint.common.trace.ServiceType;
-import com.navercorp.pinpoint.web.applicationmap.link.LinkList;
-import com.navercorp.pinpoint.web.applicationmap.nodes.Node;
-import com.navercorp.pinpoint.web.applicationmap.nodes.NodeList;
-import com.navercorp.pinpoint.web.applicationmap.histogram.NodeHistogram;
-import com.navercorp.pinpoint.web.vo.Application;
-import com.navercorp.pinpoint.web.vo.Range;
-
-import java.util.Collection;
-
-/**
- * @author emeroad
- * @author minwoo.jung
- * @author HyunGil Jeong
- */
-public class SerialNodeHistogramAppender implements NodeHistogramAppender {
-
-    private final NodeHistogramFactory nodeHistogramFactory;
-
-    public SerialNodeHistogramAppender(NodeHistogramFactory nodeHistogramFactory) {
-        if (nodeHistogramFactory == null) {
-            throw new NullPointerException("nodeHistogramFactory must not be null");
-        }
-        this.nodeHistogramFactory = nodeHistogramFactory;
-    }
-
-    @Override
-    public void appendNodeHistogram(Range range, NodeList nodeList, LinkList linkList) {
-        final Collection<Node> nodes = nodeList.getNodeList();
-        for (Node node : nodes) {
-            node.setNodeHistogram(createNodeHistogram(range, node, linkList));
-        }
-    }
-
-    private NodeHistogram createNodeHistogram(Range range, Node node, LinkList linkList) {
-        final Application application = node.getApplication();
-        final ServiceType serviceType = application.getServiceType();
-        if (serviceType.isWas()) {
-            // for WAS nodes, set their own response time histogram
-            return nodeHistogramFactory.createWasNodeHistogram(application, range);
-        } else if (serviceType.isTerminal() || serviceType.isUnknown()) {
-            return nodeHistogramFactory.createTerminalNodeHistogram(application, range, linkList);
-        } else if (serviceType.isQueue()) {
-            // Virtual queue node - queues with agent installed will be handled above as a WAS node
-            return nodeHistogramFactory.createQueueNodeHistogram(application, range, linkList);
-        } else if (serviceType.isUser()) {
-            return nodeHistogramFactory.createUserNodeHistogram(application, range, linkList);
-        } else {
-            return nodeHistogramFactory.createEmptyNodeHistogram(application, range);
-        }
-    }
-}
diff --git a/web/src/main/java/com/navercorp/pinpoint/web/applicationmap/appender/server/ParallelServerInfoAppender.java b/web/src/main/java/com/navercorp/pinpoint/web/applicationmap/appender/server/DefaultServerInfoAppender.java
similarity index 81%
rename from web/src/main/java/com/navercorp/pinpoint/web/applicationmap/appender/server/ParallelServerInfoAppender.java
rename to web/src/main/java/com/navercorp/pinpoint/web/applicationmap/appender/server/DefaultServerInfoAppender.java
index 240a48d4d..5c17a73ff 100644
--- a/web/src/main/java/com/navercorp/pinpoint/web/applicationmap/appender/server/ParallelServerInfoAppender.java
+++ b/web/src/main/java/com/navercorp/pinpoint/web/applicationmap/appender/server/DefaultServerInfoAppender.java
@@ -1,5 +1,5 @@
 /*
- * Copyright 2017 NAVER Corp.
+ * Copyright 2018 NAVER Corp.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
  * you may not use this file except in compliance with the License.
@@ -29,30 +29,25 @@ import org.springframework.util.CollectionUtils;
 import java.util.ArrayList;
 import java.util.Collection;
 import java.util.List;
+import java.util.Objects;
 import java.util.concurrent.CompletableFuture;
-import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executor;
 import java.util.function.Supplier;
 
 /**
  * @author HyunGil Jeong
  */
-public class ParallelServerInfoAppender implements ServerInfoAppender {
+public class DefaultServerInfoAppender implements ServerInfoAppender {
 
     private final Logger logger = LoggerFactory.getLogger(this.getClass());
 
     private final ServerInstanceListFactory serverInstanceListFactory;
 
-    private final ExecutorService executorService;
+    private final Executor executor;
 
-    public ParallelServerInfoAppender(ServerInstanceListFactory serverInstanceListFactory, ExecutorService executorService) {
-        if (serverInstanceListFactory == null) {
-            throw new NullPointerException("serverInstanceListFactory must not be null");
-        }
-        if (executorService == null) {
-            throw new NullPointerException("executorService must not be null");
-        }
-        this.serverInstanceListFactory = serverInstanceListFactory;
-        this.executorService = executorService;
+    public DefaultServerInfoAppender(ServerInstanceListFactory serverInstanceListFactory, Executor executor) {
+        this.serverInstanceListFactory = Objects.requireNonNull(serverInstanceListFactory, "serverInstanceListFactory must not be null");
+        this.executor = Objects.requireNonNull(executor, "executor must not be null");
     }
 
     @Override
@@ -65,11 +60,7 @@ public class ParallelServerInfoAppender implements ServerInfoAppender {
             return;
         }
         CompletableFuture[] futures = getServerInstanceListFutures(range, nodes, linkDataDuplexMap);
-        try {
-            CompletableFuture.allOf(futures).join();
-        } catch (Exception e) {
-            logger.error("Error appending node histograms", e);
-        }
+        CompletableFuture.allOf(futures).join();
     }
 
     private CompletableFuture[] getServerInstanceListFutures(Range range, Collection<Node> nodes, LinkDataDuplexMap linkDataDuplexMap) {
@@ -95,7 +86,7 @@ public class ParallelServerInfoAppender implements ServerInfoAppender {
                 public ServerInstanceList get() {
                     return serverInstanceListFactory.createWasNodeInstanceList(node, to);
                 }
-            }, executorService);
+            }, executor);
         } else if (nodeServiceType.isTerminal()) {
             // extract information about the terminal node
             serverInstanceListFuture = CompletableFuture.completedFuture(serverInstanceListFactory.createTerminalNodeInstanceList(node, linkDataDuplexMap));
diff --git a/web/src/main/java/com/navercorp/pinpoint/web/applicationmap/appender/server/SerialServerInfoAppender.java b/web/src/main/java/com/navercorp/pinpoint/web/applicationmap/appender/server/SerialServerInfoAppender.java
deleted file mode 100644
index 537d82596..000000000
--- a/web/src/main/java/com/navercorp/pinpoint/web/applicationmap/appender/server/SerialServerInfoAppender.java
+++ /dev/null
@@ -1,67 +0,0 @@
-/*
- * Copyright 2017 NAVER Corp.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package com.navercorp.pinpoint.web.applicationmap.appender.server;
-
-import com.navercorp.pinpoint.common.trace.ServiceType;
-import com.navercorp.pinpoint.web.applicationmap.nodes.Node;
-import com.navercorp.pinpoint.web.applicationmap.nodes.NodeList;
-import com.navercorp.pinpoint.web.applicationmap.nodes.ServerInstanceList;
-import com.navercorp.pinpoint.web.applicationmap.rawdata.LinkDataDuplexMap;
-import com.navercorp.pinpoint.web.vo.Range;
-
-/**
- * @author emeroad
- * @author minwoo.jung
- * @author HyunGil Jeong
- */
-public class SerialServerInfoAppender implements ServerInfoAppender {
-
-    private final ServerInstanceListFactory serverInstanceListFactory;
-
-    public SerialServerInfoAppender(ServerInstanceListFactory serverInstanceListFactory) {
-        if (serverInstanceListFactory == null) {
-            throw new NullPointerException("serverInstanceListFactory must not be null");
-        }
-        this.serverInstanceListFactory = serverInstanceListFactory;
-    }
-
-    @Override
-    public void appendServerInfo(Range range, NodeList source, LinkDataDuplexMap linkDataDuplexMap) {
-        if (source == null) {
-            return;
-        }
-       for (Node node : source.getNodeList()) {
-           ServiceType nodeServiceType = node.getServiceType();
-           if (nodeServiceType.isUnknown()) {
-               continue;
-           }
-           ServerInstanceList serverInstanceList;
-           if (nodeServiceType.isWas()) {
-               serverInstanceList = serverInstanceListFactory.createWasNodeInstanceList(node, range.getTo());
-           } else if (nodeServiceType.isTerminal()) {
-               serverInstanceList = serverInstanceListFactory.createTerminalNodeInstanceList(node, linkDataDuplexMap);
-           } else if (nodeServiceType.isQueue()) {
-               serverInstanceList = serverInstanceListFactory.createQueueNodeInstanceList(node, linkDataDuplexMap);
-           } else if (nodeServiceType.isUser()) {
-               serverInstanceList = serverInstanceListFactory.createUserNodeInstanceList();
-           } else {
-               serverInstanceList = serverInstanceListFactory.createEmptyNodeInstanceList();
-           }
-           node.setServerInstanceList(serverInstanceList);
-       }
-    }
-}
diff --git a/web/src/main/java/com/navercorp/pinpoint/web/applicationmap/appender/server/ServerInfoAppenderFactory.java b/web/src/main/java/com/navercorp/pinpoint/web/applicationmap/appender/server/ServerInfoAppenderFactory.java
index 91c405498..aadbc0f74 100644
--- a/web/src/main/java/com/navercorp/pinpoint/web/applicationmap/appender/server/ServerInfoAppenderFactory.java
+++ b/web/src/main/java/com/navercorp/pinpoint/web/applicationmap/appender/server/ServerInfoAppenderFactory.java
@@ -16,17 +16,12 @@
 
 package com.navercorp.pinpoint.web.applicationmap.appender.server;
 
-import com.navercorp.pinpoint.common.util.PinpointThreadFactory;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
 import org.springframework.beans.factory.annotation.Autowired;
-import org.springframework.beans.factory.annotation.Value;
+import org.springframework.beans.factory.annotation.Qualifier;
 import org.springframework.stereotype.Component;
 
-import javax.annotation.PreDestroy;
-import java.util.concurrent.ExecutorService;
-import java.util.concurrent.Executors;
-import java.util.concurrent.TimeUnit;
+import java.util.Objects;
+import java.util.concurrent.Executor;
 
 /**
  * @author HyunGil Jeong
@@ -34,40 +29,14 @@ import java.util.concurrent.TimeUnit;
 @Component
 public class ServerInfoAppenderFactory {
 
-    private final Logger logger = LoggerFactory.getLogger(this.getClass());
-
-    private final String mode;
-    private final ExecutorService executorService;
+    private final Executor executor;
 
     @Autowired
-    public ServerInfoAppenderFactory(
-            @Value("#{pinpointWebProps['web.servermap.appender.mode'] ?: 'serial'}") String mode,
-            @Value("#{pinpointWebProps['web.servermap.appender.parallel.maxthreads'] ?: 16}") int maxThreads) {
-        logger.info("ServerInfoAppender mode : {}", mode);
-        this.mode = mode;
-        if (this.mode.equalsIgnoreCase("parallel")) {
-            executorService = Executors.newFixedThreadPool(maxThreads, new PinpointThreadFactory("Pinpoint-node-histogram-appender", true));
-        } else {
-            executorService = null;
-        }
+    public ServerInfoAppenderFactory(@Qualifier("serverInfoAppendExecutor") Executor executor) {
+        this.executor = Objects.requireNonNull(executor, "executor must not be null");
     }
 
     public ServerInfoAppender create(ServerInstanceListFactory serverInstanceListFactory) {
-        if (mode.equalsIgnoreCase("parallel")) {
-            return new ParallelServerInfoAppender(serverInstanceListFactory, executorService);
-        }
-        return new SerialServerInfoAppender(serverInstanceListFactory);
-    }
-
-    @PreDestroy
-    public void preDestroy() {
-        if (executorService != null) {
-            executorService.shutdown();
-            try {
-                executorService.awaitTermination(10, TimeUnit.SECONDS);
-            } catch (InterruptedException e) {
-                Thread.currentThread().interrupt();
-            }
-        }
+        return new DefaultServerInfoAppender(serverInstanceListFactory, executor);
     }
 }
diff --git a/web/src/main/java/com/navercorp/pinpoint/web/controller/UserGroupController.java b/web/src/main/java/com/navercorp/pinpoint/web/controller/UserGroupController.java
index b9c75d7df..cb9f614be 100644
--- a/web/src/main/java/com/navercorp/pinpoint/web/controller/UserGroupController.java
+++ b/web/src/main/java/com/navercorp/pinpoint/web/controller/UserGroupController.java
@@ -19,6 +19,7 @@ import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 
+import com.navercorp.pinpoint.web.vo.exception.PinpointUserGroupException;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 import org.springframework.beans.factory.annotation.Autowired;
@@ -32,8 +33,6 @@ import org.springframework.web.bind.annotation.RequestMethod;
 import org.springframework.web.bind.annotation.RequestParam;
 import org.springframework.web.bind.annotation.ResponseBody;
 
-import com.navercorp.pinpoint.web.config.ConfigProperties;
-import com.navercorp.pinpoint.web.service.AlarmService;
 import com.navercorp.pinpoint.web.service.UserGroupService;
 import com.navercorp.pinpoint.web.vo.UserGroup;
 import com.navercorp.pinpoint.web.vo.UserGroupMember;
@@ -54,80 +53,46 @@ public class UserGroupController {
 
     @Autowired
     UserGroupService userGroupService;
-    
-    @Autowired
-    AlarmService alarmService;
-    
-    @Autowired
-    private ConfigProperties webProperties;
-    
+
     @RequestMapping(method = RequestMethod.POST)
     @ResponseBody
     public Map<String, String> createUserGroup(@RequestBody UserGroup userGroup, @RequestHeader(value=SSO_USER, required=false) String userId) {
         if (StringUtils.isEmpty(userGroup.getId())) {
-            Map<String, String> result = new HashMap<>();
-            result.put("errorCode", "500");
-            result.put("errorMessage", "There is not id of userGroup in params to create user group");
-            return result;
+            return createErrorMessage("500", "There is not id of userGroup in params to create user group");
         }
-        
-        String userGroupNumber = userGroupService.createUserGroup(userGroup);
 
-        if (webProperties.isOpenSource() == false) {
-            if (initUserGroup(userGroup, userId) == false) {
-                Map<String, String> result = new HashMap<>();
-                result.put("errorCode", "500");
-                result.put("errorMessage", "There is not userId or fail to create userGroup.");
-                return result;
-            }
-        }
-        Map<String, String> result = new HashMap<>();
-        result.put("number", userGroupNumber);
-        return result;
-    }
-    
-    private boolean initUserGroup(UserGroup userGroup, String userId) {
-        if (StringUtils.isEmpty(userId)) {
-            userGroupService.deleteUserGroup(userGroup);
-            return false;
+        try {
+            String userGroupNumber = userGroupService.createUserGroup(userGroup, userId);
+            Map<String, String> result = new HashMap<>();
+            result.put("number", userGroupNumber);
+            return result;
+        } catch (PinpointUserGroupException e) {
+            logger.error(e.getMessage(), e);
+            return createErrorMessage("500", e.getMessage());
         }
-        
-        userGroupService.insertMember(new UserGroupMember(userGroup.getId(), userId));
-        return true;
     }
 
     @RequestMapping(method = RequestMethod.DELETE)
     @ResponseBody
     public Map<String, String> deleteUserGroup(@RequestBody UserGroup userGroup, @RequestHeader(value=SSO_USER, required=false) String userId) {
         if (StringUtils.isEmpty(userGroup.getId())) {
+            return createErrorMessage("500", "there is id of userGroup in params to delete user group");
+        }
+
+        try {
+            userGroupService.deleteUserGroup(userGroup, userId);
             Map<String, String> result = new HashMap<>();
-            result.put("errorCode", "500");
-            result.put("errorMessage", "there is id of userGroup in params to delete user group");
+            result.put("result", "SUCCESS");
             return result;
+        } catch (PinpointUserGroupException e) {
+            logger.error(e.getMessage(), e);
+            return createErrorMessage("500", e.getMessage());
         }
-        if (webProperties.isOpenSource() == false) {
-            if (checkValid(userId, userGroup.getId()) == false) {
-                Map<String, String> result = new HashMap<>();
-                result.put("errorCode", "500");
-                result.put("errorMessage", "There is not userId or you don't have authoriy for user group.");
-                return result;
-            }
-        }
-        userGroupService.deleteUserGroup(userGroup);
-        userGroupService.deleteMemberByUserGroupId(userGroup.getId());
-        alarmService.deleteRuleByUserGroupId(userGroup.getId());
-        
-        
-
-        Map<String, String> result = new HashMap<>();
-        result.put("result", "SUCCESS");
-        return result;
     }
     
     @RequestMapping(method = RequestMethod.GET)
     @ResponseBody
     public List<UserGroup> getUserGroup(@RequestParam(value=USER_ID, required=false) String userId, @RequestParam(value=USER_GROUP_ID, required=false) String userGroupId) {
-        
         if (!StringUtils.isEmpty(userId)) {
             return userGroupService.selectUserGroupByUserId(userId);
         } else if (!StringUtils.isEmpty(userGroupId)) {
@@ -135,69 +100,41 @@ public class UserGroupController {
         }
         return userGroupService.selectUserGroup();
     }
-    
+
     @RequestMapping(value = "/member", method = RequestMethod.POST)
     @ResponseBody
     public Map<String, String> insertUserGroupMember(@RequestBody UserGroupMemberParam userGroupMember, @RequestHeader(value=SSO_USER, required=false) String userId) {
         if (StringUtils.isEmpty(userGroupMember.getMemberId()) || StringUtils.isEmpty(userGroupMember.getUserGroupId())) {
-            Map<String, String> result = new HashMap<>();
-            result.put("errorCode", "500");
-            result.put("errorMessage", "there is not userGroupId or memberId in params to insert user group member");
-            return result;
-        }
-
-        if (webProperties.isOpenSource() == false) {
-            boolean isValid = checkValid(userId, userGroupMember.getUserGroupId());
-            if (isValid == false) {
-                Map<String, String> result = new HashMap<>();
-                result.put("errorCode", "500");
-                result.put("errorMessage", "there is not userId or you don't have authority for user group.");
-                return result;
-            }
+            return createErrorMessage("500", "there is not userGroupId or memberId in params to insert user group member");
         }
-        
-        userGroupService.insertMember(userGroupMember);
 
-        Map<String, String> result = new HashMap<>();
-        result.put("result", "SUCCESS");
-        return result;
-    }
-    
-    private boolean checkValid(String userId, String userGroupId) {
-        if (StringUtils.isEmpty(userId)) {
-            return false;
-        }
-        if (userGroupService.containMemberForUserGroup(userId, userGroupId) == false) {
-            return false;
+        try {
+            userGroupService.insertMemberWithCheckAuthority(userGroupMember, userId);
+            Map<String, String> result = new HashMap<>();
+            result.put("result", "SUCCESS");
+            return result;
+        } catch (PinpointUserGroupException e) {
+            logger.error(e.getMessage(), e);
+            return createErrorMessage("500", e.getMessage());
         }
-        
-        return true;
     }
 
     @RequestMapping(value = "/member", method = RequestMethod.DELETE)
     @ResponseBody
     public Map<String, String> deleteUserGroupMember(@RequestBody UserGroupMemberParam userGroupMember, @RequestHeader(value=SSO_USER, required=false) String userId) {
         if (StringUtils.isEmpty(userGroupMember.getUserGroupId()) || StringUtils.isEmpty(userGroupMember.getMemberId())) {
+            return createErrorMessage("500", "there is not userGroupId or memberId in params to delete user group member");
+        }
+
+        try {
+            userGroupService.deleteMemberWithCheckAuthority(userGroupMember, userId);
             Map<String, String> result = new HashMap<>();
-            result.put("errorCode", "500");
-            result.put("errorMessage", "there is not userGroupId or memberId in params to delete user group member");
+            result.put("result", "SUCCESS");
             return result;
+        } catch (PinpointUserGroupException e) {
+            logger.error(e.getMessage(), e);
+            return createErrorMessage("500", e.getMessage());
         }
-        if (webProperties.isOpenSource() == false) {
-            boolean isValid = checkValid(userId, userGroupMember.getUserGroupId());
-            if (isValid == false) {
-                Map<String, String> result = new HashMap<>();
-                result.put("errorCode", "500");
-                result.put("errorMessage", "there is not userId or you don't have authority for user group.");
-                return result;
-            }
-        }
-        
-        userGroupService.deleteMember(userGroupMember);
-        
-        Map<String, String> result = new HashMap<>();
-        result.put("result", "SUCCESS");
-        return result;
     }
     
     @RequestMapping(value = "/member", method = RequestMethod.GET)
@@ -210,9 +147,13 @@ public class UserGroupController {
     @ResponseBody
     public Map<String, String> handleException(Exception e) {
         logger.error("Exception occurred while trying to CRUD userGroup information", e);
+        return createErrorMessage("500", "Exception occurred while trying to CRUD userGroup information");
+    }
+
+    private Map<String, String> createErrorMessage(String code, String errorMessage) {
         Map<String, String> result = new HashMap<>();
-        result.put("errorCode", "500");
-        result.put("errorMessage", "Exception occurred while trying to CRUD userGroup information");
+        result.put("errorCode", code);
+        result.put("errorMessage", errorMessage);
         return result;
     }
 }
diff --git a/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseAgentEventDao.java b/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseAgentEventDao.java
index e808224df..400641324 100644
--- a/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseAgentEventDao.java
+++ b/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseAgentEventDao.java
@@ -19,6 +19,7 @@ package com.navercorp.pinpoint.web.dao.hbase;
 import com.navercorp.pinpoint.common.hbase.HBaseTables;
 import com.navercorp.pinpoint.common.hbase.HbaseOperations2;
 import com.navercorp.pinpoint.common.hbase.RowMapper;
+import com.navercorp.pinpoint.common.hbase.TableNameProvider;
 import com.navercorp.pinpoint.common.server.bo.event.AgentEventBo;
 import com.navercorp.pinpoint.common.server.util.AgentEventType;
 import com.navercorp.pinpoint.common.server.util.RowKeyUtils;
@@ -28,6 +29,7 @@ import com.navercorp.pinpoint.web.dao.AgentEventDao;
 import com.navercorp.pinpoint.web.mapper.AgentEventResultsExtractor;
 import com.navercorp.pinpoint.web.vo.Range;
 import org.apache.commons.collections.CollectionUtils;
+import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.client.Scan;
 import org.apache.hadoop.hbase.filter.BinaryComparator;
 import org.apache.hadoop.hbase.filter.CompareFilter;
@@ -57,6 +59,9 @@ public class HbaseAgentEventDao implements AgentEventDao {
     private HbaseOperations2 hbaseOperations2;
 
     @Autowired
+    private TableNameProvider tableNameProvider;
+
+    @Autowired
     @Qualifier("agentEventMapper")
     private RowMapper<List<AgentEventBo>> agentEventMapper;
 
@@ -88,7 +93,9 @@ public class HbaseAgentEventDao implements AgentEventDao {
             }
             scan.setFilter(filterList);
         }
-        List<AgentEventBo> agentEvents = this.hbaseOperations2.find(HBaseTables.AGENT_EVENT, scan, agentEventResultsExtractor);
+
+        TableName agentEventTableName = tableNameProvider.getTableName(HBaseTables.AGENT_EVENT_STR);
+        List<AgentEventBo> agentEvents = this.hbaseOperations2.find(agentEventTableName, scan, agentEventResultsExtractor);
         logger.debug("agentEvents found. {}", agentEvents);
         return agentEvents;
     }
@@ -107,7 +114,9 @@ public class HbaseAgentEventDao implements AgentEventDao {
 
         final byte[] rowKey = createRowKey(agentId, eventTimestamp);
         byte[] qualifier = Bytes.toBytes(eventType.getCode());
-        List<AgentEventBo> events = this.hbaseOperations2.get(HBaseTables.AGENT_EVENT, rowKey,
+
+        TableName agentEventTableName = tableNameProvider.getTableName(HBaseTables.AGENT_EVENT_STR);
+        List<AgentEventBo> events = this.hbaseOperations2.get(agentEventTableName, rowKey,
                 HBaseTables.AGENT_EVENT_CF_EVENTS, qualifier, this.agentEventMapper);
         if (CollectionUtils.isEmpty(events)) {
             return null;
diff --git a/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseAgentInfoDao.java b/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseAgentInfoDao.java
index 00e9fe199..b4bf2973c 100644
--- a/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseAgentInfoDao.java
+++ b/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseAgentInfoDao.java
@@ -18,6 +18,7 @@ package com.navercorp.pinpoint.web.dao.hbase;
 
 import com.navercorp.pinpoint.common.hbase.HBaseTables;
 import com.navercorp.pinpoint.common.hbase.HbaseOperations2;
+import com.navercorp.pinpoint.common.hbase.TableNameProvider;
 import com.navercorp.pinpoint.common.server.util.RowKeyUtils;
 import com.navercorp.pinpoint.common.util.TimeUtils;
 import com.navercorp.pinpoint.web.dao.AgentInfoDao;
@@ -25,6 +26,7 @@ import com.navercorp.pinpoint.web.dao.AgentInfoDao;
 import com.navercorp.pinpoint.web.mapper.AgentInfoResultsExtractor;
 import com.navercorp.pinpoint.web.vo.AgentInfo;
 import org.apache.commons.collections.CollectionUtils;
+import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.client.*;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.springframework.beans.factory.annotation.Autowired;
@@ -47,6 +49,9 @@ public class HbaseAgentInfoDao implements AgentInfoDao {
     private HbaseOperations2 hbaseOperations2;
 
     @Autowired
+    private TableNameProvider tableNameProvider;
+
+    @Autowired
     private AgentInfoResultsExtractor agentInfoResultsExtractor;
 
     /**
@@ -60,7 +65,9 @@ public class HbaseAgentInfoDao implements AgentInfoDao {
             throw new NullPointerException("agentId must not be null");
         }
         Scan scan = createScanForInitialAgentInfo(agentId);
-        return this.hbaseOperations2.find(HBaseTables.AGENTINFO, scan, agentInfoResultsExtractor);
+
+        TableName agentInfoTableName = tableNameProvider.getTableName(HBaseTables.AGENTINFO_STR);
+        return this.hbaseOperations2.find(agentInfoTableName, scan, agentInfoResultsExtractor);
     }
 
     @Override
@@ -72,7 +79,9 @@ public class HbaseAgentInfoDao implements AgentInfoDao {
         for (String agentId : agentIds) {
             scans.add(createScanForInitialAgentInfo(agentId));
         }
-        return this.hbaseOperations2.find(HBaseTables.AGENTINFO, scans, agentInfoResultsExtractor);
+
+        TableName agentInfoTableName = tableNameProvider.getTableName(HBaseTables.AGENTINFO_STR);
+        return this.hbaseOperations2.find(agentInfoTableName, scans, agentInfoResultsExtractor);
     }
 
     private Scan createScanForInitialAgentInfo(String agentId) {
@@ -101,7 +110,8 @@ public class HbaseAgentInfoDao implements AgentInfoDao {
 
         Scan scan = createScan(agentId, timestamp);
 
-        return this.hbaseOperations2.find(HBaseTables.AGENTINFO, scan, agentInfoResultsExtractor);
+        TableName agentInfoTableName = tableNameProvider.getTableName(HBaseTables.AGENTINFO_STR);
+        return this.hbaseOperations2.find(agentInfoTableName, scan, agentInfoResultsExtractor);
     }
 
     @Override
@@ -115,7 +125,8 @@ public class HbaseAgentInfoDao implements AgentInfoDao {
             scans.add(createScan(agentId, timestamp));
         }
 
-        return this.hbaseOperations2.findParallel(HBaseTables.AGENTINFO, scans, agentInfoResultsExtractor);
+        TableName agentInfoTableName = tableNameProvider.getTableName(HBaseTables.AGENTINFO_STR);
+        return this.hbaseOperations2.findParallel(agentInfoTableName, scans, agentInfoResultsExtractor);
     }
 
     private Scan createScan(String agentId, long currentTime) {
diff --git a/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseAgentLifeCycleDao.java b/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseAgentLifeCycleDao.java
index a5a133678..78cc216a1 100644
--- a/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseAgentLifeCycleDao.java
+++ b/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseAgentLifeCycleDao.java
@@ -20,10 +20,12 @@ import java.util.ArrayList;
 import java.util.Collection;
 import java.util.List;
 
+import com.navercorp.pinpoint.common.hbase.TableNameProvider;
 import com.navercorp.pinpoint.common.server.util.AgentLifeCycleState;
 import com.navercorp.pinpoint.web.vo.AgentInfo;
 import com.navercorp.pinpoint.web.vo.AgentStatus;
 import org.apache.commons.collections.CollectionUtils;
+import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.client.Result;
 import org.apache.hadoop.hbase.client.ResultScanner;
 import org.apache.hadoop.hbase.client.Scan;
@@ -54,6 +56,9 @@ public class HbaseAgentLifeCycleDao implements AgentLifeCycleDao {
     private HbaseOperations2 hbaseOperations2;
 
     @Autowired
+    private TableNameProvider tableNameProvider;
+
+    @Autowired
     @Qualifier("agentLifeCycleMapper")
     private RowMapper<AgentLifeCycleBo> agentLifeCycleMapper;
 
@@ -64,7 +69,8 @@ public class HbaseAgentLifeCycleDao implements AgentLifeCycleDao {
 
         Scan scan = createScan(agentId, 0, timestamp);
 
-        AgentLifeCycleBo agentLifeCycleBo = this.hbaseOperations2.find(HBaseTables.AGENT_LIFECYCLE, scan, new MostRecentAgentLifeCycleResultsExtractor(this.agentLifeCycleMapper, timestamp));
+        TableName agentLifeCycleTableName = tableNameProvider.getTableName(HBaseTables.AGENT_LIFECYCLE_STR);
+        AgentLifeCycleBo agentLifeCycleBo = this.hbaseOperations2.find(agentLifeCycleTableName, scan, new MostRecentAgentLifeCycleResultsExtractor(this.agentLifeCycleMapper, timestamp));
         return createAgentStatus(agentId, agentLifeCycleBo);
     }
 
@@ -80,7 +86,8 @@ public class HbaseAgentLifeCycleDao implements AgentLifeCycleDao {
         final long fromTimestamp = toTimestamp - 1;
         Scan scan = createScan(agentId, fromTimestamp, toTimestamp);
 
-        AgentLifeCycleBo agentLifeCycleBo = this.hbaseOperations2.find(HBaseTables.AGENT_LIFECYCLE, scan, new MostRecentAgentLifeCycleResultsExtractor(this.agentLifeCycleMapper, timestamp));
+        TableName agentLifeCycleTableName = tableNameProvider.getTableName(HBaseTables.AGENT_LIFECYCLE_STR);
+        AgentLifeCycleBo agentLifeCycleBo = this.hbaseOperations2.find(agentLifeCycleTableName, scan, new MostRecentAgentLifeCycleResultsExtractor(this.agentLifeCycleMapper, timestamp));
         AgentStatus agentStatus = createAgentStatus(agentId, agentLifeCycleBo);
         agentInfo.setStatus(agentStatus);
     }
@@ -100,7 +107,9 @@ public class HbaseAgentLifeCycleDao implements AgentLifeCycleDao {
                 scans.add(createScan(agentId, fromTimestamp, toTimestamp));
             }
         }
-        List<AgentLifeCycleBo> agentLifeCycles = this.hbaseOperations2.findParallel(HBaseTables.AGENT_LIFECYCLE, scans, new MostRecentAgentLifeCycleResultsExtractor(this.agentLifeCycleMapper, timestamp));
+
+        TableName agentLifeCycleTableName = tableNameProvider.getTableName(HBaseTables.AGENT_LIFECYCLE_STR);
+        List<AgentLifeCycleBo> agentLifeCycles = this.hbaseOperations2.findParallel(agentLifeCycleTableName, scans, new MostRecentAgentLifeCycleResultsExtractor(this.agentLifeCycleMapper, timestamp));
         int idx = 0;
         for (AgentInfo agentInfo : agentInfos) {
             if (agentInfo != null) {
diff --git a/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseApiMetaDataDao.java b/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseApiMetaDataDao.java
index ae96e3289..a39dea240 100644
--- a/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseApiMetaDataDao.java
+++ b/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseApiMetaDataDao.java
@@ -18,8 +18,10 @@ package com.navercorp.pinpoint.web.dao.hbase;
 
 import java.util.List;
 
+import com.navercorp.pinpoint.common.hbase.TableNameProvider;
 import com.sematext.hbase.wd.RowKeyDistributorByHashPrefix;
 
+import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.client.Get;
 import org.springframework.beans.factory.annotation.Autowired;
 import org.springframework.beans.factory.annotation.Qualifier;
@@ -44,6 +46,9 @@ public class HbaseApiMetaDataDao implements ApiMetaDataDao {
     private HbaseOperations2 hbaseOperations2;
 
     @Autowired
+    private TableNameProvider tableNameProvider;
+
+    @Autowired
     @Qualifier("apiMetaDataMapper")
     private RowMapper<List<ApiMetaDataBo>> apiMetaDataMapper;
 
@@ -62,7 +67,9 @@ public class HbaseApiMetaDataDao implements ApiMetaDataDao {
         byte[] sqlId = getDistributedKey(apiMetaDataBo.toRowKey());
         Get get = new Get(sqlId);
         get.addFamily(HBaseTables.API_METADATA_CF_API);
-        return hbaseOperations2.get(HBaseTables.API_METADATA, get, apiMetaDataMapper);
+
+        TableName apiMetaDataTableName = tableNameProvider.getTableName(HBaseTables.API_METADATA_STR);
+        return hbaseOperations2.get(apiMetaDataTableName, get, apiMetaDataMapper);
     }
 
     private byte[] getDistributedKey(byte[] rowKey) {
diff --git a/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseApplicationIndexDao.java b/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseApplicationIndexDao.java
index 9cab5e30a..9cb10183b 100644
--- a/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseApplicationIndexDao.java
+++ b/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseApplicationIndexDao.java
@@ -20,9 +20,11 @@ import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
 
+import com.navercorp.pinpoint.common.hbase.TableNameProvider;
 import org.apache.commons.collections.CollectionUtils;
 import org.apache.commons.collections.MapUtils;
 import org.apache.commons.lang3.StringUtils;
+import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.client.Delete;
 import org.apache.hadoop.hbase.client.Get;
 import org.apache.hadoop.hbase.client.Scan;
@@ -48,6 +50,9 @@ public class HbaseApplicationIndexDao implements ApplicationIndexDao {
     private HbaseOperations2 hbaseOperations2;
 
     @Autowired
+    private TableNameProvider tableNameProvider;
+
+    @Autowired
     @Qualifier("applicationNameMapper")
     private RowMapper<List<Application>> applicationNameMapper;
 
@@ -59,7 +64,9 @@ public class HbaseApplicationIndexDao implements ApplicationIndexDao {
     public List<Application> selectAllApplicationNames() {
         Scan scan = new Scan();
         scan.setCaching(30);
-        List<List<Application>> results = hbaseOperations2.find(HBaseTables.APPLICATION_INDEX, scan, applicationNameMapper);
+
+        TableName applicationIndexTableName = tableNameProvider.getTableName(HBaseTables.APPLICATION_INDEX_STR);
+        List<List<Application>> results = hbaseOperations2.find(applicationIndexTableName, scan, applicationNameMapper);
         List<Application> applications = new ArrayList<>();
         for (List<Application> result : results) {
             applications.addAll(result);
@@ -77,14 +84,17 @@ public class HbaseApplicationIndexDao implements ApplicationIndexDao {
         Get get = new Get(rowKey);
         get.addFamily(HBaseTables.APPLICATION_INDEX_CF_AGENTS);
 
-        return hbaseOperations2.get(HBaseTables.APPLICATION_INDEX, get, agentIdMapper);
+        TableName applicationIndexTableName = tableNameProvider.getTableName(HBaseTables.APPLICATION_INDEX_STR);
+        return hbaseOperations2.get(applicationIndexTableName, get, agentIdMapper);
     }
 
     @Override
     public void deleteApplicationName(String applicationName) {
         byte[] rowKey = Bytes.toBytes(applicationName);
         Delete delete = new Delete(rowKey);
-        hbaseOperations2.delete(HBaseTables.APPLICATION_INDEX, delete);
+
+        TableName applicationIndexTableName = tableNameProvider.getTableName(HBaseTables.APPLICATION_INDEX_STR);
+        hbaseOperations2.delete(applicationIndexTableName, delete);
     }
 
     @Override
@@ -112,7 +122,9 @@ public class HbaseApplicationIndexDao implements ApplicationIndexDao {
                 deletes.add(delete);
             }
         }
-        hbaseOperations2.delete(HBaseTables.APPLICATION_INDEX, deletes);
+
+        TableName applicationIndexTableName = tableNameProvider.getTableName(HBaseTables.APPLICATION_INDEX_STR);
+        hbaseOperations2.delete(applicationIndexTableName, deletes);
     }
 
     @Override
@@ -127,6 +139,8 @@ public class HbaseApplicationIndexDao implements ApplicationIndexDao {
         Delete delete = new Delete(rowKey);
         byte[] qualifier = Bytes.toBytes(agentId);
         delete.addColumns(HBaseTables.APPLICATION_INDEX_CF_AGENTS, qualifier);
-        hbaseOperations2.delete(HBaseTables.APPLICATION_INDEX, delete);
+
+        TableName applicationIndexTableName = tableNameProvider.getTableName(HBaseTables.APPLICATION_INDEX_STR);
+        hbaseOperations2.delete(applicationIndexTableName, delete);
     }
 }
diff --git a/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseApplicationStatDaoOperations.java b/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseApplicationStatDaoOperations.java
index 702d738bb..262ec1117 100644
--- a/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseApplicationStatDaoOperations.java
+++ b/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseApplicationStatDaoOperations.java
@@ -18,6 +18,7 @@ package com.navercorp.pinpoint.web.dao.hbase;
 
 import com.navercorp.pinpoint.common.hbase.HBaseTables;
 import com.navercorp.pinpoint.common.hbase.HbaseOperations2;
+import com.navercorp.pinpoint.common.hbase.TableNameProvider;
 import com.navercorp.pinpoint.common.server.bo.codec.stat.ApplicationStatDecoder;
 import com.navercorp.pinpoint.common.server.bo.serializer.stat.AgentStatUtils;
 import com.navercorp.pinpoint.common.server.bo.serializer.stat.ApplicationStatHbaseOperationFactory;
@@ -28,6 +29,7 @@ import com.navercorp.pinpoint.web.mapper.stat.ApplicationStatMapper;
 import com.navercorp.pinpoint.web.mapper.stat.SampledApplicationStatResultExtractor;
 import com.navercorp.pinpoint.web.vo.Range;
 import com.navercorp.pinpoint.web.vo.stat.AggregationStatData;
+import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.client.Scan;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -51,6 +53,9 @@ public class HbaseApplicationStatDaoOperations {
     private HbaseOperations2 hbaseOperations2;
 
     @Autowired
+    private TableNameProvider tableNameProvider;
+
+    @Autowired
     private ApplicationStatHbaseOperationFactory operationFactory;
 
     List<AggregationStatData> getSampledStatList(StatType statType, SampledApplicationStatResultExtractor resultExtractor, String applicationId, Range range) {
@@ -64,7 +69,9 @@ public class HbaseApplicationStatDaoOperations {
             throw new NullPointerException("resultExtractor must not be null");
         }
         Scan scan = this.createScan(statType, applicationId, range);
-        return hbaseOperations2.findParallel(HBaseTables.APPLICATION_STAT_AGGRE, scan, this.operationFactory.getRowKeyDistributor(), resultExtractor, APPLICATION_STAT_NUM_PARTITIONS);
+
+        TableName applicationStatAggreTableName = tableNameProvider.getTableName(HBaseTables.APPLICATION_STAT_AGGRE_STR);
+        return hbaseOperations2.findParallel(applicationStatAggreTableName, scan, this.operationFactory.getRowKeyDistributor(), resultExtractor, APPLICATION_STAT_NUM_PARTITIONS);
     }
 
     ApplicationStatMapper createRowMapper(ApplicationStatDecoder decoder, Range range) {
diff --git a/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseApplicationTraceIndexDao.java b/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseApplicationTraceIndexDao.java
index 7b7f4d09a..77e2215a4 100644
--- a/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseApplicationTraceIndexDao.java
+++ b/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseApplicationTraceIndexDao.java
@@ -23,6 +23,7 @@ import com.navercorp.pinpoint.common.hbase.HBaseTables;
 import com.navercorp.pinpoint.common.hbase.HbaseOperations2;
 import com.navercorp.pinpoint.common.hbase.LimitEventHandler;
 import com.navercorp.pinpoint.common.hbase.RowMapper;
+import com.navercorp.pinpoint.common.hbase.TableNameProvider;
 import com.navercorp.pinpoint.common.util.BytesUtils;
 import com.navercorp.pinpoint.common.util.DateUtils;
 import com.navercorp.pinpoint.common.server.util.SpanUtils;
@@ -42,6 +43,7 @@ import com.sematext.hbase.wd.AbstractRowKeyDistributor;
 import org.apache.commons.collections.CollectionUtils;
 import org.apache.hadoop.hbase.Cell;
 import org.apache.hadoop.hbase.CellUtil;
+import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.client.Result;
 import org.apache.hadoop.hbase.client.Scan;
 import org.apache.hadoop.hbase.filter.BinaryPrefixComparator;
@@ -75,6 +77,9 @@ public class HbaseApplicationTraceIndexDao implements ApplicationTraceIndexDao {
     private HbaseOperations2 hbaseOperations2;
 
     @Autowired
+    private TableNameProvider tableNameProvider;
+
+    @Autowired
     @Qualifier("transactionIdMapper")
     private RowMapper<List<TransactionId>> traceIndexMapper;
 
@@ -108,7 +113,9 @@ public class HbaseApplicationTraceIndexDao implements ApplicationTraceIndexDao {
 
         final LimitedScanResult<List<TransactionId>> limitedScanResult = new LimitedScanResult<>();
         LastRowAccessor lastRowAccessor = new LastRowAccessor();
-        List<List<TransactionId>> traceIndexList = hbaseOperations2.findParallel(HBaseTables.APPLICATION_TRACE_INDEX,
+
+        TableName applicationTraceIndexTableName = tableNameProvider.getTableName(HBaseTables.APPLICATION_TRACE_INDEX_STR);
+        List<List<TransactionId>> traceIndexList = hbaseOperations2.findParallel(applicationTraceIndexTableName,
                 scan, traceIdRowKeyDistributor, limit, traceIndexMapper, lastRowAccessor, APPLICATION_TRACE_INDEX_NUM_PARTITIONS);
 
         List<TransactionId> transactionIdSum = new ArrayList<>(128);
@@ -149,7 +156,9 @@ public class HbaseApplicationTraceIndexDao implements ApplicationTraceIndexDao {
 
         final LimitedScanResult<List<TransactionId>> limitedScanResult = new LimitedScanResult<>();
         LastRowAccessor lastRowAccessor = new LastRowAccessor();
-        List<List<TransactionId>> traceIndexList = hbaseOperations2.findParallel(HBaseTables.APPLICATION_TRACE_INDEX,
+
+        TableName applicationTraceIndexTableName = tableNameProvider.getTableName(HBaseTables.APPLICATION_TRACE_INDEX_STR);
+        List<List<TransactionId>> traceIndexList = hbaseOperations2.findParallel(applicationTraceIndexTableName,
                 scan, traceIdRowKeyDistributor, limit, traceIndexMapper, lastRowAccessor, APPLICATION_TRACE_INDEX_NUM_PARTITIONS);
 
         List<TransactionId> transactionIdSum = new ArrayList<>(128);
@@ -269,7 +278,8 @@ public class HbaseApplicationTraceIndexDao implements ApplicationTraceIndexDao {
         ResponseTimeRange responseTimeRange = area.getResponseTimeRange();
         TraceIndexScatterMapper2 mapper = new TraceIndexScatterMapper2(responseTimeRange.getFrom(), responseTimeRange.getTo());
 
-        List<List<Dot>> dotListList = hbaseOperations2.findParallel(HBaseTables.APPLICATION_TRACE_INDEX, scan, traceIdRowKeyDistributor, limit, mapper, APPLICATION_TRACE_INDEX_NUM_PARTITIONS);
+        TableName applicationTraceIndexTableName = tableNameProvider.getTableName(HBaseTables.APPLICATION_TRACE_INDEX_STR);
+        List<List<Dot>> dotListList = hbaseOperations2.findParallel(applicationTraceIndexTableName, scan, traceIdRowKeyDistributor, limit, mapper, APPLICATION_TRACE_INDEX_NUM_PARTITIONS);
 
         List<Dot> result = new ArrayList<>();
         for(List<Dot> dotList : dotListList) {
@@ -294,7 +304,9 @@ public class HbaseApplicationTraceIndexDao implements ApplicationTraceIndexDao {
         Scan scan = createScan(applicationName, range, scanBackward);
 
         TraceIndexScatterMapper3 mapper = new TraceIndexScatterMapper3(range.getFrom(), range.getTo(), xGroupUnit, yGroupUnit);
-        List<ScatterData> dotGroupList = hbaseOperations2.findParallel(HBaseTables.APPLICATION_TRACE_INDEX, scan, traceIdRowKeyDistributor, limit, mapper, APPLICATION_TRACE_INDEX_NUM_PARTITIONS);
+
+        TableName applicationTraceIndexTableName = tableNameProvider.getTableName(HBaseTables.APPLICATION_TRACE_INDEX_STR);
+        List<ScatterData> dotGroupList = hbaseOperations2.findParallel(applicationTraceIndexTableName, scan, traceIdRowKeyDistributor, limit, mapper, APPLICATION_TRACE_INDEX_NUM_PARTITIONS);
 
         if (CollectionUtils.isEmpty(dotGroupList)) {
             return new ScatterData(range.getFrom(), range.getTo(), xGroupUnit, yGroupUnit);
diff --git a/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseHostApplicationMapDao.java b/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseHostApplicationMapDao.java
index 0cbce95ec..4e633ed26 100644
--- a/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseHostApplicationMapDao.java
+++ b/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseHostApplicationMapDao.java
@@ -26,6 +26,7 @@ import com.navercorp.pinpoint.common.buffer.Buffer;
 import com.navercorp.pinpoint.common.hbase.HBaseTables;
 import com.navercorp.pinpoint.common.hbase.HbaseOperations2;
 import com.navercorp.pinpoint.common.hbase.RowMapper;
+import com.navercorp.pinpoint.common.hbase.TableNameProvider;
 import com.navercorp.pinpoint.common.util.TimeSlot;
 import com.navercorp.pinpoint.common.util.TimeUtils;
 import com.navercorp.pinpoint.web.dao.HostApplicationMapDao;
@@ -35,6 +36,7 @@ import com.navercorp.pinpoint.web.vo.Range;
 import com.sematext.hbase.wd.AbstractRowKeyDistributor;
 
 import org.apache.commons.collections.CollectionUtils;
+import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.client.Scan;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -60,6 +62,9 @@ public class HbaseHostApplicationMapDao implements HostApplicationMapDao {
     private HbaseOperations2 hbaseOperations2;
 
     @Autowired
+    private TableNameProvider tableNameProvider;
+
+    @Autowired
     @Qualifier("hostApplicationMapperVer2")
     private RowMapper<List<AcceptApplication>> hostApplicationMapperVer2;
 
@@ -77,7 +82,9 @@ public class HbaseHostApplicationMapDao implements HostApplicationMapDao {
             throw new NullPointerException("fromApplication must not be null");
         }
         final Scan scan = createScan(fromApplication, range);
-        final List<List<AcceptApplication>> result = hbaseOperations2.findParallel(HBaseTables.HOST_APPLICATION_MAP_VER2, scan, acceptApplicationRowKeyDistributor, hostApplicationMapperVer2, HOST_APPLICATION_MAP_VER2_NUM_PARTITIONS);
+
+        TableName hostApplicationMapTableName = tableNameProvider.getTableName(HBaseTables.HOST_APPLICATION_MAP_VER2_STR);
+        final List<List<AcceptApplication>> result = hbaseOperations2.findParallel(hostApplicationMapTableName, scan, acceptApplicationRowKeyDistributor, hostApplicationMapperVer2, HOST_APPLICATION_MAP_VER2_NUM_PARTITIONS);
         if (CollectionUtils.isNotEmpty(result)) {
             final Set<AcceptApplication> resultSet = new HashSet<>();
             for (List<AcceptApplication> resultList : result) {
diff --git a/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseMapResponseTimeDao.java b/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseMapResponseTimeDao.java
index 57e682cb4..95c95cc7f 100644
--- a/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseMapResponseTimeDao.java
+++ b/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseMapResponseTimeDao.java
@@ -19,6 +19,7 @@ package com.navercorp.pinpoint.web.dao.hbase;
 import com.navercorp.pinpoint.common.hbase.HBaseTables;
 import com.navercorp.pinpoint.common.hbase.HbaseOperations2;
 import com.navercorp.pinpoint.common.hbase.RowMapper;
+import com.navercorp.pinpoint.common.hbase.TableNameProvider;
 import com.navercorp.pinpoint.common.util.ApplicationMapStatisticsUtils;
 import com.navercorp.pinpoint.web.dao.MapResponseDao;
 import com.navercorp.pinpoint.web.vo.Application;
@@ -27,6 +28,7 @@ import com.navercorp.pinpoint.web.vo.RangeFactory;
 import com.navercorp.pinpoint.web.vo.ResponseTime;
 
 import com.sematext.hbase.wd.RowKeyDistributorByHashPrefix;
+import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.client.Scan;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -58,6 +60,9 @@ public class HbaseMapResponseTimeDao implements MapResponseDao {
     private HbaseOperations2 hbaseOperations2;
 
     @Autowired
+    private TableNameProvider tableNameProvider;
+
+    @Autowired
     private RangeFactory rangeFactory;
 
     @Autowired
@@ -76,7 +81,8 @@ public class HbaseMapResponseTimeDao implements MapResponseDao {
 
         Scan scan = createScan(application, range, HBaseTables.MAP_STATISTICS_SELF_VER2_CF_COUNTER);
 
-        List<ResponseTime> responseTimeList = hbaseOperations2.findParallel(HBaseTables.MAP_STATISTICS_SELF_VER2, scan, rowKeyDistributorByHashPrefix, responseTimeMapper, MAP_STATISTICS_SELF_VER2_NUM_PARTITIONS);
+        TableName mapStatisticsSelfTableName = tableNameProvider.getTableName(HBaseTables.MAP_STATISTICS_SELF_VER2_STR);
+        List<ResponseTime> responseTimeList = hbaseOperations2.findParallel(mapStatisticsSelfTableName, scan, rowKeyDistributorByHashPrefix, responseTimeMapper, MAP_STATISTICS_SELF_VER2_NUM_PARTITIONS);
         if (logger.isDebugEnabled()) {
             logger.debug("Self data {}", responseTimeList);
         }
diff --git a/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseMapStatisticsCalleeDao.java b/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseMapStatisticsCalleeDao.java
index 53db8debf..78f8aa6b0 100644
--- a/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseMapStatisticsCalleeDao.java
+++ b/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseMapStatisticsCalleeDao.java
@@ -20,6 +20,7 @@ import com.navercorp.pinpoint.common.hbase.HBaseTables;
 import com.navercorp.pinpoint.common.hbase.HbaseOperations2;
 import com.navercorp.pinpoint.common.hbase.ResultsExtractor;
 import com.navercorp.pinpoint.common.hbase.RowMapper;
+import com.navercorp.pinpoint.common.hbase.TableNameProvider;
 import com.navercorp.pinpoint.common.util.ApplicationMapStatisticsUtils;
 import com.navercorp.pinpoint.web.applicationmap.rawdata.LinkDataMap;
 import com.navercorp.pinpoint.web.dao.MapStatisticsCalleeDao;
@@ -31,6 +32,7 @@ import com.navercorp.pinpoint.web.vo.Range;
 import com.navercorp.pinpoint.web.vo.RangeFactory;
 
 import com.sematext.hbase.wd.RowKeyDistributorByHashPrefix;
+import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.client.Scan;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -38,6 +40,8 @@ import org.springframework.beans.factory.annotation.Autowired;
 import org.springframework.beans.factory.annotation.Qualifier;
 import org.springframework.stereotype.Repository;
 
+import java.util.Objects;
+
 /**
  * @author netspider
  * @author emeroad
@@ -46,24 +50,33 @@ import org.springframework.stereotype.Repository;
 public class HbaseMapStatisticsCalleeDao implements MapStatisticsCalleeDao {
 
     private static final int MAP_STATISTICS_CALLER_VER2_NUM_PARTITIONS = 32;
+    private static final int SCAN_CACHE_SIZE = 40;
 
     private Logger logger = LoggerFactory.getLogger(this.getClass());
-    private int scanCacheSize = 40;
 
-    @Autowired
-    private HbaseOperations2 hbaseOperations2;
+    private final HbaseOperations2 hbaseTemplate;
 
-    @Autowired
-    @Qualifier("mapStatisticsCalleeMapper")
-    private RowMapper<LinkDataMap> mapStatisticsCalleeMapper;
+    private final TableNameProvider tableNameProvider;
 
-    @Autowired
-    private RangeFactory rangeFactory;
+    private final RowMapper<LinkDataMap> mapStatisticsCalleeMapper;
 
-    @Autowired
-    @Qualifier("statisticsCalleeRowKeyDistributor")
-    private RowKeyDistributorByHashPrefix rowKeyDistributorByHashPrefix;
+    private final RangeFactory rangeFactory;
 
+    private final RowKeyDistributorByHashPrefix rowKeyDistributorByHashPrefix;
+
+    @Autowired
+    public HbaseMapStatisticsCalleeDao(
+            HbaseOperations2 hbaseTemplate,
+            TableNameProvider tableNameProvider,
+            @Qualifier("mapStatisticsCalleeMapper") RowMapper<LinkDataMap> mapStatisticsCalleeMapper,
+            RangeFactory rangeFactory,
+            @Qualifier("statisticsCalleeRowKeyDistributor") RowKeyDistributorByHashPrefix rowKeyDistributorByHashPrefix)  {
+        this.hbaseTemplate = Objects.requireNonNull(hbaseTemplate, "hbaseTemplate must not be null");
+        this.tableNameProvider = Objects.requireNonNull(tableNameProvider, "tableNameProvider must not be null");
+        this.mapStatisticsCalleeMapper = Objects.requireNonNull(mapStatisticsCalleeMapper, "mapStatisticsCalleeMapper must not be null");
+        this.rangeFactory = Objects.requireNonNull(rangeFactory, "rangeFactory must not be null");
+        this.rowKeyDistributorByHashPrefix = Objects.requireNonNull(rowKeyDistributorByHashPrefix, "rowKeyDistributorByHashPrefix must not be null");
+    }
 
     @Override
     public LinkDataMap selectCallee(Application calleeApplication, Range range) {
@@ -78,7 +91,9 @@ public class HbaseMapStatisticsCalleeDao implements MapStatisticsCalleeDao {
         // find distributed key - ver2.
         final Scan scan = createScan(calleeApplication, range, HBaseTables.MAP_STATISTICS_CALLER_VER2_CF_COUNTER);
         ResultsExtractor<LinkDataMap> resultExtractor = new RowMapReduceResultExtractor<>(mapStatisticsCalleeMapper, new MapStatisticsTimeWindowReducer(timeWindow));
-        LinkDataMap linkDataMap = hbaseOperations2.findParallel(HBaseTables.MAP_STATISTICS_CALLER_VER2, scan, rowKeyDistributorByHashPrefix, resultExtractor, MAP_STATISTICS_CALLER_VER2_NUM_PARTITIONS);
+
+        TableName mapStatisticsCallerTableName = tableNameProvider.getTableName(HBaseTables.MAP_STATISTICS_CALLER_VER2_STR);
+        LinkDataMap linkDataMap = hbaseTemplate.findParallel(mapStatisticsCallerTableName, scan, rowKeyDistributorByHashPrefix, resultExtractor, MAP_STATISTICS_CALLER_VER2_NUM_PARTITIONS);
         logger.debug("Callee data. {}, {}", linkDataMap, range);
         if (linkDataMap != null && linkDataMap.size() > 0) {
             return linkDataMap;
@@ -100,7 +115,7 @@ public class HbaseMapStatisticsCalleeDao implements MapStatisticsCalleeDao {
         byte[] endKey = ApplicationMapStatisticsUtils.makeRowKey(application.getName(), application.getServiceTypeCode(), range.getFrom());
 
         Scan scan = new Scan();
-        scan.setCaching(this.scanCacheSize);
+        scan.setCaching(SCAN_CACHE_SIZE);
         scan.setStartRow(startKey);
         scan.setStopRow(endKey);
         scan.addFamily(family);
diff --git a/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseMapStatisticsCallerDao.java b/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseMapStatisticsCallerDao.java
index 3373d212e..2b9ea29e7 100644
--- a/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseMapStatisticsCallerDao.java
+++ b/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseMapStatisticsCallerDao.java
@@ -20,6 +20,7 @@ import com.navercorp.pinpoint.common.hbase.HBaseTables;
 import com.navercorp.pinpoint.common.hbase.HbaseOperations2;
 import com.navercorp.pinpoint.common.hbase.ResultsExtractor;
 import com.navercorp.pinpoint.common.hbase.RowMapper;
+import com.navercorp.pinpoint.common.hbase.TableNameProvider;
 import com.navercorp.pinpoint.common.util.ApplicationMapStatisticsUtils;
 import com.navercorp.pinpoint.web.applicationmap.rawdata.LinkDataMap;
 import com.navercorp.pinpoint.web.dao.MapStatisticsCallerDao;
@@ -31,6 +32,7 @@ import com.navercorp.pinpoint.web.vo.Range;
 import com.navercorp.pinpoint.web.vo.RangeFactory;
 
 import com.sematext.hbase.wd.RowKeyDistributorByHashPrefix;
+import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.client.Scan;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -38,32 +40,44 @@ import org.springframework.beans.factory.annotation.Autowired;
 import org.springframework.beans.factory.annotation.Qualifier;
 import org.springframework.stereotype.Repository;
 
+import java.util.Objects;
+
 /**
  * @author netspider
  * @author emeroad
+ * @author HyunGil Jeong
  */
 @Repository
 public class HbaseMapStatisticsCallerDao implements MapStatisticsCallerDao {
 
     private static final int MAP_STATISTICS_CALLEE_VER2_NUM_PARTITIONS = 32;
+    private static final int SCAN_CACHE_SIZE = 40;
 
     private final Logger logger = LoggerFactory.getLogger(this.getClass());
-    private int scanCacheSize = 40;
 
-    @Autowired
-    private HbaseOperations2 hbaseOperations2;
+    private final HbaseOperations2 hbaseTemplate;
 
-    @Autowired
-    @Qualifier("mapStatisticsCallerMapper")
-    private RowMapper<LinkDataMap> mapStatisticsCallerMapper;
+    private final TableNameProvider tableNameProvider;
 
-    @Autowired
-    private RangeFactory rangeFactory;
+    private final RowMapper<LinkDataMap> mapStatisticsCallerMapper;
 
-    @Autowired
-    @Qualifier("statisticsCallerRowKeyDistributor")
-    private RowKeyDistributorByHashPrefix rowKeyDistributorByHashPrefix;
+    private final RangeFactory rangeFactory;
 
+    private final RowKeyDistributorByHashPrefix rowKeyDistributorByHashPrefix;
+
+    @Autowired
+    public HbaseMapStatisticsCallerDao(
+            HbaseOperations2 hbaseTemplate,
+            TableNameProvider tableNameProvider,
+            @Qualifier("mapStatisticsCallerMapper") RowMapper<LinkDataMap> mapStatisticsCallerMapper,
+            RangeFactory rangeFactory,
+            @Qualifier("statisticsCallerRowKeyDistributor") RowKeyDistributorByHashPrefix rowKeyDistributorByHashPrefix) {
+        this.hbaseTemplate = Objects.requireNonNull(hbaseTemplate, "hbaseTemplate must not be null");
+        this.tableNameProvider = Objects.requireNonNull(tableNameProvider, "tableNameProvider must not be null");
+        this.mapStatisticsCallerMapper = Objects.requireNonNull(mapStatisticsCallerMapper, "mapStatisticsCallerMapper must not be null");
+        this.rangeFactory = Objects.requireNonNull(rangeFactory, "rangeFactory must not be null");
+        this.rowKeyDistributorByHashPrefix = Objects.requireNonNull(rowKeyDistributorByHashPrefix, "rowKeyDistributorByHashPrefix must not be null");
+    }
 
     @Override
     public LinkDataMap selectCaller(Application callerApplication, Range range) {
@@ -78,7 +92,9 @@ public class HbaseMapStatisticsCallerDao implements MapStatisticsCallerDao {
         // find distributed key.
         final Scan scan = createScan(callerApplication, range, HBaseTables.MAP_STATISTICS_CALLEE_VER2_CF_COUNTER);
         ResultsExtractor<LinkDataMap> resultExtractor = new RowMapReduceResultExtractor<>(mapStatisticsCallerMapper, new MapStatisticsTimeWindowReducer(timeWindow));
-        LinkDataMap linkDataMap = hbaseOperations2.findParallel(HBaseTables.MAP_STATISTICS_CALLEE_VER2, scan, rowKeyDistributorByHashPrefix, resultExtractor, MAP_STATISTICS_CALLEE_VER2_NUM_PARTITIONS);
+
+        TableName mapStatisticsCalleeTableName = tableNameProvider.getTableName(HBaseTables.MAP_STATISTICS_CALLEE_VER2_STR);
+        LinkDataMap linkDataMap = this.hbaseTemplate.findParallel(mapStatisticsCalleeTableName, scan, rowKeyDistributorByHashPrefix, resultExtractor, MAP_STATISTICS_CALLEE_VER2_NUM_PARTITIONS);
         logger.debug("Caller data. {}, {}", linkDataMap, range);
         if (linkDataMap != null && linkDataMap.size() > 0) {
             return linkDataMap;
@@ -100,7 +116,7 @@ public class HbaseMapStatisticsCallerDao implements MapStatisticsCallerDao {
         byte[] endKey = ApplicationMapStatisticsUtils.makeRowKey(application.getName(), application.getServiceTypeCode(), range.getFrom());
 
         Scan scan = new Scan();
-        scan.setCaching(this.scanCacheSize);
+        scan.setCaching(SCAN_CACHE_SIZE);
         scan.setStartRow(startKey);
         scan.setStopRow(endKey);
         for(byte[] family : familyArgs) {
diff --git a/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseSqlMetaDataDao.java b/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseSqlMetaDataDao.java
index d5ff76933..dc63a88ea 100644
--- a/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseSqlMetaDataDao.java
+++ b/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseSqlMetaDataDao.java
@@ -18,8 +18,10 @@ package com.navercorp.pinpoint.web.dao.hbase;
 
 import java.util.List;
 
+import com.navercorp.pinpoint.common.hbase.TableNameProvider;
 import com.sematext.hbase.wd.RowKeyDistributorByHashPrefix;
 
+import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.client.Get;
 import org.springframework.beans.factory.annotation.Autowired;
 
@@ -39,6 +41,9 @@ public class HbaseSqlMetaDataDao implements SqlMetaDataDao {
     @Autowired
     private HbaseOperations2 hbaseOperations2;
 
+    @Autowired
+    private TableNameProvider tableNameProvider;
+
 //    @Autowired
 //    @Qualifier("sqlMetaDataMapper")
     private RowMapper<List<SqlMetaDataBo>> sqlMetaDataMapper;
@@ -59,7 +64,8 @@ public class HbaseSqlMetaDataDao implements SqlMetaDataDao {
         Get get = new Get(rowKey);
         get.addFamily(HBaseTables.SQL_METADATA_VER2_CF_SQL);
 
-        return hbaseOperations2.get(HBaseTables.SQL_METADATA_VER2, get, sqlMetaDataMapper);
+        TableName sqlMetaDataTableName = tableNameProvider.getTableName(HBaseTables.SQL_METADATA_VER2_STR);
+        return hbaseOperations2.get(sqlMetaDataTableName, get, sqlMetaDataMapper);
     }
 
     private byte[] getDistributedKey(byte[] rowKey) {
diff --git a/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseStringMetaDataDao.java b/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseStringMetaDataDao.java
index a394339cb..88f7b5c86 100644
--- a/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseStringMetaDataDao.java
+++ b/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseStringMetaDataDao.java
@@ -16,6 +16,7 @@
 
 package com.navercorp.pinpoint.web.dao.hbase;
 
+import com.navercorp.pinpoint.common.hbase.TableNameProvider;
 import com.navercorp.pinpoint.common.server.bo.StringMetaDataBo;
 import com.navercorp.pinpoint.common.hbase.HBaseTables;
 import com.navercorp.pinpoint.common.hbase.HbaseOperations2;
@@ -23,6 +24,7 @@ import com.navercorp.pinpoint.common.hbase.RowMapper;
 import com.navercorp.pinpoint.web.dao.StringMetaDataDao;
 import com.sematext.hbase.wd.RowKeyDistributorByHashPrefix;
 
+import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.client.Get;
 import org.springframework.beans.factory.annotation.Autowired;
 import org.springframework.beans.factory.annotation.Qualifier;
@@ -40,6 +42,9 @@ public class HbaseStringMetaDataDao implements StringMetaDataDao {
     private HbaseOperations2 hbaseOperations2;
 
     @Autowired
+    private TableNameProvider tableNameProvider;
+
+    @Autowired
     @Qualifier("stringMetaDataMapper")
     private RowMapper<List<StringMetaDataBo>> stringMetaDataMapper;
 
@@ -59,7 +64,8 @@ public class HbaseStringMetaDataDao implements StringMetaDataDao {
         Get get = new Get(rowKey);
         get.addFamily(HBaseTables.STRING_METADATA_CF_STR);
 
-        return hbaseOperations2.get(HBaseTables.STRING_METADATA, get, stringMetaDataMapper);
+        TableName stringMetaDataTableName = tableNameProvider.getTableName(HBaseTables.STRING_METADATA_STR);
+        return hbaseOperations2.get(stringMetaDataTableName, get, stringMetaDataMapper);
     }
 
     private byte[] getDistributedKey(byte[] rowKey) {
diff --git a/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseTraceDaoV2.java b/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseTraceDaoV2.java
index 9e53f0af5..67cabb7fd 100644
--- a/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseTraceDaoV2.java
+++ b/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/HbaseTraceDaoV2.java
@@ -4,6 +4,7 @@ import com.google.common.collect.Lists;
 import com.navercorp.pinpoint.common.hbase.HBaseTables;
 import com.navercorp.pinpoint.common.hbase.HbaseOperations2;
 import com.navercorp.pinpoint.common.hbase.RowMapper;
+import com.navercorp.pinpoint.common.hbase.TableNameProvider;
 import com.navercorp.pinpoint.common.server.bo.SpanBo;
 import com.navercorp.pinpoint.common.server.bo.serializer.RowKeyEncoder;
 import com.navercorp.pinpoint.common.server.bo.serializer.trace.v2.SpanEncoder;
@@ -11,6 +12,7 @@ import com.navercorp.pinpoint.common.util.TransactionId;
 import com.navercorp.pinpoint.web.dao.TraceDao;
 import com.navercorp.pinpoint.web.mapper.CellTraceMapper;
 import org.apache.commons.collections.CollectionUtils;
+import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.client.Get;
 import org.apache.hadoop.hbase.filter.BinaryPrefixComparator;
 import org.apache.hadoop.hbase.filter.CompareFilter;
@@ -39,6 +41,9 @@ public class HbaseTraceDaoV2 implements TraceDao {
     private HbaseOperations2 template2;
 
     @Autowired
+    private TableNameProvider tableNameProvider;
+
+    @Autowired
     @Qualifier("traceRowKeyEncoderV2")
     private RowKeyEncoder<TransactionId> rowKeyEncoder;
 
@@ -72,7 +77,8 @@ public class HbaseTraceDaoV2 implements TraceDao {
         }
 
         byte[] transactionIdRowKey = rowKeyEncoder.encodeRowKey(transactionId);
-        return template2.get(HBaseTables.TRACE_V2, transactionIdRowKey, HBaseTables.TRACE_V2_CF_SPAN, spanMapperV2);
+        TableName traceTableName = tableNameProvider.getTableName(HBaseTables.TRACE_V2_STR);
+        return template2.get(traceTableName, transactionIdRowKey, HBaseTables.TRACE_V2_CF_SPAN, spanMapperV2);
     }
 
 
@@ -137,7 +143,9 @@ public class HbaseTraceDaoV2 implements TraceDao {
             final Get get = createGet(transactionId, columnFamily, filter);
             multiGet.add(get);
         }
-        return template2.get(HBaseTables.TRACE_V2, multiGet, spanMapperV2);
+
+        TableName traceTableName = tableNameProvider.getTableName(HBaseTables.TRACE_V2_STR);
+        return template2.get(traceTableName, multiGet, spanMapperV2);
     }
 
     private Get createGet(TransactionId transactionId, byte[] columnFamily, Filter filter) {
diff --git a/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/stat/v2/HbaseAgentStatDaoOperationsV2.java b/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/stat/v2/HbaseAgentStatDaoOperationsV2.java
index 146be05d1..9f135a708 100644
--- a/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/stat/v2/HbaseAgentStatDaoOperationsV2.java
+++ b/web/src/main/java/com/navercorp/pinpoint/web/dao/hbase/stat/v2/HbaseAgentStatDaoOperationsV2.java
@@ -19,6 +19,7 @@ package com.navercorp.pinpoint.web.dao.hbase.stat.v2;
 import com.navercorp.pinpoint.common.hbase.HBaseTables;
 import com.navercorp.pinpoint.common.hbase.HbaseOperations2;
 import com.navercorp.pinpoint.common.hbase.ResultsExtractor;
+import com.navercorp.pinpoint.common.hbase.TableNameProvider;
 import com.navercorp.pinpoint.common.server.bo.codec.stat.AgentStatDecoder;
 import com.navercorp.pinpoint.common.server.bo.serializer.stat.AgentStatHbaseOperationFactory;
 import com.navercorp.pinpoint.common.server.bo.serializer.stat.AgentStatUtils;
@@ -29,6 +30,7 @@ import com.navercorp.pinpoint.web.mapper.TimestampFilter;
 import com.navercorp.pinpoint.web.mapper.stat.AgentStatMapperV2;
 import com.navercorp.pinpoint.web.vo.Range;
 import com.navercorp.pinpoint.web.vo.stat.SampledAgentStatDataPoint;
+import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.client.Scan;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -53,6 +55,9 @@ public class HbaseAgentStatDaoOperationsV2 {
     private HbaseOperations2 hbaseOperations2;
 
     @Autowired
+    private TableNameProvider tableNameProvider;
+
+    @Autowired
     private AgentStatHbaseOperationFactory operationFactory;
 
     <T extends AgentStatDataPoint> List<T> getAgentStatList(AgentStatType agentStatType, AgentStatMapperV2<T> mapper, String agentId, Range range) {
@@ -65,7 +70,8 @@ public class HbaseAgentStatDaoOperationsV2 {
 
         Scan scan = this.createScan(agentStatType, agentId, range);
 
-        List<List<T>> intermediate = hbaseOperations2.findParallel(HBaseTables.AGENT_STAT_VER2, scan, this.operationFactory.getRowKeyDistributor(), mapper, AGENT_STAT_VER2_NUM_PARTITIONS);
+        TableName agentStatTableName = tableNameProvider.getTableName(HBaseTables.AGENT_STAT_VER2_STR);
+        List<List<T>> intermediate = hbaseOperations2.findParallel(agentStatTableName, scan, this.operationFactory.getRowKeyDistributor(), mapper, AGENT_STAT_VER2_NUM_PARTITIONS);
         int expectedSize = (int) (range.getRange() / HBaseTables.AGENT_STAT_TIMESPAN_MS);
         List<T> merged = new ArrayList<>(expectedSize);
         for (List<T> each : intermediate) {
@@ -89,7 +95,8 @@ public class HbaseAgentStatDaoOperationsV2 {
         int resultLimit = 20;
         Scan scan = this.createScan(agentStatType, agentId, range, resultLimit);
 
-        List<List<T>> result = hbaseOperations2.findParallel(HBaseTables.AGENT_STAT_VER2, scan, this.operationFactory.getRowKeyDistributor(), resultLimit, mapper, AGENT_STAT_VER2_NUM_PARTITIONS);
+        TableName agentStatTableName = tableNameProvider.getTableName(HBaseTables.AGENT_STAT_VER2_STR);
+        List<List<T>> result = hbaseOperations2.findParallel(agentStatTableName, scan, this.operationFactory.getRowKeyDistributor(), resultLimit, mapper, AGENT_STAT_VER2_NUM_PARTITIONS);
         if (result.isEmpty()) {
             return false;
         } else {
@@ -108,7 +115,9 @@ public class HbaseAgentStatDaoOperationsV2 {
             throw new NullPointerException("resultExtractor must not be null");
         }
         Scan scan = this.createScan(agentStatType, agentId, range);
-        return hbaseOperations2.findParallel(HBaseTables.AGENT_STAT_VER2, scan, this.operationFactory.getRowKeyDistributor(), resultExtractor, AGENT_STAT_VER2_NUM_PARTITIONS);
+
+        TableName agentStatTableName = tableNameProvider.getTableName(HBaseTables.AGENT_STAT_VER2_STR);
+        return hbaseOperations2.findParallel(agentStatTableName, scan, this.operationFactory.getRowKeyDistributor(), resultExtractor, AGENT_STAT_VER2_NUM_PARTITIONS);
     }
 
     <T extends AgentStatDataPoint> AgentStatMapperV2<T> createRowMapper(AgentStatDecoder<T> decoder, Range range) {
diff --git a/web/src/main/java/com/navercorp/pinpoint/web/service/AlarmServiceImpl.java b/web/src/main/java/com/navercorp/pinpoint/web/service/AlarmServiceImpl.java
index 4fedf3c37..2e87122d1 100644
--- a/web/src/main/java/com/navercorp/pinpoint/web/service/AlarmServiceImpl.java
+++ b/web/src/main/java/com/navercorp/pinpoint/web/service/AlarmServiceImpl.java
@@ -27,11 +27,13 @@ import com.navercorp.pinpoint.web.alarm.vo.CheckerResult;
 import com.navercorp.pinpoint.web.alarm.vo.Rule;
 import com.navercorp.pinpoint.web.dao.AlarmDao;
 import com.navercorp.pinpoint.web.vo.UserGroup;
+import org.springframework.transaction.annotation.Transactional;
 
 /**
  * @author minwoo.jung
  */
 @Service
+@Transactional(rollbackFor = {Exception.class})
 public class AlarmServiceImpl implements AlarmService {
 
     @Autowired
@@ -49,11 +51,13 @@ public class AlarmServiceImpl implements AlarmService {
     }
 
     @Override
+    @Transactional(readOnly = true)
     public List<Rule> selectRuleByUserGroupId(String userGroupId) {
         return alarmDao.selectRuleByUserGroupId(userGroupId);
     }
 
     @Override
+    @Transactional(readOnly = true)
     public List<Rule> selectRuleByApplicationId(String applicationId) {
         return alarmDao.selectRuleByApplicationId(applicationId);
     }
@@ -64,6 +68,7 @@ public class AlarmServiceImpl implements AlarmService {
     }
 
     @Override
+    @Transactional(readOnly = true)
     public Map<String, CheckerResult> selectBeforeCheckerResults(String applicationId) {
         Map<String, CheckerResult> checkerResults = new HashMap<>();
         List<CheckerResult> CheckerResultList = alarmDao.selectBeforeCheckerResultList(applicationId);
diff --git a/web/src/main/java/com/navercorp/pinpoint/web/service/UserGroupService.java b/web/src/main/java/com/navercorp/pinpoint/web/service/UserGroupService.java
index 8f3e77b67..d6cbd3c71 100644
--- a/web/src/main/java/com/navercorp/pinpoint/web/service/UserGroupService.java
+++ b/web/src/main/java/com/navercorp/pinpoint/web/service/UserGroupService.java
@@ -19,12 +19,14 @@ import java.util.List;
 
 import com.navercorp.pinpoint.web.vo.UserGroup;
 import com.navercorp.pinpoint.web.vo.UserGroupMember;
+import com.navercorp.pinpoint.web.vo.UserGroupMemberParam;
+import com.navercorp.pinpoint.web.vo.exception.PinpointUserGroupException;
 
 /**
  * @author minwoo.jung
  */
 public interface UserGroupService {
-    String createUserGroup(UserGroup userGroup);
+    String createUserGroup(UserGroup userGroup, String userId) throws PinpointUserGroupException;
     
     List<UserGroup> selectUserGroup();
     
@@ -33,11 +35,13 @@ public interface UserGroupService {
     List<UserGroup> selectUserGroupByUserGroupId(String userGroupId);
 
     void updateUserGroup(UserGroup userGroup);
-    
-    void deleteUserGroup(UserGroup userGroup);
+
+    void deleteUserGroup(UserGroup userGroup, String userId) throws PinpointUserGroupException;
 
     void insertMember(UserGroupMember userGroupMember);
 
+    void deleteMemberWithCheckAuthority(UserGroupMember userGroupMember, String userId) throws PinpointUserGroupException;
+
     void deleteMember(UserGroupMember userGroupMember);
 
     List<UserGroupMember> selectMember(String userGroupId);
@@ -54,4 +58,5 @@ public interface UserGroupService {
 
     boolean containMemberForUserGroup(String userId, String userGroupId);
 
+    void insertMemberWithCheckAuthority(UserGroupMemberParam userGroupMember, String userId) throws PinpointUserGroupException;
 }
diff --git a/web/src/main/java/com/navercorp/pinpoint/web/service/UserGroupServiceImpl.java b/web/src/main/java/com/navercorp/pinpoint/web/service/UserGroupServiceImpl.java
index c5430ad26..b2eb94d90 100644
--- a/web/src/main/java/com/navercorp/pinpoint/web/service/UserGroupServiceImpl.java
+++ b/web/src/main/java/com/navercorp/pinpoint/web/service/UserGroupServiceImpl.java
@@ -17,20 +17,26 @@ package com.navercorp.pinpoint.web.service;
 
 import java.util.List;
 
+import com.navercorp.pinpoint.web.config.ConfigProperties;
 import com.navercorp.pinpoint.web.util.DefaultUserInfoDecoder;
 import com.navercorp.pinpoint.web.util.UserInfoDecoder;
 import com.navercorp.pinpoint.web.vo.User;
+import com.navercorp.pinpoint.web.vo.UserGroupMemberParam;
+import com.navercorp.pinpoint.web.vo.exception.PinpointUserGroupException;
 import org.springframework.beans.factory.annotation.Autowired;
 import org.springframework.stereotype.Service;
 
 import com.navercorp.pinpoint.web.dao.UserGroupDao;
 import com.navercorp.pinpoint.web.vo.UserGroup;
 import com.navercorp.pinpoint.web.vo.UserGroupMember;
+import org.springframework.transaction.annotation.Transactional;
+import org.springframework.util.StringUtils;
 
 /**
  * @author minwoo.jung
  */
 @Service
+@Transactional(rollbackFor = {Exception.class})
 public class UserGroupServiceImpl implements UserGroupService {
 
     @Autowired
@@ -38,23 +44,44 @@ public class UserGroupServiceImpl implements UserGroupService {
 
     @Autowired(required = false)
     UserInfoDecoder userInfoDecoder = DefaultUserInfoDecoder.EMPTY_USER_INFO_DECODER;
+
+    @Autowired
+    AlarmService alarmService;
+
+    @Autowired
+    private ConfigProperties webProperties;
+
+
     
     @Override
-    public String createUserGroup(UserGroup userGroup) {
-        return userGroupDao.createUserGroup(userGroup);
+    public String createUserGroup(UserGroup userGroup, String userId) throws PinpointUserGroupException {
+        String userGroupNumber = userGroupDao.createUserGroup(userGroup);
+
+        if (webProperties.isOpenSource() == false) {
+            if (StringUtils.isEmpty(userId)) {
+                throw new PinpointUserGroupException("There is not userId or fail to create userGroup.");
+            }
+
+            insertMember(new UserGroupMember(userGroup.getId(), userId));
+        }
+
+        return userGroupNumber;
     }
 
     @Override
+    @Transactional(readOnly = true)
     public List<UserGroup> selectUserGroup() {
         return userGroupDao.selectUserGroup();
     }
 
     @Override
+    @Transactional(readOnly = true)
     public List<UserGroup> selectUserGroupByUserId(String userId) {
         return userGroupDao.selectUserGroupByUserId(userId);
     }
 
     @Override
+    @Transactional(readOnly = true)
     public List<UserGroup> selectUserGroupByUserGroupId(String userGroupId) {
         return userGroupDao.selectUserGroupByUserGroupId(userGroupId);
     }
@@ -65,8 +92,27 @@ public class UserGroupServiceImpl implements UserGroupService {
     }
 
     @Override
-    public void deleteUserGroup(UserGroup userGroup) {
+    public void deleteUserGroup(UserGroup userGroup, String userId) throws PinpointUserGroupException {
+        if (webProperties.isOpenSource() == false) {
+            if (checkValid(userId, userGroup.getId()) == false) {
+                throw new PinpointUserGroupException("There is not userId or you don't have authoriy for user group.");
+            }
+        }
+
         userGroupDao.deleteUserGroup(userGroup);
+        deleteMemberByUserGroupId(userGroup.getId());
+        alarmService.deleteRuleByUserGroupId(userGroup.getId());
+    }
+
+    private boolean checkValid(String userId, String userGroupId) {
+        if (StringUtils.isEmpty(userId)) {
+            return false;
+        }
+        if (containMemberForUserGroup(userId, userGroupId) == false) {
+            return false;
+        }
+
+        return true;
     }
 
     @Override
@@ -75,11 +121,37 @@ public class UserGroupServiceImpl implements UserGroupService {
     }
 
     @Override
+    public void insertMemberWithCheckAuthority(UserGroupMemberParam userGroupMember, String userId) throws PinpointUserGroupException {
+        if (webProperties.isOpenSource() == false) {
+            boolean isValid = checkValid(userId, userGroupMember.getUserGroupId());
+            if (isValid == false) {
+                throw new PinpointUserGroupException("there is not userId or you don't have authority for user group.");
+            }
+        }
+
+        insertMember(userGroupMember);
+    }
+
+
+    @Override
+    public void deleteMemberWithCheckAuthority(UserGroupMember userGroupMember, String userId) throws PinpointUserGroupException {
+        if (webProperties.isOpenSource() == false) {
+            boolean isValid = checkValid(userId, userGroupMember.getUserGroupId());
+            if (isValid == false) {
+                throw new PinpointUserGroupException("there is not userId or you don't have authority for user group.");
+            }
+        }
+
+        userGroupDao.deleteMember(userGroupMember);
+    }
+
+    @Override
     public void deleteMember(UserGroupMember userGroupMember) {
-        userGroupDao.deleteMember(userGroupMember); 
+        userGroupDao.deleteMember(userGroupMember);
     }
 
     @Override
+    @Transactional(readOnly = true)
     public List<UserGroupMember> selectMember(String userGroupId) {
         return userGroupDao.selectMember(userGroupId);
     }
@@ -90,6 +162,7 @@ public class UserGroupServiceImpl implements UserGroupService {
     }
     
     @Override
+    @Transactional(readOnly = true)
     public List<String> selectPhoneNumberOfMember(String userGroupId) {
         final List<String> phoneNumberList = userGroupDao.selectPhoneNumberOfMember(userGroupId);
         List<String> decodedPhoneNumberList = phoneNumberList;
@@ -102,6 +175,7 @@ public class UserGroupServiceImpl implements UserGroupService {
     }
 
     @Override
+    @Transactional(readOnly = true)
     public List<String> selectEmailOfMember(String userGroupId) {
         return userGroupDao.selectEmailOfMember(userGroupId);
     }
@@ -117,6 +191,7 @@ public class UserGroupServiceImpl implements UserGroupService {
     }
 
     @Override
+    @Transactional(readOnly = true)
     public boolean containMemberForUserGroup(String userId, String userGroupId) {
         List<UserGroupMember> memberList = userGroupDao.selectMember(userGroupId);
         for (UserGroupMember member : memberList) {
diff --git a/web/src/main/java/com/navercorp/pinpoint/web/service/UserService.java b/web/src/main/java/com/navercorp/pinpoint/web/service/UserService.java
index 11f254c09..ec8d78663 100644
--- a/web/src/main/java/com/navercorp/pinpoint/web/service/UserService.java
+++ b/web/src/main/java/com/navercorp/pinpoint/web/service/UserService.java
@@ -28,6 +28,8 @@ public interface UserService {
     
     void insertUser(User user);
 
+    void insertUserList(List<User> users);
+
     void deleteUser(User user);
 
     void updateUser(User user);
diff --git a/web/src/main/java/com/navercorp/pinpoint/web/service/UserServiceImpl.java b/web/src/main/java/com/navercorp/pinpoint/web/service/UserServiceImpl.java
index 4413302ac..7afaed039 100644
--- a/web/src/main/java/com/navercorp/pinpoint/web/service/UserServiceImpl.java
+++ b/web/src/main/java/com/navercorp/pinpoint/web/service/UserServiceImpl.java
@@ -22,11 +22,13 @@ import org.springframework.stereotype.Service;
 
 import com.navercorp.pinpoint.web.dao.UserDao;
 import com.navercorp.pinpoint.web.vo.User;
+import org.springframework.transaction.annotation.Transactional;
 
 /**
  * @author minwoo.jung
  */
 @Service
+@Transactional(rollbackFor = {Exception.class})
 public class UserServiceImpl implements UserService {
 
     @Autowired
@@ -49,21 +51,25 @@ public class UserServiceImpl implements UserService {
     }
 
     @Override
+    @Transactional(readOnly = true)
     public List<User> selectUser() {
         return userDao.selectUser();
     }
 
     @Override
+    @Transactional(readOnly = true)
     public User selectUserByUserId(String userId) {
         return userDao.selectUserByUserId(userId);
     }
 
     @Override
+    @Transactional(readOnly = true)
     public List<User> selectUserByUserName(String userName) {
         return userDao.selectUserByUserName(userName);
     }
 
     @Override
+    @Transactional(readOnly = true)
     public List<User> selectUserByDepartment(String department) {
         return userDao.selectUserByDepartment(department);
     }
@@ -73,4 +79,9 @@ public class UserServiceImpl implements UserService {
         userDao.dropAndCreateUserTable();
     }
 
+    @Override
+    public void insertUserList(List<User> users) {
+        userDao.insertUserList(users);
+    }
+
 }
diff --git a/web/src/main/java/com/navercorp/pinpoint/web/service/map/ApplicationsMapCreatorFactory.java b/web/src/main/java/com/navercorp/pinpoint/web/service/map/ApplicationsMapCreatorFactory.java
index 900a35285..36944b097 100644
--- a/web/src/main/java/com/navercorp/pinpoint/web/service/map/ApplicationsMapCreatorFactory.java
+++ b/web/src/main/java/com/navercorp/pinpoint/web/service/map/ApplicationsMapCreatorFactory.java
@@ -16,17 +16,12 @@
 
 package com.navercorp.pinpoint.web.service.map;
 
-import com.navercorp.pinpoint.common.util.PinpointThreadFactory;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
 import org.springframework.beans.factory.annotation.Autowired;
-import org.springframework.beans.factory.annotation.Value;
+import org.springframework.beans.factory.annotation.Qualifier;
 import org.springframework.stereotype.Component;
 
-import javax.annotation.PreDestroy;
-import java.util.concurrent.ExecutorService;
-import java.util.concurrent.Executors;
-import java.util.concurrent.TimeUnit;
+import java.util.Objects;
+import java.util.concurrent.Executor;
 
 /**
  * @author HyunGil Jeong
@@ -34,41 +29,14 @@ import java.util.concurrent.TimeUnit;
 @Component
 public class ApplicationsMapCreatorFactory {
 
-    private final Logger logger = LoggerFactory.getLogger(this.getClass());
-
-    private final String mode;
-
-    private final ExecutorService executorService;
+    private final Executor executor;
 
     @Autowired
-    public ApplicationsMapCreatorFactory(
-            @Value("#{pinpointWebProps['web.servermap.creator.mode'] ?: 'serial'}") String mode,
-            @Value("#{pinpointWebProps['web.servermap.creator.parallel.maxthreads'] ?: '16'}") int threadCount) {
-        logger.info("ApplicationsMapCreatorFactory mode : {}", mode);
-        this.mode = mode;
-        if (this.mode.equalsIgnoreCase("parallel")) {
-            this.executorService = Executors.newFixedThreadPool(threadCount, new PinpointThreadFactory("Pinpoint-parallel-link-selector", true));
-        } else {
-            this.executorService = null;
-        }
+    public ApplicationsMapCreatorFactory(@Qualifier("applicationsMapCreateExecutor") Executor executor) {
+        this.executor = Objects.requireNonNull(executor, "executor must not be null");
     }
 
     public ApplicationsMapCreator create(ApplicationMapCreator applicationMapCreator) {
-        if (mode.equalsIgnoreCase("parallel")) {
-            return new ParallelApplicationsMapCreator(applicationMapCreator, executorService);
-        }
-        return new SerialApplicationsMapCreator(applicationMapCreator);
-    }
-
-    @PreDestroy
-    public void preDestroy() {
-        if (executorService != null) {
-            executorService.shutdown();
-            try {
-                executorService.awaitTermination(10, TimeUnit.SECONDS);
-            } catch (InterruptedException e) {
-                Thread.currentThread().interrupt();
-            }
-        }
+        return new DefaultApplicationsMapCreator(applicationMapCreator, executor);
     }
 }
diff --git a/web/src/main/java/com/navercorp/pinpoint/web/service/map/ParallelApplicationsMapCreator.java b/web/src/main/java/com/navercorp/pinpoint/web/service/map/DefaultApplicationsMapCreator.java
similarity index 61%
rename from web/src/main/java/com/navercorp/pinpoint/web/service/map/ParallelApplicationsMapCreator.java
rename to web/src/main/java/com/navercorp/pinpoint/web/service/map/DefaultApplicationsMapCreator.java
index 19b743d1d..4fe98cfbe 100644
--- a/web/src/main/java/com/navercorp/pinpoint/web/service/map/ParallelApplicationsMapCreator.java
+++ b/web/src/main/java/com/navercorp/pinpoint/web/service/map/DefaultApplicationsMapCreator.java
@@ -1,5 +1,5 @@
 /*
- * Copyright 2017 NAVER Corp.
+ * Copyright 2018 NAVER Corp.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
  * you may not use this file except in compliance with the License.
@@ -21,50 +21,63 @@ import com.navercorp.pinpoint.web.applicationmap.rawdata.LinkDataDuplexMap;
 import com.navercorp.pinpoint.web.vo.Application;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
+import org.springframework.util.CollectionUtils;
 
 import java.util.ArrayList;
 import java.util.List;
+import java.util.Objects;
 import java.util.Set;
 import java.util.concurrent.CompletableFuture;
-import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executor;
 import java.util.function.Supplier;
 
 /**
  * @author HyunGil Jeong
  */
-public class ParallelApplicationsMapCreator implements ApplicationsMapCreator {
+public class DefaultApplicationsMapCreator implements ApplicationsMapCreator {
 
     private final Logger logger = LoggerFactory.getLogger(this.getClass());
 
     private final ApplicationMapCreator applicationMapCreator;
 
-    private final ExecutorService executorService;
+    private final Executor executor;
 
-    public ParallelApplicationsMapCreator(ApplicationMapCreator applicationMapCreator, ExecutorService executorService) {
-        if (applicationMapCreator == null) {
-            throw new NullPointerException("applicationMapCreator must not be null");
-        }
-        if (executorService == null) {
-            throw new NullPointerException("executorService must not be null");
-        }
-        this.applicationMapCreator = applicationMapCreator;
-        this.executorService = executorService;
+    public DefaultApplicationsMapCreator(ApplicationMapCreator applicationMapCreator, Executor executor) {
+        this.applicationMapCreator = Objects.requireNonNull(applicationMapCreator, "applicationMapCreator must not be null");
+        this.executor = Objects.requireNonNull(executor, "executor must not be null");
     }
 
     @Override
     public LinkDataDuplexMap createLinkDataDuplexMap(List<Application> applications, LinkSelectContext linkSelectContext) {
-        final Set<LinkDataDuplexMap> searchResults = Sets.newConcurrentHashSet();
-        CompletableFuture[] futures = getLinkDataMapFutures(searchResults, applications, linkSelectContext);
-        try {
-            CompletableFuture.allOf(futures).join();
-        } catch (Exception e) {
-            logger.error("Error selecting link", e);
+        if (CollectionUtils.isEmpty(applications)) {
             return new LinkDataDuplexMap();
         }
+
+        if (applications.size() > 1) {
+            return createParallel(applications, linkSelectContext);
+        }
+        return createSerial(applications, linkSelectContext);
+    }
+
+    private LinkDataDuplexMap createSerial(List<Application> applications, LinkSelectContext linkSelectContext) {
+        final LinkDataDuplexMap resultMap = new LinkDataDuplexMap();
+        for (Application application : applications) {
+            LinkDataDuplexMap searchResult = applicationMapCreator.createMap(application, linkSelectContext);
+            resultMap.addLinkDataDuplexMap(searchResult);
+        }
+        logger.debug("depth search. callerDepth : {}, calleeDepth : {}", linkSelectContext.getCallerDepth(), linkSelectContext.getCalleeDepth());
+        return resultMap;
+    }
+
+    private LinkDataDuplexMap createParallel(List<Application> applications, LinkSelectContext linkSelectContext) {
+        final Set<LinkDataDuplexMap> searchResults = Sets.newConcurrentHashSet();
+        CompletableFuture[] futures = getLinkDataMapFutures(searchResults, applications, linkSelectContext);
+        CompletableFuture.allOf(futures).join();
         LinkDataDuplexMap resultMap = new LinkDataDuplexMap();
         for (LinkDataDuplexMap searchResult : searchResults) {
             resultMap.addLinkDataDuplexMap(searchResult);
         }
+        logger.debug("depth search. callerDepth : {}, calleeDepth : {}", linkSelectContext.getCallerDepth(), linkSelectContext.getCalleeDepth());
         return resultMap;
     }
 
@@ -76,7 +89,7 @@ public class ParallelApplicationsMapCreator implements ApplicationsMapCreator {
                 public LinkDataDuplexMap get() {
                     return applicationMapCreator.createMap(targetApplication, linkSelectContext);
                 }
-            }, executorService);
+            }, executor);
             CompletableFuture<Void> searchResultsFuture = linkDataDuplexMapFuture.thenAccept(searchResults::add);
             linkDataDuplexMapFutures.add(searchResultsFuture);
         }
diff --git a/web/src/main/java/com/navercorp/pinpoint/web/service/map/SerialApplicationsMapCreator.java b/web/src/main/java/com/navercorp/pinpoint/web/service/map/SerialApplicationsMapCreator.java
deleted file mode 100644
index 0ef04d6e9..000000000
--- a/web/src/main/java/com/navercorp/pinpoint/web/service/map/SerialApplicationsMapCreator.java
+++ /dev/null
@@ -1,52 +0,0 @@
-/*
- * Copyright 2017 NAVER Corp.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package com.navercorp.pinpoint.web.service.map;
-
-import com.navercorp.pinpoint.web.applicationmap.rawdata.LinkDataDuplexMap;
-import com.navercorp.pinpoint.web.vo.Application;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import java.util.List;
-
-/**
- * @author HyunGil Jeong
- */
-public class SerialApplicationsMapCreator implements ApplicationsMapCreator {
-
-    private final Logger logger = LoggerFactory.getLogger(this.getClass());
-
-    private final ApplicationMapCreator applicationMapCreator;
-
-    public SerialApplicationsMapCreator(ApplicationMapCreator applicationMapCreator) {
-        if (applicationMapCreator == null) {
-            throw new NullPointerException("applicationMapCreator must not be null");
-        }
-        this.applicationMapCreator = applicationMapCreator;
-    }
-
-    @Override
-    public LinkDataDuplexMap createLinkDataDuplexMap(List<Application> applications, LinkSelectContext linkSelectContext) {
-        final LinkDataDuplexMap resultMap = new LinkDataDuplexMap();
-        for (Application application : applications) {
-            LinkDataDuplexMap searchResult = applicationMapCreator.createMap(application, linkSelectContext);
-            resultMap.addLinkDataDuplexMap(searchResult);
-        }
-        logger.debug("depth search end. callerDepth : {}, calleeDepth : {}", linkSelectContext.getCallerDepth(), linkSelectContext.getCalleeDepth());
-        return resultMap;
-    }
-}
diff --git a/web/src/main/java/com/navercorp/pinpoint/web/task/ChainedTaskDecorator.java b/web/src/main/java/com/navercorp/pinpoint/web/task/ChainedTaskDecorator.java
new file mode 100644
index 000000000..efa83315e
--- /dev/null
+++ b/web/src/main/java/com/navercorp/pinpoint/web/task/ChainedTaskDecorator.java
@@ -0,0 +1,43 @@
+/*
+ * Copyright 2018 NAVER Corp.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package com.navercorp.pinpoint.web.task;
+
+import org.springframework.core.task.TaskDecorator;
+
+import java.util.List;
+import java.util.Objects;
+
+/**
+ * @author HyunGil Jeong
+ */
+public class ChainedTaskDecorator implements TaskDecorator {
+
+    private final List<TaskDecorator> taskDecorators;
+
+    public ChainedTaskDecorator(List<TaskDecorator> taskDecorators) {
+        this.taskDecorators = Objects.requireNonNull(taskDecorators, "taskDecorators must not be null");
+    }
+
+    @Override
+    public Runnable decorate(Runnable runnable) {
+        Runnable decoratedRunnable = runnable;
+        for (TaskDecorator taskDecorator : taskDecorators) {
+            decoratedRunnable = taskDecorator.decorate(decoratedRunnable);
+        }
+        return decoratedRunnable;
+    }
+}
diff --git a/web/src/main/java/com/navercorp/pinpoint/web/task/RequestContextPropagatingTaskDecorator.java b/web/src/main/java/com/navercorp/pinpoint/web/task/RequestContextPropagatingTaskDecorator.java
new file mode 100644
index 000000000..e7e0839c3
--- /dev/null
+++ b/web/src/main/java/com/navercorp/pinpoint/web/task/RequestContextPropagatingTaskDecorator.java
@@ -0,0 +1,56 @@
+/*
+ * Copyright 2018 NAVER Corp.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package com.navercorp.pinpoint.web.task;
+
+import org.springframework.core.task.TaskDecorator;
+import org.springframework.web.context.request.RequestAttributes;
+import org.springframework.web.context.request.RequestContextHolder;
+
+import java.util.Objects;
+
+/**
+ * @author HyunGil Jeong
+ */
+public class RequestContextPropagatingTaskDecorator implements TaskDecorator {
+
+    @Override
+    public Runnable decorate(Runnable runnable) {
+        return new RequestContextPropagatingRunnable(runnable);
+    }
+
+    private class RequestContextPropagatingRunnable implements Runnable {
+
+        private final Runnable delegate;
+        private final RequestAttributes requestAttributes;
+
+        private RequestContextPropagatingRunnable(Runnable delegate) {
+            this.delegate = Objects.requireNonNull(delegate, "delegate must not be null");
+            this.requestAttributes = RequestContextHolder.getRequestAttributes();
+        }
+
+        @Override
+        public void run() {
+            RequestAttributes previousRequestAttributes = RequestContextHolder.getRequestAttributes();
+            RequestContextHolder.setRequestAttributes(requestAttributes);
+            try {
+                delegate.run();
+            } finally {
+                RequestContextHolder.setRequestAttributes(previousRequestAttributes);
+            }
+        }
+    }
+}
diff --git a/web/src/main/java/com/navercorp/pinpoint/web/task/SecurityContextPropagatingTaskDecorator.java b/web/src/main/java/com/navercorp/pinpoint/web/task/SecurityContextPropagatingTaskDecorator.java
new file mode 100644
index 000000000..d52bafd43
--- /dev/null
+++ b/web/src/main/java/com/navercorp/pinpoint/web/task/SecurityContextPropagatingTaskDecorator.java
@@ -0,0 +1,56 @@
+/*
+ * Copyright 2018 NAVER Corp.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package com.navercorp.pinpoint.web.task;
+
+import org.springframework.core.task.TaskDecorator;
+import org.springframework.security.core.context.SecurityContext;
+import org.springframework.security.core.context.SecurityContextHolder;
+
+import java.util.Objects;
+
+/**
+ * @author HyunGil Jeong
+ */
+public class SecurityContextPropagatingTaskDecorator implements TaskDecorator {
+
+    @Override
+    public Runnable decorate(Runnable runnable) {
+        return new SecurityContextPropagatingRunnable(runnable);
+    }
+
+    private class SecurityContextPropagatingRunnable implements Runnable {
+
+        private final Runnable delegate;
+        private final SecurityContext securityContext;
+
+        private SecurityContextPropagatingRunnable(Runnable delegate) {
+            this.delegate = Objects.requireNonNull(delegate, "delegate must not be null");
+            this.securityContext = SecurityContextHolder.getContext();
+        }
+
+        @Override
+        public void run() {
+            SecurityContext previousSecurityContext = SecurityContextHolder.getContext();
+            SecurityContextHolder.setContext(securityContext);
+            try {
+                delegate.run();
+            } finally {
+                SecurityContextHolder.setContext(previousSecurityContext);
+            }
+        }
+    }
+}
diff --git a/web/src/test/java/com/navercorp/pinpoint/web/service/map/BidirectionalLinkSelector_serial_Test.java b/web/src/main/java/com/navercorp/pinpoint/web/vo/exception/PinpointUserGroupException.java
similarity index 61%
rename from web/src/test/java/com/navercorp/pinpoint/web/service/map/BidirectionalLinkSelector_serial_Test.java
rename to web/src/main/java/com/navercorp/pinpoint/web/vo/exception/PinpointUserGroupException.java
index f5d0605b4..072eda1e9 100644
--- a/web/src/test/java/com/navercorp/pinpoint/web/service/map/BidirectionalLinkSelector_serial_Test.java
+++ b/web/src/main/java/com/navercorp/pinpoint/web/vo/exception/PinpointUserGroupException.java
@@ -1,5 +1,5 @@
 /*
- * Copyright 2017 NAVER Corp.
+ * Copyright 2018 NAVER Corp.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
  * you may not use this file except in compliance with the License.
@@ -13,16 +13,14 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-
-package com.navercorp.pinpoint.web.service.map;
+package com.navercorp.pinpoint.web.vo.exception;
 
 /**
- * @author HyunGil Jeong
+ * @author minwoo.jung
  */
-public class BidirectionalLinkSelector_serial_Test extends BidirectionalLinkSelectorTestBase {
+public class PinpointUserGroupException extends Exception {
 
-    @Override
-    protected ApplicationsMapCreatorFactory createApplicationsMapCreatorFactory() {
-        return new ApplicationsMapCreatorFactory("serial", 16);
+    public PinpointUserGroupException(String message) {
+        super(message);
     }
 }
diff --git a/web/src/main/resources/applicationContext-dao-config.xml b/web/src/main/resources/applicationContext-dao-config.xml
index 914c11b79..64605404c 100644
--- a/web/src/main/resources/applicationContext-dao-config.xml
+++ b/web/src/main/resources/applicationContext-dao-config.xml
@@ -16,20 +16,7 @@
         <property name="dataSource" ref="dataSource"/>
     </bean>
 
-    <tx:advice id="transactionAdvice" transaction-manager="transactionManager">
-        <tx:attributes>
-            <tx:method name="select*" read-only="true" />
-            <tx:method name="read*" read-only="true" />
-            <tx:method name="get*" read-only="true" />
-            <tx:method name="view*"/>
-            <tx:method name="list*"/>
-            <tx:method name="add*"/>
-            <tx:method name="create*"/>
-            <tx:method name="delete*"/>
-            <tx:method name="update*"/>
-        </tx:attributes>
-    </tx:advice>
-
+    <tx:annotation-driven/>
 
     <!-- SqlsessionFactory setup for MyBatis Database Layer -->
     <bean id="sqlSessionFactory" class="org.mybatis.spring.SqlSessionFactoryBean">
diff --git a/web/src/main/resources/applicationContext-hbase.xml b/web/src/main/resources/applicationContext-hbase.xml
index 798e3111b..e3daa6a8d 100644
--- a/web/src/main/resources/applicationContext-hbase.xml
+++ b/web/src/main/resources/applicationContext-hbase.xml
@@ -63,6 +63,10 @@
 
     <bean class="org.apache.hadoop.util.ShutdownHookManagerProxy"/>
 
+    <bean id="tableNameProvider" class="com.navercorp.pinpoint.common.hbase.HbaseTableNameProvider">
+        <constructor-arg index="0" value="${hbase.namespace:}"/>
+    </bean>
+
     <bean id="applicationTraceIndexDistributor" class="com.sematext.hbase.wd.RowKeyDistributorByHashPrefix">
         <constructor-arg ref="applicationTraceIndex"/>
     </bean>
diff --git a/web/src/main/resources/applicationContext-web.xml b/web/src/main/resources/applicationContext-web.xml
index 8b719882c..3a1d1d343 100644
--- a/web/src/main/resources/applicationContext-web.xml
+++ b/web/src/main/resources/applicationContext-web.xml
@@ -2,25 +2,13 @@
 <beans xmlns="http://www.springframework.org/schema/beans"
        xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
        xmlns:context="http://www.springframework.org/schema/context"
-       xmlns:lang="http://www.springframework.org/schema/lang"
-       xmlns:beans="http://www.springframework.org/schema/beans"
        xmlns:util="http://www.springframework.org/schema/util"
        xsi:schemaLocation="
-        http://www.springframework.org/schema/lang http://www.springframework.org/schema/lang/spring-lang.xsd
         http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd
         http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util.xsd
         http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd">
 
     <context:annotation-config/>
-
-        <!--
-    <context:component-scan base-package="com.navercorp.pinpoint.web.dao.hbase,
-                                         com.navercorp.pinpoint.web.dao.mysql,
-                                         com.navercorp.pinpoint.web.service,
-                                         com.navercorp.pinpoint.web.mapper,
-                                         com.navercorp.pinpoint.web.filter" />
-        -->
-
     <context:component-scan base-package="com.navercorp.pinpoint.web.dao.hbase,
                                          com.navercorp.pinpoint.web.service,
                                          com.navercorp.pinpoint.web.mapper,
@@ -164,4 +152,43 @@
             </list>
         </property>
     </bean>
+
+    <bean id="requestContextPropagatingTaskDecorator" class="com.navercorp.pinpoint.web.task.RequestContextPropagatingTaskDecorator"/>
+    <bean id="securityContextPropagatingTaskDecorator" class="com.navercorp.pinpoint.web.task.SecurityContextPropagatingTaskDecorator"/>
+    <bean id="contextPropagatingTaskDecorator" class="com.navercorp.pinpoint.web.task.ChainedTaskDecorator">
+        <constructor-arg>
+            <list>
+                <ref bean="requestContextPropagatingTaskDecorator"/>
+                <ref bean="securityContextPropagatingTaskDecorator"/>
+            </list>
+        </constructor-arg>
+    </bean>
+
+    <bean id="baseTaskExecutor" class="org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor" abstract="true">
+        <property name="taskDecorator" ref="contextPropagatingTaskDecorator"/>
+        <property name="daemon" value="true"/>
+        <property name="waitForTasksToCompleteOnShutdown" value="true"/>
+        <property name="awaitTerminationSeconds" value="10"/>
+    </bean>
+
+    <bean id="applicationsMapCreateExecutor" class="org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor" parent="baseTaskExecutor">
+        <property name="corePoolSize" value="#{pinpointWebProps['web.servermap.creator.worker.threadSize'] ?: 16}"/>
+        <property name="maxPoolSize" value="#{pinpointWebProps['web.servermap.creator.worker.threadSize'] ?: 16}"/>
+        <property name="queueCapacity" value="#{pinpointWebProps['web.servermap.creator.worker.queueSize'] ?: 1024}"/>
+        <property name="threadNamePrefix" value="Pinpoint-Link-Selector-"/>
+    </bean>
+
+    <bean id="nodeHistogramAppendExecutor" class="org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor" parent="baseTaskExecutor">
+        <property name="corePoolSize" value="#{pinpointWebProps['web.servermap.appender.worker.threadSize'] ?: 16}"/>
+        <property name="maxPoolSize" value="#{pinpointWebProps['web.servermap.appender.worker.threadSize'] ?: 16}"/>
+        <property name="queueCapacity" value="#{pinpointWebProps['web.servermap.appender.worker.queueSize'] ?: 1024}"/>
+        <property name="threadNamePrefix" value="Pinpoint-Node-Histogram-Appender-"/>
+    </bean>
+
+    <bean id="serverInfoAppendExecutor" class="org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor" parent="baseTaskExecutor">
+        <property name="corePoolSize" value="#{pinpointWebProps['web.servermap.appender.worker.threadSize'] ?: 16}"/>
+        <property name="maxPoolSize" value="#{pinpointWebProps['web.servermap.appender.worker.threadSize'] ?: 16}"/>
+        <property name="queueCapacity" value="#{pinpointWebProps['web.servermap.appender.worker.queueSize'] ?: 1024}"/>
+        <property name="threadNamePrefix" value="Pinpoint-Server-Info-Appender-"/>
+    </bean>
 </beans>
diff --git a/web/src/main/resources/hbase.properties b/web/src/main/resources/hbase.properties
index 9766d3647..c663a4db9 100644
--- a/web/src/main/resources/hbase.properties
+++ b/web/src/main/resources/hbase.properties
@@ -4,6 +4,9 @@ hbase.client.port=2181
 # hbase default:/hbase
 hbase.zookeeper.znode.parent=/hbase
 
+# hbase namespace to use default:default - you may leave this empty to use the default namespace
+hbase.namespace=
+
 # hbase timeout option==================================================================================
 # hbase default:true
 hbase.ipc.client.tcpnodelay=true
diff --git a/web/src/main/resources/pinpoint-web.properties b/web/src/main/resources/pinpoint-web.properties
index c64007993..44647443f 100644
--- a/web/src/main/resources/pinpoint-web.properties
+++ b/web/src/main/resources/pinpoint-web.properties
@@ -45,13 +45,14 @@ web.hbase.selectAllSpans.limit=500
 
 web.activethread.activeAgent.duration.days=7
 
-# server map link creator mode = serial or parallel (default = serial)
-web.servermap.creator.mode=parallel
-web.servermap.creator.parallel.maxthreads=16
-
-# server map appender mode = serial or parallel (default = serial)
-web.servermap.appender.mode=parallel
-web.servermap.appender.parallel.maxthreads=16
+# number of server map link select worker threads
+web.servermap.creator.worker.threadSize=32
+# capacity of server map link select worker queue
+web.servermap.creator.worker.queueSize=1024
+# number of server node appender worker threads
+web.servermap.appender.worker.threadSize=32
+# capacity of server node appender worker queue
+web.servermap.appender.worker.queueSize=1024
 
 # see RFC 6454: The Web Origin Concept(https://tools.ietf.org/html/rfc6454) for more details
 # 1. Allow only same origin requests (value : websocket.allowedOrigins=)
diff --git a/web/src/main/webapp/common/help/help-content-en.js b/web/src/main/webapp/common/help/help-content-en.js
index 3fa4fcd83..fe52968f0 100644
--- a/web/src/main/webapp/common/help/help-content-en.js
+++ b/web/src/main/webapp/common/help/help-content-en.js
@@ -4,11 +4,11 @@
 		configuration: {
 			general: {
 				warning: "(User configuration is stored in browser cache. Server-side storage will be supported in a future release.)",
-				empty: "Favorite list empty"
+				empty: "Favorite list is empty"
 			},
 			alarmRules: {
 				mainStyle: "",
-				title: "Alarm Rule Type",
+				title: "Types of Alarm Rule",
 				desc: "The following types of alarm rules are supported by Pinpoint.",
 				category: [{
 					title: "[Type]",
@@ -58,7 +58,7 @@
 				}]
 			},
 			installation: {
-				desc: "* You can check whether the Application Name and Agent Id are duplicated.",
+				desc: "* You can check if Application Name and Agent Id are duplicated.",
 				lengthGuide: "You can enter up to {{MAX_CHAR}} characters."
 			}
 		},	
@@ -75,10 +75,10 @@
 					title: "[Legend]",
 					items: [{
 						name: "Icon",
-						desc: "Application Type"
+						desc: "Type of the Application"
 					}, {
 						name: "Text",
-						desc: "Application Name. The value set using <code>-Dpinpoint.applicationName</code> when launching Pinpoint agent."
+						desc: "Name of the Application. The value of <code>-Dpinpoint.applicationName</code> in Pinpoint agent configuration."
 					}]
 				}]
 			},
@@ -90,7 +90,7 @@
 					title: "[Legend]",
 					items: [{
 						name: "Inbound",
-						desc: "Number of depth to render for requests coming in to the selected node."
+						desc: "Number of depth to render for requests coming into the selected node."
 					}, {
 						name: "Outbound",
 						desc: "Number of depth to render for requests going out from the selected node"
@@ -112,7 +112,7 @@
 			periodSelector: {
 				mainStyle: "",
 				title: "Period Selector",
-				desc: "Selects the time period for querying data.",
+				desc: "Selects the time range for search data.",
 				category: [{
 					title: "[Usage]",
 					items: [{
@@ -120,7 +120,7 @@
 						desc: "Query for data traced during the most recent selected time-period.<br/>Auto-refresh is supported for 5m, 10m, 3h time-period."
 					},{
 						name: "<button type='button' class='btn btn-success btn-xs'><span class='glyphicon glyphicon-calendar'></span></button>",
-						desc: "Query for data traced between the two selected times for a maximum of 48 hours."
+						desc: "Query for traced data between two selected times with the maximum of 48 hours."
 					}]
 				}]
 			}
@@ -142,17 +142,17 @@
 					title: "[Arrow]",
 					list: [
 						"Each arrow represents a transaction flow.",
-						"The number shows the transaction count and is displayed in red for transactions with error.",
+						"The number shows the transaction count. Displayed in red for error counts that exceeds the threshold.",
 						"<span class='glyphicon glyphicon-filter' style='color:green;'></span> is shown when a filter is applied.",
 						"Clicking an arrow shows information on all transactions passing through the selected section on the right-hand side of the screen."
 				    ]
 				},{
-					title: "[Function of the box]",
+					title: "[Usage of the Node]",
 					list: [
-						"When the box is selected, the transaction information flowing into the application is displayed on the right side of the screen."
+						"When the node is selected, the transaction information flowing into the application is displayed on the right side of the screen."
 					]
 				},{
-					title: "[Function of the arrow]",
+					title: "[Usage of the Arrow]",
 					list: [
 						"Select the arrow to show the transaction information that passes through the selected section on the right side of the screen.",
 						"The Filter in the Context menu shows only the transactions that pass through the selected section.",
@@ -162,16 +162,16 @@
 				},{
 					title: "[Applying Filter]",
 					list: [
-				        "Right-clicking on an arrow displays a filter menu.",
-				        "'Filter' filters the server map to only show transactions that has passed through the selected section.",
+				        "Right-clicking on an arrow displays the filter menu.",
+				        "'Filter' filters the server map to show transactions that have passed through the selected section.",
 				        "'Filter Wizard' allows additional filter configurations."
 					]
 				},{
 					title: "[Chart Configuration]",
 					list: [
-				        "Right-clicking on an empty area displays a chart configuration menu.",
-				        "Node Setting / Merge Unknown : Groups all agent-less applications into a single node.",
-				        "Double-clicking on an empty resets the zoom level of the server map."
+				        "Right-clicking on any blank area displays a chart configuration menu.",
+				        "Node Setting / Merge Unknown : Groups all applications without agent and displays it as a single node.",
+				        "Double-clicking on any blank area resets the zoom level of the server map."
 					]
 				}]
 			} 
@@ -478,7 +478,7 @@
 					title: "[Usage]",
 					items: [{
 						name: "<button type='button' class='btn btn-default btn-xs'>Inspector</button>",
-						desc: "Open a new window with detailed information on the WAS with Pinpoint installed."
+						desc: "Open a new window with detailed information on the WAS where Pinpoint is installed."
 					},{
 						name: "<button type='button' class='btn btn-default btn-xs'><span class='glyphicon glyphicon-plus'></span></button>",
 						desc: "Display statistics on transactions carried out by the server instance."
@@ -551,7 +551,7 @@
 			heap: {
 				mainStyle: "",
 				title: "Heap",
-				desc: "JVM's heap information and full garbage collection times(if any)",
+				desc: "JVM's heap information and full garbage collection time required",
 				category: [{
 					title: "[Legend]",
 					items: [{
@@ -559,17 +559,17 @@
 						desc: "Maximum heap size"
 					},{
 						name: "Used",
-						desc: "Heap currently in use"
+						desc: "Heap size currently in use"
 					},{
 						name: "FGC",
-						desc: "Full garbage collection duration (number of FGCs in parenthesis if it occurred more than once)"
+						desc: "Time required for full garbage collection (number of FGCs in parenthesis if it occurred more than once)"
 					}]
 				}]
 			},
 			permGen: {
 				mainStyle: "",
 				title: "Non-Heap",
-				desc: "JVM's non-heap information and full garbage collection times(if any)",
+				desc: "JVM's non-heap information and full garbage collection time required",
 				category: [{
 					title: "[Legend]",
 					items: [{
@@ -577,10 +577,10 @@
 						desc: "Maximum non-heap size"
 					},{
 						name: "Used",
-						desc: "Non-heap currently in use"
+						desc: "Non-heap size currently in use"
 					},{
 						name: "FGC",
-						desc: "Full garbage collection duration (number of FGCs in parenthesis if it occurred more than once)"
+						desc: "Time required for full garbage collection (number of FGCs in parenthesis if it occurred more than once)"
 					}]
 				}]
 			},
@@ -592,28 +592,28 @@
 					title: "[Legend]",
 					items: [{
 						name: "Java 1.6",
-						desc: "Only the JVM's CPU usage is collected"
+						desc: "Only JVM's CPU usage is collected"
 					},{
 						name: "Java 1.7+",
-						desc: "Both the JVM's and the system's CPU usage are collected"
+						desc: "Both JVM's and system's CPU usage are collected"
 					}]
 				}]
 			},
             tps: {
                 mainStyle: "",
                 title: "TPS",
-                desc: "Transactions per second received by the server",
+                desc: "Transactions received by the server per second ",
                 category: [{
                     title: "[Legend]",
                     items: [{
                         name: "Sampled New (S.N)",
-                        desc: "Profiled transactions that started from the current agent"
+                        desc: "Profiled transactions that started from the selected agent"
                     },{
                         name: "Sampled Continuation (S.C)",
                         desc: "Profiled transactions that started from another agent"
                     },{
                         name: "Unsampled New (U.N)",
-                        desc: "Unprofiled transactions that started from the current agent"
+                        desc: "Unprofiled transactions that started from the selected agent"
                     },{
                         name: "Unsampled Continuation (U.C)",
                         desc: "Unprofiled transactions that started from another agent"
@@ -626,7 +626,7 @@
 			activeThread: {
 				mainStyle: "",
 				title: "Active Thread",
-				desc: "Snapshots of the agent's active thread count, categorized by how long they have active for serving a request.",
+				desc: "Snapshots of the agent's active thread status, categorized by how long they have been active for serving a request.",
 				category: [{
 					title: "[Legend]",
 					items: [{
@@ -658,10 +658,10 @@
 						desc: "Maximum number of active connections"
 					},{
 						name: "Total Max",
-						desc: "The maximum number of active connections that can be allocated at the same time"
+						desc: "The maximum number of connections that can be allocated at the same time"
 					},{
 						name: "Type",
-						desc: "DB Connection Pool Type"
+						desc: "Type of DB Connection Pool"
 					}]
 				}]
 			},
@@ -681,24 +681,24 @@
 				"<div style='font-size:12px'>The agent is currently registered under {{application2}} due to the following:<br>",
 				"1. The agent has moved from {{application1}} to {{application2}}<br>",
 				"2. A different agent with the same agent id has been registered to {{application2}}<hr>",
-				"For the former case, you should delete the mapping between {{application1}} and {{agentId}}.<br>",
-				"For the latter case, the agent id of the duplicate agent must be changed.</div>"
+				"For case1, you should delete the mapping between {{application1}} and {{agentId}}.<br>",
+				"For case2, the agent id of the duplicate agent must be changed.</div>"
 			].join(""),
 			statHeap: {
 				mainStyle: "",
 				title: "Heap",
-				desc: "Heap size used by agent JVMs",
+				desc: "Heap size used by the agent JVMs",
 				category: [{
 					title: "[Legend]",
 					items: [{
 						name: "MAX",
-						desc: "Largest heap size used by agent JVMs"
+						desc: "Largest heap size used by an agent JVM"
 					},{
 						name: "AVG",
-						desc: "Average heap size used by agent JVMs"
+						desc: "Average heap size used by the agent JVMs"
 					},{
 						name: "MIN",
-						desc: "Smallest heap size used by agent JVMs"
+						desc: "Smallest heap size used by an agent JVM"
 					}]
 				}]
 			},
@@ -710,13 +710,13 @@
 					title: "[Legend]",
 					items: [{
 						name: "MAX",
-						desc: "Largest non-heap size used by agent JVMs"
+						desc: "Largest non-heap size used by an agent JVM"
 					},{
 						name: "AVG",
-						desc: "Average non-heap size used by agent JVMs"
+						desc: "Average non-heap size used by the agent JVMs"
 					},{
 						name: "MIN",
-						desc: "Smallest non-heap size used by agent JVMs"
+						desc: "Smallest non-heap size used by an agent JVM"
 					}]
 				}]
 			},
@@ -741,7 +741,7 @@
 			statSystemCpu: {
 				mainStyle: "",
 				title: "System pu Usage",
-				desc: "CPU usage of agents' whole system - For multi-core CPUs, displays the average CPU usage of all the cores.",
+				desc: "CPU usage of every agent's system - For multi-core CPUs, displays the average CPU usage of all cores.",
 				category: [{
 					title: "[Legend]",
 					items: [{
@@ -761,7 +761,7 @@
 						desc: "Only the JVM's CPU usage is collected"
 					},{
 						name: "Java 1.7+",
-						desc: "Both the JVM's and the system's CPU usage are collected"
+						desc: "Both JVM's and system's CPU usage are collected"
 					}]
 				}]
 			},
@@ -809,13 +809,13 @@
 					title: "[Legend]",
 					items: [{
 						name: "MAX",
-						desc: "Highest average response time of requests served by an agent"
+						desc: "Highest value of agents' average response time"
 					},{
 						name: "AVG",
-						desc: "Average response time of requests served by all agents"
+						desc: "Average value of agents' average response time"
 					},{
 						name: "MIN",
-						desc: "Lowest average response time of requests served by an agent"
+						desc: "Lowest value of agents' average response time"
 					}]
 				}]
 			},
@@ -847,10 +847,10 @@
 					title: "[Column]",
 					items: [{
 						name: "Gap",
-						desc: "Time elapsed between the start of the previous method and entry of this method"
+						desc: "Elapsed time between the start of the previous method and the entry of this method"
 					},{
 						name: "Exec",
-						desc: "The overall duration of the method call from method entry until method exit"
+						desc: "Duration of the method call from entry to exit"
 					},{
 						name: "Exec(%)",
 						desc: "<img src='images/help/callTree_01.png'/>"
@@ -862,7 +862,7 @@
 						desc: "<span style='background-color:#FFFFFF;color:#4343C8'>Dark blue</span> A percentage of the self execution time"
 					},{
 						name: "Self",
-						desc: "The time that was used for execution of this method only, excluding time consumed in nested methods call"
+						desc: "Duration of the method call from entry to exit, excluding time consumed in nested methods call"
 					}]
 				}]
 			}
@@ -872,7 +872,7 @@
 		},
 		transactionList: {
 			openError: {
-				noParent: "Scatter data of parent window had been changed.\r\nso can\'t scan the data any more.",
+				noParent: "Unable to scan any more data due to the change of\r\nscatter data in parent window.",
 				noData: "There is no {{application}} scatter data in parent window."
 			}
 		},
diff --git a/web/src/main/webapp/features/configuration/userGroup/group-member.directive.js b/web/src/main/webapp/features/configuration/userGroup/group-member.directive.js
index b882b92a5..f6ac5dee5 100644
--- a/web/src/main/webapp/features/configuration/userGroup/group-member.directive.js
+++ b/web/src/main/webapp/features/configuration/userGroup/group-member.directive.js
@@ -143,10 +143,8 @@
 							scope.$emit( "groupMember.sendCallbackAddedUser", true, oUser.userId );
 							AlarmUtilService.setTotal( $elTotal, oGroupMemberList.length );
 							AlarmUtilService.hide( $elLoading );
-						}, function() {
-							showAlert({
-								message: CONSTS.EXIST_A_SAME
-							});
+						}, function(oServerError) {
+							showAlert(oServerError);
 							scope.$emit( "groupMember.sendCallbackAddedUser", false, oUser.userId );
 						});
 	    			});
diff --git a/web/src/test/java/com/navercorp/pinpoint/web/applicationmap/ApplicationMapBuilderCompatibilityTest.java b/web/src/test/java/com/navercorp/pinpoint/web/applicationmap/ApplicationMapBuilderTest.java
similarity index 92%
rename from web/src/test/java/com/navercorp/pinpoint/web/applicationmap/ApplicationMapBuilderCompatibilityTest.java
rename to web/src/test/java/com/navercorp/pinpoint/web/applicationmap/ApplicationMapBuilderTest.java
index 4d29c11f4..90378ae7e 100644
--- a/web/src/test/java/com/navercorp/pinpoint/web/applicationmap/ApplicationMapBuilderCompatibilityTest.java
+++ b/web/src/test/java/com/navercorp/pinpoint/web/applicationmap/ApplicationMapBuilderTest.java
@@ -1,5 +1,5 @@
 /*
- * Copyright 2017 NAVER Corp.
+ * Copyright 2018 NAVER Corp.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
  * you may not use this file except in compliance with the License.
@@ -34,6 +34,7 @@ import com.navercorp.pinpoint.web.vo.Application;
 import com.navercorp.pinpoint.web.vo.Range;
 import com.navercorp.pinpoint.web.vo.ResponseHistograms;
 import com.navercorp.pinpoint.web.vo.ResponseTime;
+import org.junit.After;
 import org.junit.Assert;
 import org.junit.Before;
 import org.junit.Test;
@@ -45,6 +46,9 @@ import java.util.Collections;
 import java.util.HashSet;
 import java.util.List;
 import java.util.Set;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executors;
+import java.util.concurrent.TimeUnit;
 
 import static org.mockito.ArgumentMatchers.any;
 import static org.mockito.ArgumentMatchers.anyCollection;
@@ -57,7 +61,11 @@ import static org.mockito.Mockito.when;
 /**
  * @author HyunGil Jeong
  */
-public class ApplicationMapBuilderCompatibilityTest {
+public class ApplicationMapBuilderTest {
+
+    private final ExecutorService serialExecutor = Executors.newSingleThreadExecutor();
+
+    private final ExecutorService parallelExecutor = Executors.newFixedThreadPool(8);
 
     private MapResponseNodeHistogramDataSource mapResponseNodeHistogramDataSource;
 
@@ -140,6 +148,26 @@ public class ApplicationMapBuilderCompatibilityTest {
         }).when(agentInfoService).populateAgentStatuses(anyCollection(), anyLong());
     }
 
+    @After
+    public void cleanUp() {
+        shutdownExecutor(serialExecutor);
+        shutdownExecutor(parallelExecutor);
+    }
+
+    private void shutdownExecutor(ExecutorService executor) {
+        if (executor != null) {
+            executor.shutdown();
+            try {
+                if (!executor.awaitTermination(10, TimeUnit.SECONDS)) {
+                    executor.shutdownNow();
+                }
+            } catch (InterruptedException e) {
+                executor.shutdownNow();
+                Thread.currentThread().interrupt();
+            }
+        }
+    }
+
     @Test
     public void testNoCallData() {
         Range range = new Range(0, 1000);
@@ -147,8 +175,8 @@ public class ApplicationMapBuilderCompatibilityTest {
 
         ServerInstanceListFactory serverInstanceListFactory = new DefaultServerInstanceListFactory(agentInfoServerInstanceListDataSource);
 
-        ApplicationMapBuilder applicationMapBuilder = ApplicationMapBuilderTestHelper.createApplicationMapBuilder(range);
-        ApplicationMapBuilder applicationMapBuilder_parallelAppenders = ApplicationMapBuilderTestHelper.createApplicationMapBuilder_parallelAppenders(range);
+        ApplicationMapBuilder applicationMapBuilder = ApplicationMapBuilderTestHelper.createApplicationMapBuilder(range, serialExecutor);
+        ApplicationMapBuilder applicationMapBuilder_parallelAppenders = ApplicationMapBuilderTestHelper.createApplicationMapBuilder(range, parallelExecutor);
         ApplicationMap applicationMap = applicationMapBuilder
                 .includeServerInfo(serverInstanceListFactory)
                 .build(application);
@@ -176,8 +204,8 @@ public class ApplicationMapBuilderCompatibilityTest {
         NodeHistogramFactory nodeHistogramFactory = new DefaultNodeHistogramFactory(mapResponseNodeHistogramDataSource);
         ServerInstanceListFactory serverInstanceListFactory = new DefaultServerInstanceListFactory(agentInfoServerInstanceListDataSource);
 
-        ApplicationMapBuilder applicationMapBuilder = ApplicationMapBuilderTestHelper.createApplicationMapBuilder(range);
-        ApplicationMapBuilder applicationMapBuilder_parallelAppenders = ApplicationMapBuilderTestHelper.createApplicationMapBuilder_parallelAppenders(range);
+        ApplicationMapBuilder applicationMapBuilder = ApplicationMapBuilderTestHelper.createApplicationMapBuilder(range, serialExecutor);
+        ApplicationMapBuilder applicationMapBuilder_parallelAppenders = ApplicationMapBuilderTestHelper.createApplicationMapBuilder(range, parallelExecutor);
         ApplicationMap applicationMap = applicationMapBuilder
                 .includeNodeHistogram(nodeHistogramFactory)
                 .includeServerInfo(serverInstanceListFactory)
@@ -276,8 +304,8 @@ public class ApplicationMapBuilderCompatibilityTest {
         ServerInstanceListFactory serverInstanceListFactory = new DefaultServerInstanceListFactory(agentInfoServerInstanceListDataSource);
 
         LinkDataDuplexMap linkDataDuplexMap = ApplicationMapBuilderTestHelper.createLinkDataDuplexMap(calleeDepth, callerDepth);
-        ApplicationMapBuilder applicationMapBuilder = ApplicationMapBuilderTestHelper.createApplicationMapBuilder(range);
-        ApplicationMapBuilder applicationMapBuilder_parallelAppenders = ApplicationMapBuilderTestHelper.createApplicationMapBuilder_parallelAppenders(range);
+        ApplicationMapBuilder applicationMapBuilder = ApplicationMapBuilderTestHelper.createApplicationMapBuilder(range, serialExecutor);
+        ApplicationMapBuilder applicationMapBuilder_parallelAppenders = ApplicationMapBuilderTestHelper.createApplicationMapBuilder(range, parallelExecutor);
 
         // test builder using MapResponseDao
         ApplicationMap applicationMap_MapResponseDao = applicationMapBuilder
diff --git a/web/src/test/java/com/navercorp/pinpoint/web/applicationmap/ApplicationMapBuilderTestHelper.java b/web/src/test/java/com/navercorp/pinpoint/web/applicationmap/ApplicationMapBuilderTestHelper.java
index 4580eccbe..b5ed9f9b6 100644
--- a/web/src/test/java/com/navercorp/pinpoint/web/applicationmap/ApplicationMapBuilderTestHelper.java
+++ b/web/src/test/java/com/navercorp/pinpoint/web/applicationmap/ApplicationMapBuilderTestHelper.java
@@ -26,6 +26,8 @@ import com.navercorp.pinpoint.web.vo.AgentInfo;
 import com.navercorp.pinpoint.web.vo.Application;
 import com.navercorp.pinpoint.web.vo.Range;
 
+import java.util.concurrent.Executor;
+
 import static com.navercorp.pinpoint.common.trace.ServiceTypeProperty.INCLUDE_DESTINATION_ID;
 import static com.navercorp.pinpoint.common.trace.ServiceTypeProperty.RECORD_STATISTICS;
 import static com.navercorp.pinpoint.common.trace.ServiceTypeProperty.TERMINAL;
@@ -41,19 +43,12 @@ public class ApplicationMapBuilderTestHelper {
     private static final ServiceType TERMINAL_TYPE = ServiceTypeFactory.of(2000, "TERMINAL", TERMINAL, INCLUDE_DESTINATION_ID);
     private static final ServiceType RPC_TYPE = ServiceTypeFactory.of(9000, "RPC", RECORD_STATISTICS);
 
-    public static ApplicationMapBuilder createApplicationMapBuilder(Range range) {
-        NodeHistogramAppenderFactory nodeHistogramAppenderFactory = new NodeHistogramAppenderFactory("serial", 16);
-        ServerInfoAppenderFactory serverInfoAppenderFactory = new ServerInfoAppenderFactory("serial", 16);
+    public static ApplicationMapBuilder createApplicationMapBuilder(Range range, Executor executor) {
+        NodeHistogramAppenderFactory nodeHistogramAppenderFactory = new NodeHistogramAppenderFactory(executor);
+        ServerInfoAppenderFactory serverInfoAppenderFactory = new ServerInfoAppenderFactory(executor);
         return new ApplicationMapBuilder(range, nodeHistogramAppenderFactory, serverInfoAppenderFactory);
     }
 
-    public static ApplicationMapBuilder createApplicationMapBuilder_parallelAppenders(Range range) {
-        NodeHistogramAppenderFactory nodeHistogramAppenderFactory = new NodeHistogramAppenderFactory("parallel", 16);
-        ServerInfoAppenderFactory serverInfoAppenderFactory = new ServerInfoAppenderFactory("parallel", 16);
-        return new ApplicationMapBuilder(range, nodeHistogramAppenderFactory, serverInfoAppenderFactory);
-
-    }
-
     public static int getExpectedNumNodes(int calleeDepth, int callerDepth) {
         if (calleeDepth < 1) {
             throw new IllegalArgumentException("calleeDepth must be greater than 0");
diff --git a/web/src/test/java/com/navercorp/pinpoint/web/applicationmap/appender/histogram/NodeHistogramAppenderTestBase.java b/web/src/test/java/com/navercorp/pinpoint/web/applicationmap/appender/histogram/NodeHistogramAppenderTest.java
similarity index 95%
rename from web/src/test/java/com/navercorp/pinpoint/web/applicationmap/appender/histogram/NodeHistogramAppenderTestBase.java
rename to web/src/test/java/com/navercorp/pinpoint/web/applicationmap/appender/histogram/NodeHistogramAppenderTest.java
index 90ef6fac3..df68c9b2b 100644
--- a/web/src/test/java/com/navercorp/pinpoint/web/applicationmap/appender/histogram/NodeHistogramAppenderTestBase.java
+++ b/web/src/test/java/com/navercorp/pinpoint/web/applicationmap/appender/histogram/NodeHistogramAppenderTest.java
@@ -31,11 +31,15 @@ import com.navercorp.pinpoint.web.applicationmap.histogram.NodeHistogram;
 import com.navercorp.pinpoint.web.applicationmap.rawdata.LinkCallDataMap;
 import com.navercorp.pinpoint.web.vo.Application;
 import com.navercorp.pinpoint.web.vo.Range;
+import org.junit.After;
 import org.junit.Assert;
 import org.junit.Before;
 import org.junit.Test;
 
 import java.util.Map;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executors;
+import java.util.concurrent.TimeUnit;
 
 import static org.mockito.Mockito.mock;
 import static org.mockito.Mockito.verifyZeroInteractions;
@@ -44,22 +48,36 @@ import static org.mockito.Mockito.when;
 /**
  * @author HyunGil Jeong
  */
-public abstract class NodeHistogramAppenderTestBase {
+public class NodeHistogramAppenderTest {
+
+    private final ExecutorService executor = Executors.newFixedThreadPool(4);
+
+    private final NodeHistogramAppenderFactory nodeHistogramAppenderFactory = new NodeHistogramAppenderFactory(executor);
 
     private WasNodeHistogramDataSource wasNodeHistogramDataSource;
 
     private NodeHistogramAppender nodeHistogramAppender;
 
-    protected abstract NodeHistogramAppenderFactory createNodeHistogramAppenderFactory();
-
     @Before
     public void setUp() {
         wasNodeHistogramDataSource = mock(WasNodeHistogramDataSource.class);
         NodeHistogramFactory nodeHistogramFactory = new DefaultNodeHistogramFactory(wasNodeHistogramDataSource);
-        NodeHistogramAppenderFactory nodeHistogramAppenderFactory = createNodeHistogramAppenderFactory();
         nodeHistogramAppender = nodeHistogramAppenderFactory.create(nodeHistogramFactory);
     }
 
+    @After
+    public void cleanUp() {
+        executor.shutdown();
+        try {
+            if (!executor.awaitTermination(10, TimeUnit.SECONDS)) {
+                executor.shutdownNow();
+            }
+        } catch (InterruptedException e) {
+            executor.shutdownNow();
+            Thread.currentThread().interrupt();
+        }
+    }
+
     @Test
     public void emptyNodeList() {
         // Given
diff --git a/web/src/test/java/com/navercorp/pinpoint/web/applicationmap/appender/histogram/SerialNodeHistogramAppenderTest.java b/web/src/test/java/com/navercorp/pinpoint/web/applicationmap/appender/histogram/SerialNodeHistogramAppenderTest.java
deleted file mode 100644
index ec33ee4f7..000000000
--- a/web/src/test/java/com/navercorp/pinpoint/web/applicationmap/appender/histogram/SerialNodeHistogramAppenderTest.java
+++ /dev/null
@@ -1,28 +0,0 @@
-/*
- * Copyright 2017 NAVER Corp.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package com.navercorp.pinpoint.web.applicationmap.appender.histogram;
-
-/**
- * @author HyunGil Jeong
- */
-public class SerialNodeHistogramAppenderTest extends NodeHistogramAppenderTestBase {
-
-    @Override
-    protected NodeHistogramAppenderFactory createNodeHistogramAppenderFactory() {
-        return new NodeHistogramAppenderFactory("serial", 1);
-    }
-}
diff --git a/web/src/test/java/com/navercorp/pinpoint/web/applicationmap/appender/server/ServerInfoAppenderTestBase.java b/web/src/test/java/com/navercorp/pinpoint/web/applicationmap/appender/server/ServerInfoAppenderTest.java
similarity index 92%
rename from web/src/test/java/com/navercorp/pinpoint/web/applicationmap/appender/server/ServerInfoAppenderTestBase.java
rename to web/src/test/java/com/navercorp/pinpoint/web/applicationmap/appender/server/ServerInfoAppenderTest.java
index dbf1067ab..945bba942 100644
--- a/web/src/test/java/com/navercorp/pinpoint/web/applicationmap/appender/server/ServerInfoAppenderTestBase.java
+++ b/web/src/test/java/com/navercorp/pinpoint/web/applicationmap/appender/server/ServerInfoAppenderTest.java
@@ -26,10 +26,15 @@ import com.navercorp.pinpoint.web.applicationmap.rawdata.LinkData;
 import com.navercorp.pinpoint.web.applicationmap.rawdata.LinkDataDuplexMap;
 import com.navercorp.pinpoint.web.vo.Application;
 import com.navercorp.pinpoint.web.vo.Range;
+import org.junit.After;
 import org.junit.Assert;
 import org.junit.Before;
 import org.junit.Test;
 
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executors;
+import java.util.concurrent.TimeUnit;
+
 import static com.navercorp.pinpoint.common.trace.ServiceTypeProperty.INCLUDE_DESTINATION_ID;
 import static com.navercorp.pinpoint.common.trace.ServiceTypeProperty.TERMINAL;
 import static org.mockito.Mockito.mock;
@@ -39,22 +44,36 @@ import static org.mockito.Mockito.when;
 /**
  * @author HyunGil Jeong
  */
-public abstract class ServerInfoAppenderTestBase {
+public class ServerInfoAppenderTest {
+
+    private final ExecutorService executor = Executors.newFixedThreadPool(4);
+
+    private final ServerInfoAppenderFactory serverInfoAppenderFactory = new ServerInfoAppenderFactory(executor);
 
     private ServerInstanceListDataSource serverInstanceListDataSource;
 
     private ServerInfoAppender serverInfoAppender;
 
-    protected abstract ServerInfoAppenderFactory createServerInfoAppenderFactory();
-
     @Before
     public void setUp() {
         serverInstanceListDataSource = mock(ServerInstanceListDataSource.class);
         ServerInstanceListFactory serverInstanceListFactory = new DefaultServerInstanceListFactory(serverInstanceListDataSource);
-        ServerInfoAppenderFactory serverInfoAppenderFactory = createServerInfoAppenderFactory();
         serverInfoAppender = serverInfoAppenderFactory.create(serverInstanceListFactory);
     }
 
+    @After
+    public void cleanUp() {
+        executor.shutdown();
+        try {
+            if (!executor.awaitTermination(10, TimeUnit.SECONDS)) {
+                executor.shutdownNow();
+            }
+        } catch (InterruptedException e) {
+            executor.shutdownNow();
+            Thread.currentThread().interrupt();
+        }
+    }
+
     @Test
     public void nullNodeList() {
         // Given
diff --git a/web/src/test/java/com/navercorp/pinpoint/web/dao/hbase/HbaseAgentLifeCycleDaoTest.java b/web/src/test/java/com/navercorp/pinpoint/web/dao/hbase/HbaseAgentLifeCycleDaoTest.java
index 75f5cf873..eb74ef618 100644
--- a/web/src/test/java/com/navercorp/pinpoint/web/dao/hbase/HbaseAgentLifeCycleDaoTest.java
+++ b/web/src/test/java/com/navercorp/pinpoint/web/dao/hbase/HbaseAgentLifeCycleDaoTest.java
@@ -18,6 +18,7 @@ package com.navercorp.pinpoint.web.dao.hbase;
 
 import static org.mockito.Mockito.*;
 
+import com.navercorp.pinpoint.common.hbase.TableNameProvider;
 import com.navercorp.pinpoint.common.server.bo.AgentLifeCycleBo;
 import com.navercorp.pinpoint.common.hbase.HbaseOperations2;
 import com.navercorp.pinpoint.common.hbase.ResultsExtractor;
@@ -35,6 +36,7 @@ import org.junit.Test;
 import org.mockito.InjectMocks;
 import org.mockito.Mock;
 import org.mockito.MockitoAnnotations;
+import org.mockito.Spy;
 
 import java.util.Arrays;
 import java.util.List;
@@ -47,6 +49,14 @@ public class HbaseAgentLifeCycleDaoTest {
     @Mock
     private HbaseOperations2 hbaseOperations2;
 
+    @Spy
+    private TableNameProvider tableNameProvider = new TableNameProvider() {
+        @Override
+        public TableName getTableName(String tableName) {
+            return TableName.valueOf(tableName);
+        }
+    };
+
     @Mock
     private RowMapper<AgentLifeCycleBo> agentLifeCycleMapper;
 
diff --git a/web/src/test/java/com/navercorp/pinpoint/web/service/FilteredMapServiceImplTest.java b/web/src/test/java/com/navercorp/pinpoint/web/service/FilteredMapServiceImplTest.java
index 6daceb32e..9fde09d64 100644
--- a/web/src/test/java/com/navercorp/pinpoint/web/service/FilteredMapServiceImplTest.java
+++ b/web/src/test/java/com/navercorp/pinpoint/web/service/FilteredMapServiceImplTest.java
@@ -41,6 +41,7 @@ import com.navercorp.pinpoint.web.view.AgentResponseTimeViewModelList;
 import com.navercorp.pinpoint.web.view.ResponseTimeViewModel;
 import com.navercorp.pinpoint.web.vo.Application;
 import com.navercorp.pinpoint.web.vo.Range;
+import org.junit.After;
 import org.junit.Assert;
 import org.junit.Before;
 import org.junit.Test;
@@ -57,6 +58,9 @@ import java.util.Collections;
 import java.util.List;
 import java.util.Map;
 import java.util.Random;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executors;
+import java.util.concurrent.TimeUnit;
 
 import static org.junit.Assert.fail;
 import static org.mockito.ArgumentMatchers.any;
@@ -73,6 +77,8 @@ public class FilteredMapServiceImplTest {
 
     private static final Random RANDOM = new Random();
 
+    private final ExecutorService executor = Executors.newFixedThreadPool(8);
+
     @Mock
     private AgentInfoService agentInfoService;
 
@@ -90,8 +96,8 @@ public class FilteredMapServiceImplTest {
 
     @Spy
     private ApplicationMapBuilderFactory applicationMapBuilderFactory = new ApplicationMapBuilderFactory(
-            new NodeHistogramAppenderFactory("serial", 0),
-            new ServerInfoAppenderFactory("serial", 0)
+            new NodeHistogramAppenderFactory(executor),
+            new ServerInfoAppenderFactory(executor)
     );
 
     @InjectMocks
@@ -116,6 +122,19 @@ public class FilteredMapServiceImplTest {
         });
     }
 
+    @After
+    public void cleanUp() {
+        executor.shutdown();
+        try {
+            if (!executor.awaitTermination(10, TimeUnit.SECONDS)) {
+                executor.shutdownNow();
+            }
+        } catch (InterruptedException e) {
+            executor.shutdownNow();
+            Thread.currentThread().interrupt();
+        }
+    }
+
     /**
      * USER -> ROOT_APP -> APP_A -> CACHE
      */
diff --git a/web/src/test/java/com/navercorp/pinpoint/web/service/map/BidirectionalLinkSelectorTestBase.java b/web/src/test/java/com/navercorp/pinpoint/web/service/map/BidirectionalLinkSelectorTest.java
similarity index 98%
rename from web/src/test/java/com/navercorp/pinpoint/web/service/map/BidirectionalLinkSelectorTestBase.java
rename to web/src/test/java/com/navercorp/pinpoint/web/service/map/BidirectionalLinkSelectorTest.java
index 27067d1cf..6d9ba5fa3 100644
--- a/web/src/test/java/com/navercorp/pinpoint/web/service/map/BidirectionalLinkSelectorTestBase.java
+++ b/web/src/test/java/com/navercorp/pinpoint/web/service/map/BidirectionalLinkSelectorTest.java
@@ -37,7 +37,7 @@ import static org.mockito.Mockito.when;
 /**
  * @author HyunGil Jeong
  */
-public abstract class BidirectionalLinkSelectorTestBase extends LinkSelectorTestBase {
+public class BidirectionalLinkSelectorTest extends LinkSelectorTestBase {
 
     @Override
     protected LinkSelectorType getLinkSelectorType() {
diff --git a/web/src/test/java/com/navercorp/pinpoint/web/service/map/BidirectionalLinkSelector_parallel_Test.java b/web/src/test/java/com/navercorp/pinpoint/web/service/map/BidirectionalLinkSelector_parallel_Test.java
deleted file mode 100644
index 9917d410f..000000000
--- a/web/src/test/java/com/navercorp/pinpoint/web/service/map/BidirectionalLinkSelector_parallel_Test.java
+++ /dev/null
@@ -1,28 +0,0 @@
-/*
- * Copyright 2017 NAVER Corp.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package com.navercorp.pinpoint.web.service.map;
-
-/**
- * @author HyunGil Jeong
- */
-public class BidirectionalLinkSelector_parallel_Test extends BidirectionalLinkSelectorTestBase {
-
-    @Override
-    protected ApplicationsMapCreatorFactory createApplicationsMapCreatorFactory() {
-        return new ApplicationsMapCreatorFactory("parallel", 16);
-    }
-}
diff --git a/web/src/test/java/com/navercorp/pinpoint/web/service/map/LinkSelectorTestBase.java b/web/src/test/java/com/navercorp/pinpoint/web/service/map/LinkSelectorTestBase.java
index a651522dd..2d3533cb5 100644
--- a/web/src/test/java/com/navercorp/pinpoint/web/service/map/LinkSelectorTestBase.java
+++ b/web/src/test/java/com/navercorp/pinpoint/web/service/map/LinkSelectorTestBase.java
@@ -29,6 +29,7 @@ import com.navercorp.pinpoint.web.service.LinkDataMapService;
 import com.navercorp.pinpoint.web.vo.Application;
 import com.navercorp.pinpoint.web.vo.LinkKey;
 import com.navercorp.pinpoint.web.vo.Range;
+import org.junit.After;
 import org.junit.Assert;
 import org.junit.Before;
 import org.junit.Test;
@@ -39,6 +40,9 @@ import java.util.HashSet;
 import java.util.List;
 import java.util.Random;
 import java.util.Set;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executors;
+import java.util.concurrent.TimeUnit;
 
 import static org.mockito.Matchers.any;
 import static org.mockito.Matchers.eq;
@@ -53,6 +57,10 @@ public abstract class LinkSelectorTestBase {
 
     private static final Random RANDOM = new Random();
 
+    private final ExecutorService executor = Executors.newFixedThreadPool(4);
+
+    private final ApplicationsMapCreatorFactory applicationsMapCreatorFactory = new ApplicationsMapCreatorFactory(executor);
+
     protected final ServiceType testRpcServiceType = ServiceTypeFactory.of(9000, "TEST_RPC_CLIENT", ServiceTypeProperty.RECORD_STATISTICS);
 
     protected final Range range = new Range(0, 100);
@@ -61,18 +69,28 @@ public abstract class LinkSelectorTestBase {
     protected HostApplicationMapDao hostApplicationMapDao;
     protected LinkSelectorFactory linkSelectorFactory;
 
-    protected abstract ApplicationsMapCreatorFactory createApplicationsMapCreatorFactory();
-
     protected abstract LinkSelectorType getLinkSelectorType();
 
     @Before
     public void setUp() throws Exception {
         this.linkDataMapService = mock(LinkDataMapService.class);
         this.hostApplicationMapDao = mock(HostApplicationMapDao.class);
-        ApplicationsMapCreatorFactory applicationsMapCreatorFactory = createApplicationsMapCreatorFactory();
         this.linkSelectorFactory = new LinkSelectorFactory(linkDataMapService, applicationsMapCreatorFactory, hostApplicationMapDao);
     }
 
+    @After
+    public void cleanUp() {
+        executor.shutdown();
+        try {
+            if (!executor.awaitTermination(10, TimeUnit.SECONDS)) {
+                executor.shutdownNow();
+            }
+        } catch (InterruptedException e) {
+            executor.shutdownNow();
+            Thread.currentThread().interrupt();
+        }
+    }
+
     final LinkDataMap newEmptyLinkDataMap() {
         return new LinkDataMap();
     }
diff --git a/web/src/test/java/com/navercorp/pinpoint/web/service/map/UnidirectionalLinkSelectorTestBase.java b/web/src/test/java/com/navercorp/pinpoint/web/service/map/UnidirectionalLinkSelectorTest.java
similarity index 98%
rename from web/src/test/java/com/navercorp/pinpoint/web/service/map/UnidirectionalLinkSelectorTestBase.java
rename to web/src/test/java/com/navercorp/pinpoint/web/service/map/UnidirectionalLinkSelectorTest.java
index a4cf13277..93d01bd4f 100644
--- a/web/src/test/java/com/navercorp/pinpoint/web/service/map/UnidirectionalLinkSelectorTestBase.java
+++ b/web/src/test/java/com/navercorp/pinpoint/web/service/map/UnidirectionalLinkSelectorTest.java
@@ -37,7 +37,7 @@ import static org.mockito.Mockito.when;
 /**
  * @author HyunGil Jeong
  */
-public abstract class UnidirectionalLinkSelectorTestBase extends LinkSelectorTestBase {
+public class UnidirectionalLinkSelectorTest extends LinkSelectorTestBase {
 
     @Override
     protected LinkSelectorType getLinkSelectorType() {
diff --git a/web/src/test/java/com/navercorp/pinpoint/web/service/map/UnidirectionalLinkSelector_parallel_Test.java b/web/src/test/java/com/navercorp/pinpoint/web/service/map/UnidirectionalLinkSelector_parallel_Test.java
deleted file mode 100644
index 5f7dc742e..000000000
--- a/web/src/test/java/com/navercorp/pinpoint/web/service/map/UnidirectionalLinkSelector_parallel_Test.java
+++ /dev/null
@@ -1,28 +0,0 @@
-/*
- * Copyright 2017 NAVER Corp.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package com.navercorp.pinpoint.web.service.map;
-
-/**
- * @author HyunGil Jeong
- */
-public class UnidirectionalLinkSelector_parallel_Test extends UnidirectionalLinkSelectorTestBase {
-
-    @Override
-    protected ApplicationsMapCreatorFactory createApplicationsMapCreatorFactory() {
-        return new ApplicationsMapCreatorFactory("parallel", 16);
-    }
-}
diff --git a/web/src/test/java/com/navercorp/pinpoint/web/service/map/UnidirectionalLinkSelector_serial_Test.java b/web/src/test/java/com/navercorp/pinpoint/web/service/map/UnidirectionalLinkSelector_serial_Test.java
deleted file mode 100644
index 82a94c012..000000000
--- a/web/src/test/java/com/navercorp/pinpoint/web/service/map/UnidirectionalLinkSelector_serial_Test.java
+++ /dev/null
@@ -1,28 +0,0 @@
-/*
- * Copyright 2017 NAVER Corp.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package com.navercorp.pinpoint.web.service.map;
-
-/**
- * @author HyunGil Jeong
- */
-public class UnidirectionalLinkSelector_serial_Test extends UnidirectionalLinkSelectorTestBase {
-
-    @Override
-    protected ApplicationsMapCreatorFactory createApplicationsMapCreatorFactory() {
-        return new ApplicationsMapCreatorFactory("serial", 16);
-    }
-}
diff --git a/web/src/test/java/com/navercorp/pinpoint/web/task/ChainedTaskDecoratorTest.java b/web/src/test/java/com/navercorp/pinpoint/web/task/ChainedTaskDecoratorTest.java
new file mode 100644
index 000000000..b7b4a8fc7
--- /dev/null
+++ b/web/src/test/java/com/navercorp/pinpoint/web/task/ChainedTaskDecoratorTest.java
@@ -0,0 +1,79 @@
+/*
+ * Copyright 2018 NAVER Corp.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package com.navercorp.pinpoint.web.task;
+
+import org.junit.Assert;
+import org.junit.Before;
+import org.junit.Test;
+import org.springframework.core.task.SimpleAsyncTaskExecutor;
+import org.springframework.core.task.TaskDecorator;
+
+import java.util.Arrays;
+import java.util.List;
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.atomic.AtomicInteger;
+
+/**
+ * @author HyunGil Jeong
+ */
+public class ChainedTaskDecoratorTest {
+
+    private SimpleAsyncTaskExecutor executor;
+
+    @Before
+    public void setup() {
+        executor = new SimpleAsyncTaskExecutor("Test-Worker-");
+    }
+
+    @Test
+    public void chainedDecoratorsShouldBeCalled() throws InterruptedException {
+        // Given
+        final int testCount = 100;
+        final CountDownLatch completeLatch = new CountDownLatch(testCount);
+        final CountingTaskDecorator decorator1 = new CountingTaskDecorator();
+        final CountingTaskDecorator decorator2 = new CountingTaskDecorator();
+        final CountingTaskDecorator decorator3 = new CountingTaskDecorator();
+        final List<TaskDecorator> decorators = Arrays.asList(decorator1, decorator2, decorator3);
+        final ChainedTaskDecorator chainedDecorator = new ChainedTaskDecorator(decorators);
+        executor.setTaskDecorator(chainedDecorator);
+        // When
+        for (int i = 0; i < testCount; i++) {
+            executor.execute(new TestWorker(completeLatch));
+        }
+        completeLatch.await(5L, TimeUnit.SECONDS);
+        // Then
+        Assert.assertEquals(testCount, decorator1.getCount());
+        Assert.assertEquals(testCount, decorator2.getCount());
+        Assert.assertEquals(testCount, decorator3.getCount());
+    }
+
+    private static class CountingTaskDecorator implements TaskDecorator {
+
+        private final AtomicInteger count = new AtomicInteger(0);
+
+        @Override
+        public Runnable decorate(Runnable runnable) {
+            count.incrementAndGet();
+            return runnable;
+        }
+
+        public int getCount() {
+            return count.get();
+        }
+    }
+}
diff --git a/web/src/test/java/com/navercorp/pinpoint/web/task/RequestContextPropagatingTaskDecoratorTest.java b/web/src/test/java/com/navercorp/pinpoint/web/task/RequestContextPropagatingTaskDecoratorTest.java
new file mode 100644
index 000000000..c6a7efcd7
--- /dev/null
+++ b/web/src/test/java/com/navercorp/pinpoint/web/task/RequestContextPropagatingTaskDecoratorTest.java
@@ -0,0 +1,79 @@
+/*
+ * Copyright 2018 NAVER Corp.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package com.navercorp.pinpoint.web.task;
+
+import org.junit.Assert;
+import org.junit.Before;
+import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.mockito.Mock;
+import org.mockito.junit.MockitoJUnitRunner;
+import org.springframework.core.task.SimpleAsyncTaskExecutor;
+import org.springframework.web.context.request.RequestAttributes;
+import org.springframework.web.context.request.RequestContextHolder;
+
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.atomic.AtomicBoolean;
+
+/**
+ * @author HyunGil Jeong
+ */
+@RunWith(MockitoJUnitRunner.class)
+public class RequestContextPropagatingTaskDecoratorTest {
+
+    private final RequestContextPropagatingTaskDecorator decorator = new RequestContextPropagatingTaskDecorator();
+    private final SimpleAsyncTaskExecutor executor = new SimpleAsyncTaskExecutor("Test-Worker-");
+
+    @Mock
+    private RequestAttributes requestAttributes;
+
+    @Before
+    public void setup() {
+        executor.setTaskDecorator(decorator);
+    }
+
+    @Test
+    public void requestContextShouldBePropagated() throws InterruptedException {
+        // Given
+        final int testCount = 100;
+        final CountDownLatch completeLatch = new CountDownLatch(testCount);
+        final AtomicBoolean verifiedFlag = new AtomicBoolean(true);
+        final TestWorker.Callback workerCallback = new TestWorker.Callback() {
+            @Override
+            public void onRun() {
+                RequestAttributes actualRequestAttributes = RequestContextHolder.getRequestAttributes();
+                boolean verified = requestAttributes == actualRequestAttributes;
+                verifiedFlag.compareAndSet(true, verified);
+            }
+
+            @Override
+            public void onError() {
+                // do nothing
+            }
+        };
+        // When
+        RequestContextHolder.setRequestAttributes(requestAttributes);
+        for (int i = 0; i < testCount; i++) {
+            executor.execute(new TestWorker(completeLatch, workerCallback));
+        }
+        completeLatch.await(5, TimeUnit.SECONDS);
+        // Then
+        boolean testVerified = verifiedFlag.get();
+        Assert.assertTrue("RequestContext has not been propagated", testVerified);
+    }
+}
diff --git a/web/src/test/java/com/navercorp/pinpoint/web/task/SecurityContextPropagatingTaskDecoratorTest.java b/web/src/test/java/com/navercorp/pinpoint/web/task/SecurityContextPropagatingTaskDecoratorTest.java
new file mode 100644
index 000000000..fd101184f
--- /dev/null
+++ b/web/src/test/java/com/navercorp/pinpoint/web/task/SecurityContextPropagatingTaskDecoratorTest.java
@@ -0,0 +1,79 @@
+/*
+ * Copyright 2018 NAVER Corp.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package com.navercorp.pinpoint.web.task;
+
+import org.junit.Assert;
+import org.junit.Before;
+import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.mockito.Mock;
+import org.mockito.junit.MockitoJUnitRunner;
+import org.springframework.core.task.SimpleAsyncTaskExecutor;
+import org.springframework.security.core.context.SecurityContext;
+import org.springframework.security.core.context.SecurityContextHolder;
+
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.atomic.AtomicBoolean;
+
+/**
+ * @author HyunGil Jeong
+ */
+@RunWith(MockitoJUnitRunner.class)
+public class SecurityContextPropagatingTaskDecoratorTest {
+
+    private final SecurityContextPropagatingTaskDecorator decorator = new SecurityContextPropagatingTaskDecorator();
+    private final SimpleAsyncTaskExecutor executor = new SimpleAsyncTaskExecutor("Test-Worker-");
+
+    @Mock
+    private SecurityContext securityContext;
+
+    @Before
+    public void setup() {
+        executor.setTaskDecorator(decorator);
+    }
+
+    @Test
+    public void securityContextShouldBePropagated() throws InterruptedException {
+        // Given
+        final int testCount = 100;
+        final CountDownLatch completeLatch = new CountDownLatch(testCount);
+        final AtomicBoolean verifiedFlag = new AtomicBoolean(true);
+        final TestWorker.Callback workerCallback = new TestWorker.Callback() {
+            @Override
+            public void onRun() {
+                SecurityContext actualSecurityContext = SecurityContextHolder.getContext();
+                boolean verified = securityContext == actualSecurityContext;
+                verifiedFlag.compareAndSet(true, verified);
+            }
+
+            @Override
+            public void onError() {
+                // do nothing
+            }
+        };
+        // When
+        SecurityContextHolder.setContext(securityContext);
+        for (int i = 0; i < testCount; i++) {
+            executor.execute(new TestWorker(completeLatch, workerCallback));
+        }
+        completeLatch.await(5, TimeUnit.SECONDS);
+        // Then
+        boolean testVerified = verifiedFlag.get();
+        Assert.assertTrue("SecurityContext has not been propagated", testVerified);
+    }
+}
diff --git a/web/src/test/java/com/navercorp/pinpoint/web/task/TestWorker.java b/web/src/test/java/com/navercorp/pinpoint/web/task/TestWorker.java
new file mode 100644
index 000000000..1e346162d
--- /dev/null
+++ b/web/src/test/java/com/navercorp/pinpoint/web/task/TestWorker.java
@@ -0,0 +1,73 @@
+/*
+ * Copyright 2018 NAVER Corp.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package com.navercorp.pinpoint.web.task;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.util.Objects;
+import java.util.concurrent.CountDownLatch;
+
+/**
+ * @author HyunGil Jeong
+ */
+public class TestWorker implements Runnable {
+
+    private final Logger logger = LoggerFactory.getLogger(this.getClass());
+
+    private final CountDownLatch completeLatch;
+    private final Callback callback;
+
+    TestWorker(CountDownLatch completeLatch) {
+        this(completeLatch, Callback.DO_NOTHING);
+    }
+
+    TestWorker(CountDownLatch completeLatch, Callback callback) {
+        this.completeLatch = Objects.requireNonNull(completeLatch, "completeLatch must not be null");
+        this.callback = Objects.requireNonNull(callback, "onWorkerRun must not be null");
+    }
+
+    @Override
+    public void run() {
+        try {
+            callback.onRun();
+            logger.debug("{} work complete.", Thread.currentThread().getName());
+        } catch (Exception e) {
+            callback.onError();
+            logger.error("{} work complete with {}", Thread.currentThread().getName(), e);
+        } finally {
+            completeLatch.countDown();
+        }
+    }
+
+    interface Callback {
+
+        void onRun();
+
+        void onError();
+
+        Callback DO_NOTHING = new Callback() {
+            @Override
+            public void onRun() {
+            }
+
+            @Override
+            public void onError() {
+            }
+        };
+    }
+}
